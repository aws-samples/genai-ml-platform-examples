{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Regression with Amazon SageMaker XGBoost algorithm (SDK V3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/build_and_train_models|sm-regression_xgboost|sm-regression_xgboost.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "_**Single machine training for regression with Amazon SageMaker XGBoost algorithm - Migrated to SDK V3**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "  1. [Fetching the dataset](#Fetching-the-dataset)\n",
    "  2. [Data Ingestion](#Data-ingestion)\n",
    "3. [Training the XGBoost model](#Training-the-XGBoost-model)\n",
    "  1. [Plotting evaluation metrics](#Plotting-evaluation-metrics)\n",
    "4. [Set up hosting for the model](#Set-up-hosting-for-the-model)\n",
    "  1. [Deploy with ModelBuilder](#Deploy-with-ModelBuilder)\n",
    "5. [Validate the model for use](#Validate-the-model-for-use)\n",
    "\n",
    "---\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the use of Amazon SageMaker's implementation of the XGBoost algorithm to train and host a regression model using **SageMaker SDK V3**. We use the [Abalone data](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html) originally from the UCI data repository [1]. More details about the original dataset can be found [here](https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.names).  In the libsvm converted [version](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html), the nominal feature (Male/Female/Infant) has been converted into a real valued feature. Age of abalone is to be predicted from eight physical measurements. Dataset is already processed and stored on S3. Scripts used for processing the data can be found in the [Appendix](#Appendix). These include downloading the data, splitting into train, validation and test, and uploading to S3 bucket. \n",
    "\n",
    ">[1] Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "\n",
    "This notebook was tested in Amazon SageMaker Studio on a ml.t3.medium instance with Python 3 (Data Science) kernel.\n",
    "\n",
    "Let's start by specifying:\n",
    "1. The S3 buckets and prefixes that you want to use for saving the model and where training data is located. This should be within the same region as the Notebook Instance, training, and hosting. \n",
    "1. The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "# V3 imports\n",
    "from sagemaker.core.helper.session_helper import Session, get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "#TODO replace with your SageMaker Domain or user execution role\n",
    "role = 'arn:aws:iam::716664005094:role/llm-fine-tuning-product-role'\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sagemaker_session = Session()\n",
    "\n",
    "# S3 bucket where the training data is located.\n",
    "data_bucket = f\"sagemaker-sample-files\"\n",
    "data_prefix = \"datasets/tabular/uci_abalone\"\n",
    "data_bucket_path = f\"s3://{data_bucket}\"\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Handle default_bucket carefully - it may be a property or method\n",
    "output_bucket = sagemaker_session.default_bucket\n",
    "if callable(output_bucket):\n",
    "    output_bucket = output_bucket()\n",
    "\n",
    "# Ensure bucket exists\n",
    "if not isinstance(output_bucket, str) or not output_bucket:\n",
    "    account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "    output_bucket = f\"sagemaker-{region}-{account_id}\"\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=output_bucket)\n",
    "    except s3_client.exceptions.NoSuchBucket:\n",
    "        if region == 'us-east-1':\n",
    "            s3_client.create_bucket(Bucket=output_bucket)\n",
    "        else:\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=output_bucket,\n",
    "                CreateBucketConfiguration={'LocationConstraint': region}\n",
    "            )\n",
    "\n",
    "output_prefix = \"sagemaker/DEMO-xgboost-abalone-v3\"\n",
    "output_bucket_path = f\"s3://{output_bucket}\"\n",
    "\n",
    "for data_category in [\"train\", \"test\", \"validation\"]:\n",
    "    data_key = \"{0}/{1}/abalone.{1}\".format(data_prefix, data_category)\n",
    "    output_key = \"{0}/{1}/abalone.{1}\".format(output_prefix, data_category)\n",
    "    data_filename = \"abalone.{}\".format(data_category)\n",
    "    s3_client.download_file(data_bucket, data_key, data_filename)\n",
    "    s3_client.upload_file(data_filename, output_bucket, output_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Training the XGBoost model\n",
    "\n",
    "After setting training parameters, we kick off training, and poll for status until training is completed.\n",
    "\n",
    "Training can be done by either calling SageMaker Training with a set of hyperparameters values to train with, or by leveraging SageMaker Automatic Model Tuning ([AMT](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html)). AMT, also known as hyperparameter tuning (HPO), finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose.\n",
    "\n",
    "In this notebook, both methods are used for demonstration purposes, but the model that the HPO job creates is the one that is eventually hosted. You can instead choose to deploy the model created by the standalone training job by changing the below variable `deploy_amt_model` to False.\n",
    "\n",
    "### Initializing common variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V3 imports\n",
    "from sagemaker.core import image_uris\n",
    "\n",
    "container = image_uris.retrieve(\"xgboost\", region, \"1.7-1\")\n",
    "deploy_amt_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Training with ModelTrainer (V3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "import time\n",
    "\n",
    "# V3 imports\n",
    "from sagemaker.train import ModelTrainer\n",
    "from sagemaker.train.configs import InputData, Compute, StoppingCondition, OutputDataConfig\n",
    "\n",
    "# Configure input data\n",
    "train_input = InputData(\n",
    "    channel_name=\"train\",\n",
    "    data_source=f\"{output_bucket_path}/{output_prefix}/train\",\n",
    "    content_type=\"libsvm\"\n",
    ")\n",
    "\n",
    "validation_input = InputData(\n",
    "    channel_name=\"validation\",\n",
    "    data_source=f\"{output_bucket_path}/{output_prefix}/validation\",\n",
    "    content_type=\"libsvm\"\n",
    ")\n",
    "\n",
    "# Configure compute resources\n",
    "compute_config = Compute(\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    instance_count=1,\n",
    "    volume_size_in_gb=5\n",
    ")\n",
    "\n",
    "# Configure stopping condition\n",
    "stopping_config = StoppingCondition(max_runtime_in_seconds=3600)\n",
    "\n",
    "# Configure output\n",
    "output_config = OutputDataConfig(\n",
    "    s3_output_path=f\"{output_bucket_path}/{output_prefix}/single-xgboost\"\n",
    ")\n",
    "\n",
    "# Create ModelTrainer\n",
    "trainer = ModelTrainer(\n",
    "    training_image=container,\n",
    "    role=role,\n",
    "    compute=compute_config,\n",
    "    stopping_condition=stopping_config,\n",
    "    output_data_config=output_config,\n",
    "    hyperparameters={\n",
    "        \"max_depth\": \"5\",\n",
    "        \"eta\": \"0.2\",\n",
    "        \"gamma\": \"4\",\n",
    "        \"min_child_weight\": \"6\",\n",
    "        \"subsample\": \"0.7\",\n",
    "        \"objective\": \"reg:linear\",\n",
    "        \"num_round\": \"50\",\n",
    "        \"verbosity\": \"2\"\n",
    "    },\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training job. It will take between 5 and 6 minutes to complete.\")\n",
    "trainer.train(\n",
    "    input_data_config=[train_input, validation_input],\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "# Get the training job name and model artifacts\n",
    "training_job_name = trainer._latest_training_job.training_job_name\n",
    "model_artifacts = trainer._latest_training_job.model_artifacts.s3_model_artifacts\n",
    "print(f\"Training job name: {training_job_name}\")\n",
    "print(f\"Model artifacts: {model_artifacts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "Note that the \"validation\" channel has been initialized too. The SageMaker XGBoost algorithm actually calculates RMSE and writes it to the CloudWatch logs on the data passed to the \"validation\" channel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Tuning with HyperparameterTuner (V3)\n",
    "\n",
    "To create a tuning job using the V3 HyperparameterTuner, you need to:\n",
    "\n",
    "1. Create a base ModelTrainer with static hyperparameters\n",
    "2. Define hyperparameter ranges using ContinuousParameter and IntegerParameter\n",
    "3. Create HyperparameterTuner with the base trainer and ranges\n",
    "4. Call tuner.tune() to start the tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "# V3 imports\n",
    "from sagemaker.train.tuner import HyperparameterTuner\n",
    "from sagemaker.core.parameter import ContinuousParameter, IntegerParameter\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "hyperparameter_ranges = {\n",
    "    \"eta\": ContinuousParameter(min_value=0.1, max_value=0.5),\n",
    "    \"gamma\": ContinuousParameter(min_value=0, max_value=5),\n",
    "    \"min_child_weight\": ContinuousParameter(min_value=0, max_value=120),\n",
    "    \"subsample\": ContinuousParameter(min_value=0.5, max_value=1),\n",
    "    \"alpha\": ContinuousParameter(min_value=0, max_value=2),\n",
    "    \"max_depth\": IntegerParameter(min_value=0, max_value=10),\n",
    "    \"num_round\": IntegerParameter(min_value=1, max_value=4000)\n",
    "}\n",
    "\n",
    "# Create base trainer with static hyperparameters only\n",
    "base_trainer = ModelTrainer(\n",
    "    training_image=container,\n",
    "    role=role,\n",
    "    compute=compute_config,\n",
    "    stopping_condition=StoppingCondition(max_runtime_in_seconds=43200),\n",
    "    output_data_config=output_config,\n",
    "    hyperparameters={\n",
    "        \"objective\": \"reg:linear\",\n",
    "        \"verbosity\": \"2\"\n",
    "    },\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Create HyperparameterTuner\n",
    "tuner = HyperparameterTuner(\n",
    "    model_trainer=base_trainer,\n",
    "    objective_metric_name=\"validation:rmse\",\n",
    "    objective_type=\"Minimize\",\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    max_jobs=6,\n",
    "    max_parallel_jobs=2,\n",
    "    strategy=\"Bayesian\"\n",
    ")\n",
    "\n",
    "# Start tuning\n",
    "print(\"Starting tuning job. It will take between 12 and 17 minutes to complete.\")\n",
    "tuner.tune(\n",
    "    inputs=[train_input, validation_input],\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "# Get best training job name\n",
    "best_job_name = tuner.best_training_job()\n",
    "print(f\"Best training job: {best_job_name}\")\n",
    "\n",
    "# Get model artifacts from best training job\n",
    "from sagemaker.core.resources import TrainingJob\n",
    "best_training_job = TrainingJob.get(training_job_name=best_job_name)\n",
    "tuned_model_artifacts = best_training_job.model_artifacts.s3_model_artifacts\n",
    "print(f\"Model artifacts from best job: {tuned_model_artifacts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Set up hosting for the model\n",
    "\n",
    "### Deploy with ModelBuilder (V3)\n",
    "\n",
    "In V3, we use ModelBuilder to deploy models to endpoints. This simplifies the process compared to the 3-step deployment in V2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "# V3 imports\n",
    "from sagemaker.serve.model_builder import ModelBuilder\n",
    "from sagemaker.serve.mode.function_pointers import Mode\n",
    "\n",
    "# Determine which model to deploy\n",
    "if deploy_amt_model:\n",
    "    model_data = tuned_model_artifacts\n",
    "    model_source = \"tuning job\"\n",
    "else:\n",
    "    model_data = model_artifacts\n",
    "    model_source = \"training job\"\n",
    "\n",
    "print(f\"Deploying model from {model_source}\")\n",
    "print(f\"Model data: {model_data}\")\n",
    "\n",
    "# Create ModelBuilder with correct parameter names\n",
    "model_builder = ModelBuilder(\n",
    "    s3_model_data_url=model_data,  # NOT model_data\n",
    "    role_arn=role,                  # NOT role\n",
    "    image_uri=container,\n",
    "    mode=Mode.SAGEMAKER_ENDPOINT,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "model = model_builder.build()\n",
    "\n",
    "# Deploy to endpoint\n",
    "endpoint_name = f'DEMO-XGBoostEndpoint-{strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())}'\n",
    "print(f\"Creating endpoint with name: {endpoint_name}. This will take between 9 and 11 minutes to complete.\")\n",
    "\n",
    "endpoint = model_builder.deploy(\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=endpoint_name,\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "print(f\"Endpoint deployed: {endpoint.endpoint_name}\")\n",
    "print(f\"Endpoint ARN: {endpoint.endpoint_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Validate the model for use\n",
    "\n",
    "Finally, we can validate the model for use. In V3, we use `endpoint.invoke()` instead of boto3 runtime client."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "Download test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_TEST = \"abalone.test\"\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(data_bucket, f\"{data_prefix}/test/{FILE_TEST}\", FILE_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "Start with a single prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -1 abalone.test > abalone.single.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "from itertools import islice\n",
    "import math\n",
    "import struct\n",
    "\n",
    "file_name = \"abalone.single.test\"\n",
    "with open(file_name, \"r\") as f:\n",
    "    payload = f.read().strip()\n",
    "\n",
    "# V3: Use endpoint.invoke() instead of runtime_client.invoke_endpoint()\n",
    "result = endpoint.invoke(\n",
    "    body=payload,\n",
    "    content_type=\"text/x-libsvm\"\n",
    ")\n",
    "\n",
    "# V3: Parse response manually\n",
    "response_body = result.body.read().decode('utf-8')\n",
    "\n",
    "# Handle different response formats\n",
    "if ',' in response_body.strip():\n",
    "    predictions = [math.ceil(float(num.strip())) for num in response_body.strip().split(',') if num.strip()]\n",
    "elif '\\n' in response_body.strip():\n",
    "    predictions = [math.ceil(float(num.strip())) for num in response_body.strip().split('\\n') if num.strip()]\n",
    "else:\n",
    "    predictions = [math.ceil(float(response_body.strip()))]\n",
    "\n",
    "label = payload.strip(\" \").split()[0]\n",
    "print(f\"Label: {label}\\nPrediction: {predictions[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "OK, a single prediction works. Let's do a whole batch to see how good is the predictions accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "\n",
    "# V3: Updated to use endpoint.invoke()\n",
    "def do_predict(data, endpoint_obj, content_type):\n",
    "    payload = \"\\n\".join(data)\n",
    "    result = endpoint_obj.invoke(\n",
    "        body=payload,\n",
    "        content_type=content_type\n",
    "    )\n",
    "    \n",
    "    # Parse response manually\n",
    "    response_body = result.body.read().decode('utf-8')\n",
    "    response_body = response_body.strip(\"\\n\")\n",
    "    \n",
    "    # Handle different response formats\n",
    "    if '\\n' in response_body:\n",
    "        preds = [float(num.strip()) for num in response_body.split('\\n') if num.strip()]\n",
    "    elif ',' in response_body:\n",
    "        preds = [float(num.strip()) for num in response_body.split(',') if num.strip()]\n",
    "    else:\n",
    "        preds = [float(response_body)]\n",
    "    \n",
    "    preds = [math.ceil(num) for num in preds]\n",
    "    return preds\n",
    "\n",
    "\n",
    "def batch_predict(data, batch_size, endpoint_obj, content_type):\n",
    "    items = len(data)\n",
    "    arrs = []\n",
    "\n",
    "    for offset in range(0, items, batch_size):\n",
    "        if offset + batch_size < items:\n",
    "            results = do_predict(data[offset : (offset + batch_size)], endpoint_obj, content_type)\n",
    "            arrs.extend(results)\n",
    "        else:\n",
    "            arrs.extend(do_predict(data[offset:items], endpoint_obj, content_type))\n",
    "        sys.stdout.write(\".\")\n",
    "    return arrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "The following helps us calculate the Median Absolute Percent Error (MdAPE) on the batch dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open(FILE_TEST, \"r\") as f:\n",
    "    payload = f.read().strip()\n",
    "\n",
    "labels = [int(line.split(\" \")[0]) for line in payload.split(\"\\n\")]\n",
    "test_data = [line for line in payload.split(\"\\n\")]\n",
    "preds = batch_predict(test_data, 100, endpoint, \"text/x-libsvm\")\n",
    "\n",
    "print(\n",
    "    \"\\n Median Absolute Percent Error (MdAPE) = \",\n",
    "    np.median(np.abs(np.array(labels) - np.array(preds)) / np.array(labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### Delete Endpoint\n",
    "\n",
    "Once you are done using the endpoint, you can delete it. In V3, you need to delete the endpoint and endpoint config separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V3: Delete endpoint and endpoint config separately\n",
    "from sagemaker.core.resources import EndpointConfig\n",
    "\n",
    "# Get endpoint config name before deleting\n",
    "endpoint_config_name = endpoint.endpoint_config_name\n",
    "\n",
    "# Delete the endpoint\n",
    "print(f\"Deleting endpoint: {endpoint.endpoint_name}\")\n",
    "endpoint.delete()\n",
    "\n",
    "# Delete the endpoint config\n",
    "print(f\"Deleting endpoint config: {endpoint_config_name}\")\n",
    "endpoint_config = EndpointConfig.get(endpoint_config_name=endpoint_config_name)\n",
    "endpoint_config.delete()\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "### Data split and upload\n",
    "\n",
    "Following methods split the data into train/test/validation datasets and upload files to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "import random\n",
    "\n",
    "\n",
    "def data_split(\n",
    "    FILE_DATA,\n",
    "    FILE_TRAIN,\n",
    "    FILE_VALIDATION,\n",
    "    FILE_TEST,\n",
    "    PERCENT_TRAIN,\n",
    "    PERCENT_VALIDATION,\n",
    "    PERCENT_TEST,\n",
    "):\n",
    "    data = [l for l in open(FILE_DATA, \"r\")]\n",
    "    train_file = open(FILE_TRAIN, \"w\")\n",
    "    valid_file = open(FILE_VALIDATION, \"w\")\n",
    "    tests_file = open(FILE_TEST, \"w\")\n",
    "\n",
    "    num_of_data = len(data)\n",
    "    num_train = int((PERCENT_TRAIN / 100.0) * num_of_data)\n",
    "    num_valid = int((PERCENT_VALIDATION / 100.0) * num_of_data)\n",
    "    num_tests = int((PERCENT_TEST / 100.0) * num_of_data)\n",
    "\n",
    "    data_fractions = [num_train, num_valid, num_tests]\n",
    "    split_data = [[], [], []]\n",
    "\n",
    "    rand_data_ind = 0\n",
    "\n",
    "    for split_ind, fraction in enumerate(data_fractions):\n",
    "        for i in range(fraction):\n",
    "            rand_data_ind = random.randint(0, len(data) - 1)\n",
    "            split_data[split_ind].append(data[rand_data_ind])\n",
    "            data.pop(rand_data_ind)\n",
    "\n",
    "    for l in split_data[0]:\n",
    "        train_file.write(l)\n",
    "\n",
    "    for l in split_data[1]:\n",
    "        valid_file.write(l)\n",
    "\n",
    "    for l in split_data[2]:\n",
    "        tests_file.write(l)\n",
    "\n",
    "    train_file.close()\n",
    "    valid_file.close()\n",
    "    tests_file.close()\n",
    "\n",
    "\n",
    "def write_to_s3(fobj, bucket, key):\n",
    "    return (\n",
    "        boto3.Session(region_name=region)\n",
    "        .resource(\"s3\")\n",
    "        .Bucket(bucket)\n",
    "        .Object(key)\n",
    "        .upload_fileobj(fobj)\n",
    "    )\n",
    "\n",
    "\n",
    "def upload_to_s3(bucket, channel, filename):\n",
    "    fobj = open(filename, \"rb\")\n",
    "    key = f\"{prefix}/{channel}\"\n",
    "    url = f\"s3://{bucket}/{key}/{filename}\"\n",
    "    print(f\"Writing to {url}\")\n",
    "    write_to_s3(fobj, bucket, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "### Data ingestion\n",
    "\n",
    "Next, we read the dataset from the existing repository into memory, for preprocessing prior to training. This processing could be done *in situ* by Amazon Athena, Apache Spark in Amazon EMR, Amazon Redshift, etc., assuming the dataset is present in the appropriate location. Then, the next step would be to transfer the data to S3 for use in training. For small datasets, such as this one, reading into memory isn't onerous, though it would be for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "bucket = output_bucket\n",
    "prefix = \"sagemaker/DEMO-xgboost-abalone-v3\"\n",
    "\n",
    "# Load the dataset\n",
    "FILE_DATA = \"abalone\"\n",
    "s3.download_file(\n",
    "    f\"sagemaker-example-files-prod-{region}\",\n",
    "    f\"datasets/tabular/uci_abalone/abalone.libsvm\",\n",
    "    FILE_DATA,\n",
    ")\n",
    "\n",
    "# split the downloaded data into train/test/validation files\n",
    "FILE_TRAIN = \"abalone.train\"\n",
    "FILE_VALIDATION = \"abalone.validation\"\n",
    "FILE_TEST = \"abalone.test\"\n",
    "PERCENT_TRAIN = 70\n",
    "PERCENT_VALIDATION = 15\n",
    "PERCENT_TEST = 15\n",
    "data_split(\n",
    "    FILE_DATA,\n",
    "    FILE_TRAIN,\n",
    "    FILE_VALIDATION,\n",
    "    FILE_TEST,\n",
    "    PERCENT_TRAIN,\n",
    "    PERCENT_VALIDATION,\n",
    "    PERCENT_TEST,\n",
    ")\n",
    "\n",
    "# upload the files to the S3 bucket\n",
    "upload_to_s3(bucket, \"train\", FILE_TRAIN)\n",
    "upload_to_s3(bucket, \"validation\", FILE_VALIDATION)\n",
    "upload_to_s3(bucket, \"test\", FILE_TEST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
