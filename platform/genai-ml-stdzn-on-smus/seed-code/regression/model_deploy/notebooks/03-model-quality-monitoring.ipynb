{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf5362e4-c533-4bb8-a435-36eb10ab63ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T14:28:56.044081Z",
     "iopub.status.busy": "2025-09-23T14:28:56.043810Z",
     "iopub.status.idle": "2025-09-23T14:28:56.054201Z",
     "shell.execute_reply": "2025-09-23T14:28:56.052715Z",
     "shell.execute_reply.started": "2025-09-23T14:28:56.044062Z"
    }
   },
   "source": [
    "# Part 2 ‚Äî Model Quality Monitoring\n",
    "\n",
    "<div class=\"alert alert-warning\"> This notebook has been last tested on a SageMaker Studio JupyterLab instance using the <code>SageMaker Distribution Image 3.0.1</code> and with the SageMaker Python SDK version <code>2.245.0</code></div>\n",
    "\n",
    "In this notebook you are going to use [Amazon SageMaker model monitor](https://aws.amazon.com/sagemaker/model-monitor/) to add continuous and automated [monitoring of the model quality](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality.html) for the traffic to your real-time SageMaker inference endpoints. You also implement [model monitoring](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality.html) to detect performance drift and model metric anomalies.\n",
    "\n",
    "Using Model Monitor integration with [Amazon EventBridge](https://aws.amazon.com/eventbridge/) you can implement automated response and remediation to any detected issues with data and model quality. For example, you can launch an automated model retraining if the model performance falls below a specific threshold.\n",
    "\n",
    "Additionally to data and model quality monitoring you can implement [bias drift](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-model-monitor-bias-drift.html) and [feature attribution drift](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-model-monitor-feature-attribution-drift.html) monitoring.\n",
    "    \n",
    "##  Context\n",
    "\n",
    "In this deployment module, you have:\n",
    "1. ‚úÖ **Pre-provisioned**: SageMaker Unified Studio domain with registered models\n",
    "2. ‚úÖ **Deployed**: SageMaker endpoint with data capture enabled - preprovsioned \n",
    "3. ‚úÖ **Tested**: Basic endpoint functionality in the previous notebook\n",
    "4. üéØ **Now**: Set up comprehensive model monitoring\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Lab 5.1: Model approved and triggered CDK for endpoint deployment\n",
    "- SageMaker endpoint with data capture enabled\n",
    "- **IAM Permissions**: Your execution role must have Model Monitor permissions\n",
    "\n",
    "### ‚ö†Ô∏è Important: IAM Permissions Required\n",
    "\n",
    "Your IAM role needs these additional permissions for Model Monitor:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"sagemaker:CreateDataQualityJobDefinition\",\n",
    "                \"sagemaker:CreateModelQualityJobDefinition\",\n",
    "                \"sagemaker:CreateMonitoringSchedule\",\n",
    "                \"sagemaker:DescribeMonitoringSchedule\",\n",
    "                \"sagemaker:ListMonitoringSchedules\",\n",
    "                \"sagemaker:StopMonitoringSchedule\",\n",
    "                \"sagemaker:DeleteMonitoringSchedule\",\n",
    "                \"sagemaker:CreateProcessingJob\",\n",
    "                \"sagemaker:DescribeProcessingJob\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**If you get AccessDeniedException errors:**\n",
    "1. Go to AWS Console ‚Üí IAM ‚Üí Roles\n",
    "2. Find your execution role (shown in setup section below)\n",
    "3. Add the above permissions as an inline policy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e6510-6b9f-40b7-a93b-4c2364250287",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> üí°\n",
    "This notebook contains scripts for :<br/>\n",
    "- Part 2: Monitor Model quality<br/>\n",
    "\n",
    "<br/>\n",
    "\n",
    "You need approximately between 40 and 60 minutes to go through this notebook. To optimize time you can execute both parts independently. For both parts you must execute all following sections up to the <strong>Part 1</storng>.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\"> Make sure you using <code>Python 3</code> kernel in JupyterLab for this notebook.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5489b54f",
   "metadata": {},
   "source": [
    "## 1) Environment & configuration\n",
    "\n",
    "This cell sets up AWS SDK clients, your SageMaker session/role, and base S3 locations we will use throughout the notebook.\n",
    "\n",
    "**Why this matters:** Model Monitor writes/reads artifacts from S3 and needs an execution role. Keep track of:\n",
    "- **Bucket**: where baselines, reports, and captured data live\n",
    "- **Prefix**: root path inside the bucket\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07d4cc3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> üí°\n",
    "This workshop uses **Python 3** kernel in SageMaker Studio JupyterLab.  \n",
    "If you see import errors, select the **Python 3 (Data Science)** kernel and **Restart Kernel & Clear Output**.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741189cd",
   "metadata": {},
   "source": [
    "## Model Monitoring Architecture (quick refresher)\n",
    "\n",
    "1. **Data capture** writes requests/response payloads from your real-time endpoint to S3.  \n",
    "2. **Baselines** compute reference metrics/statistics from a labeled validation set.  \n",
    "3. A **monitoring schedule** runs on a cadence (e.g., hourly) to evaluate new data.  \n",
    "4. For **Model Quality**, the schedule **joins predictions with ground truth** and computes metrics (accuracy, precision/recall/F1, etc.).  \n",
    "5. **Reports & violations** are written to S3 and surfaced in CloudWatch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ec9f0b",
   "metadata": {},
   "source": [
    "### Before you begin ‚Äî parameters to verify\n",
    "\n",
    "Update the following variables in the **Setup** section if needed:\n",
    "\n",
    "```python\n",
    "endpoint_name = \"model-deploy-16-21-26-26-staging\"\n",
    "problem_type = \"BinaryClassification\"   # BinaryClassification | MulticlassClassification | Regression\n",
    "inference_attribute = \"prediction\"\n",
    "probability_attribute = \"probability\"  # classification only\n",
    "ground_truth_attribute = \"label\"\n",
    "bucket = \"<your-bucket>\"\n",
    "prefix = \"<your-prefix>\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d0a614-3cae-4af2-a624-a9f296c2ac49",
   "metadata": {},
   "source": [
    "## How Model Monitor works\n",
    "Amazon SageMaker Model Monitor automatically monitors ML models in production and notifies you when quality issues arise. Model Monitor uses rules to detect drift in your models and data and alerts you when it happens. The following figure shows how this process works.\n",
    "\n",
    "![](img/data-monitoring-architecture.png)\n",
    "\n",
    "The process for setting up and using the data monitoring:\n",
    "1. Enable the SageMaker endpoint to capture data from incoming requests to a trained ML model and the resulting model predictions\n",
    "2. Create a baseline from the dataset that was used to train the model. The baseline computes metrics and suggests constraints for the metrics. \n",
    "3. Create a monitoring schedule specifying what data to collect, how often to collect it, and how to analyze it. Data traffic to your model and predictions from the model are compared to the constraints, and are reported as violations if they are outside the constrained values. You can define multiple monitoring schedule per endpoint\n",
    "4. Inspect the reports, which compare the latest data with the baseline, and watch for any violations reported and for metrics and notifications from Amazon CloudWatch\n",
    "5. Implement observability for your ML models with Amazon CloudWatch and event-based architecture with Amazon EventBridge. You can automate data and model updates, model retraining, and user notification based on the data and model quality events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe50a78-81aa-4415-8185-8476ee46c4f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:46:54.900251Z",
     "iopub.status.busy": "2025-09-23T19:46:54.899860Z",
     "iopub.status.idle": "2025-09-23T19:46:54.903442Z",
     "shell.execute_reply": "2025-09-23T19:46:54.902593Z",
     "shell.execute_reply.started": "2025-09-23T19:46:54.900206Z"
    }
   },
   "source": [
    "## Section 1 - Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b0216-439b-46c4-acdb-341ccd048606",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:47:24.752305Z",
     "iopub.status.busy": "2025-09-23T19:47:24.752013Z",
     "iopub.status.idle": "2025-09-23T19:47:24.755368Z",
     "shell.execute_reply": "2025-09-23T19:47:24.754483Z",
     "shell.execute_reply.started": "2025-09-23T19:47:24.752281Z"
    }
   },
   "source": [
    "### 1.1 Import required libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "206b3130-f16a-490c-9e7b-bf266058bb33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:12:21.295212Z",
     "iopub.status.busy": "2025-09-23T19:12:21.294942Z",
     "iopub.status.idle": "2025-09-23T19:12:24.515440Z",
     "shell.execute_reply": "2025-09-23T19:12:24.514698Z",
     "shell.execute_reply.started": "2025-09-23T19:12:21.295172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Fetched defaults config from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "RoleArn: arn:aws:iam::006230620263:role/datazone_usr_role_42utgzvtmcm8ls_aehrjybegqbp8w\n",
      "Region: us-west-2\n",
      "CPU times: user 2.68 s, sys: 165 ms, total: 2.85 s\n",
      "Wall time: 3.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import boto3\n",
    "from time import sleep\n",
    "from threading import Thread\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sagemaker import get_execution_role, session, Session, image_uris\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "from sagemaker.processing import ProcessingJob\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "session = Session()\n",
    "sm_client = boto3.client('sagemaker')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Get Execution role\n",
    "role = get_execution_role()\n",
    "print(\"RoleArn:\", role)\n",
    "\n",
    "region = session.boto_region_name\n",
    "print(\"Region:\", region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f914eada-f950-442b-8f98-3682b6521f51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:48:02.337576Z",
     "iopub.status.busy": "2025-09-23T19:48:02.337169Z",
     "iopub.status.idle": "2025-09-23T19:48:02.348054Z",
     "shell.execute_reply": "2025-09-23T19:48:02.342635Z",
     "shell.execute_reply.started": "2025-09-23T19:48:02.337541Z"
    }
   },
   "source": [
    "### 1.2 S3 bucket and prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73d933c4-7622-4fa9-a86f-69785c3e9816",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:49:33.700545Z",
     "iopub.status.busy": "2025-09-23T19:49:33.700263Z",
     "iopub.status.idle": "2025-09-23T19:49:33.705915Z",
     "shell.execute_reply": "2025-09-23T19:49:33.705146Z",
     "shell.execute_reply.started": "2025-09-23T19:49:33.700519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Bucket: amazon-sagemaker-006230620263-us-west-2-f717bf909848\n"
     ]
    }
   ],
   "source": [
    "# Setup S3 bucket\n",
    "bucket = session.default_bucket()\n",
    "print(\"S3 Bucket:\", bucket)\n",
    "prefix = \"sagemaker/Abalone-ModelQualityMonitor-20201201\"\n",
    "default_bucket_prefix = session.default_bucket_prefix\n",
    "\n",
    "if default_bucket_prefix:\n",
    "    prefix = f\"{default_bucket_prefix}/{prefix}\"\n",
    "\n",
    "##S3 prefixes\n",
    "data_capture_prefix = f\"{prefix}/datacapture\"\n",
    "s3_capture_upload_path = f\"s3://{bucket}/{data_capture_prefix}\"\n",
    "\n",
    "ground_truth_upload_path = (\n",
    "    f\"s3://{bucket}/{prefix}/ground_truth_data/{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    ")\n",
    "\n",
    "reports_prefix = f\"{prefix}/reports\"\n",
    "s3_report_path = f\"s3://{bucket}/{reports_prefix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9933fac5-fc9c-4ce7-a4c6-1672838f9438",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:49:34.474338Z",
     "iopub.status.busy": "2025-09-23T19:49:34.474059Z",
     "iopub.status.idle": "2025-09-23T19:49:34.477986Z",
     "shell.execute_reply": "2025-09-23T19:49:34.477477Z",
     "shell.execute_reply.started": "2025-09-23T19:49:34.474318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capture path: s3://amazon-sagemaker-006230620263-us-west-2-f717bf909848/dzd_51okpx3pb2okls/42utgzvtmcm8ls/dev/sagemaker/Abalone-ModelQualityMonitor-20201201/datacapture\n",
      "Ground truth path: s3://amazon-sagemaker-006230620263-us-west-2-f717bf909848/dzd_51okpx3pb2okls/42utgzvtmcm8ls/dev/sagemaker/Abalone-ModelQualityMonitor-20201201/ground_truth_data/2025-09-23-19-49-33\n",
      "Report path: s3://amazon-sagemaker-006230620263-us-west-2-f717bf909848/dzd_51okpx3pb2okls/42utgzvtmcm8ls/dev/sagemaker/Abalone-ModelQualityMonitor-20201201/reports\n"
     ]
    }
   ],
   "source": [
    "print(f\"Capture path: {s3_capture_upload_path}\")\n",
    "print(f\"Ground truth path: {ground_truth_upload_path}\")\n",
    "print(f\"Report path: {s3_report_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed7c132-d5de-4595-a0f0-7e729e17c123",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:13:11.251210Z",
     "iopub.status.busy": "2025-09-23T19:13:11.250928Z",
     "iopub.status.idle": "2025-09-23T19:13:11.263772Z",
     "shell.execute_reply": "2025-09-23T19:13:11.262781Z",
     "shell.execute_reply.started": "2025-09-23T19:13:11.251189Z"
    }
   },
   "source": [
    "##  Section 2 - Discover and Inspect Deployed Endpoint\n",
    "\n",
    "Let's find the endpoint deployed by our CDK stack and verify its data capture configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7942112e-f46b-443e-ac1d-45763ccfdc78",
   "metadata": {},
   "source": [
    "### 2. Check deployed Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faf4df8d-b2e5-469b-9a84-45e8b1168807",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:13:28.555826Z",
     "iopub.status.busy": "2025-09-23T19:13:28.555531Z",
     "iopub.status.idle": "2025-09-23T19:13:28.886121Z",
     "shell.execute_reply": "2025-09-23T19:13:28.885535Z",
     "shell.execute_reply.started": "2025-09-23T19:13:28.555791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Found 2 InService endpoint(s):\n",
      "--------------------------------------------------------------------------------\n",
      "1. dev-endpoint-20250918-141753\n",
      "   Status: InService\n",
      "   Created: 2025-09-18 13:18:26.907000+00:00\n",
      "   Data Capture: ‚úÖ Enabled\n",
      "   Capture Status: Started\n",
      "   Sampling: 100%\n",
      "   S3 Location: s3://sagemaker-model-monitor-006230620263-us-west-2-dev/data-capture\n",
      "\n",
      "2. dev-endpoint-20250828-084146\n",
      "   Status: InService\n",
      "   Created: 2025-08-28 07:42:39.695000+00:00\n",
      "   Data Capture: ‚úÖ Enabled\n",
      "   Capture Status: Started\n",
      "   Sampling: 100%\n",
      "   S3 Location: s3://sagemaker-model-monitor-006230620263-us-west-2-dev/data-capture\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_deployed_endpoints():\n",
    "    \"\"\"Get all InService endpoints with data capture enabled\"\"\"\n",
    "    \n",
    "    endpoints = []\n",
    "    \n",
    "    try:\n",
    "        response = sm_client.list_endpoints(\n",
    "            SortBy='CreationTime',\n",
    "            SortOrder='Descending',\n",
    "            MaxResults=20\n",
    "        )\n",
    "        \n",
    "        for endpoint in response['Endpoints']:\n",
    "            if endpoint['EndpointStatus'] == 'InService':\n",
    "                # Get detailed endpoint info\n",
    "                endpoint_details = sm_client.describe_endpoint(\n",
    "                    EndpointName=endpoint['EndpointName']\n",
    "                )\n",
    "                \n",
    "                # Check if data capture is enabled\n",
    "                data_capture_config = endpoint_details.get('DataCaptureConfig', {})\n",
    "                \n",
    "                endpoints.append({\n",
    "                    'name': endpoint['EndpointName'],\n",
    "                    'status': endpoint['EndpointStatus'],\n",
    "                    'creation_time': endpoint['CreationTime'],\n",
    "                    'data_capture_enabled': data_capture_config.get('EnableCapture', False),\n",
    "                    'data_capture_status': data_capture_config.get('CaptureStatus', 'Not Configured'),\n",
    "                    'sampling_percentage': data_capture_config.get('CurrentSamplingPercentage', 0),\n",
    "                    's3_uri': data_capture_config.get('DestinationS3Uri', 'Not Configured')\n",
    "                })\n",
    "        \n",
    "        return endpoints\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error listing endpoints: {e}\")\n",
    "        return []\n",
    "\n",
    "# Get endpoints\n",
    "endpoints = get_deployed_endpoints()\n",
    "\n",
    "if endpoints:\n",
    "    print(f\"üìä Found {len(endpoints)} InService endpoint(s):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, ep in enumerate(endpoints, 1):\n",
    "        print(f\"{i}. {ep['name']}\")\n",
    "        print(f\"   Status: {ep['status']}\")\n",
    "        print(f\"   Created: {ep['creation_time']}\")\n",
    "        print(f\"   Data Capture: {'‚úÖ Enabled' if ep['data_capture_enabled'] else '‚ùå Disabled'}\")\n",
    "        if ep['data_capture_enabled']:\n",
    "            print(f\"   Capture Status: {ep['data_capture_status']}\")\n",
    "            print(f\"   Sampling: {ep['sampling_percentage']}%\")\n",
    "            print(f\"   S3 Location: {ep['s3_uri']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚ùå No InService endpoints found. Please deploy an endpoint first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03392d90-a71d-43b4-9e6c-e8dc18c32c7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:16:18.997425Z",
     "iopub.status.busy": "2025-09-23T19:16:18.997151Z",
     "iopub.status.idle": "2025-09-23T19:16:19.000098Z",
     "shell.execute_reply": "2025-09-23T19:16:18.999462Z",
     "shell.execute_reply.started": "2025-09-23T19:16:18.997406Z"
    }
   },
   "source": [
    "### 2.2  Verify Captured Data is Available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb7c8088-c2ba-47f3-bc04-afa467baf681",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:13:41.989455Z",
     "iopub.status.busy": "2025-09-23T19:13:41.989171Z",
     "iopub.status.idle": "2025-09-23T19:13:41.995413Z",
     "shell.execute_reply": "2025-09-23T19:13:41.994248Z",
     "shell.execute_reply.started": "2025-09-23T19:13:41.989434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Selected endpoint for monitoring: dev-endpoint-20250918-141753\n",
      "üìÅ Data capture location: s3://sagemaker-model-monitor-006230620263-us-west-2-dev/data-capture\n"
     ]
    }
   ],
   "source": [
    "# Select the endpoint to monitor\n",
    "if endpoints:\n",
    "    # Auto-select the first endpoint with data capture enabled\n",
    "    monitoring_endpoint = None\n",
    "    for ep in endpoints:\n",
    "        if ep['data_capture_enabled']:\n",
    "            monitoring_endpoint = ep\n",
    "            break\n",
    "    \n",
    "    if monitoring_endpoint:\n",
    "        endpoint_name = monitoring_endpoint['name']\n",
    "        data_capture_s3_uri = monitoring_endpoint['s3_uri']\n",
    "        \n",
    "        print(f\"üéØ Selected endpoint for monitoring: {endpoint_name}\")\n",
    "        print(f\"üìÅ Data capture location: {data_capture_s3_uri}\")\n",
    "    else:\n",
    "        print(\"‚ùå No endpoints with data capture enabled found.\")\n",
    "        print(\"Please ensure your CDK deployment includes data capture configuration.\")\n",
    "        endpoint_name = None\n",
    "else:\n",
    "    endpoint_name = None\n",
    "    print(\"‚ùå No endpoints available for monitoring.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d54098c-4132-4a28-996e-f520d7e5ab58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:14:30.633335Z",
     "iopub.status.busy": "2025-09-23T19:14:30.633066Z",
     "iopub.status.idle": "2025-09-23T19:14:30.798587Z",
     "shell.execute_reply": "2025-09-23T19:14:30.797777Z",
     "shell.execute_reply.started": "2025-09-23T19:14:30.633316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the data capture configuration for the endpoint dev-endpoint-20250918-141753\n",
      "{\n",
      "  \"EnableCapture\": true,\n",
      "  \"CaptureStatus\": \"Started\",\n",
      "  \"CurrentSamplingPercentage\": 100,\n",
      "  \"DestinationS3Uri\": \"s3://sagemaker-model-monitor-006230620263-us-west-2-dev/data-capture\"\n",
      "}\n",
      "Data capture S3 url: s3://sagemaker-model-monitor-006230620263-us-west-2-dev/data-capture\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if not endpoint_name:\n",
    "    print(f\"You must have at least on endpoint with data capture configuration enabled!\")\n",
    "else:\n",
    "    print(f\"Checking the data capture configuration for the endpoint {endpoint_name}\")\n",
    "    data_capture_config = sm_client.describe_endpoint(EndpointName=endpoint_name)['DataCaptureConfig']\n",
    "    data_capture_s3_url = data_capture_config['DestinationS3Uri']\n",
    "    data_capture_bucket = data_capture_s3_url.split('/')[2]\n",
    "    data_capture_prefix = '/'.join(data_capture_s3_url.split('/')[3:])\n",
    "\n",
    "    print(json.dumps(data_capture_config, indent=2))\n",
    "    print(f\"Data capture S3 url: {data_capture_s3_url}\")\n",
    "    \n",
    "    if not data_capture_config['EnableCapture']:\n",
    "        print(f\"Data capture config for the endpoint {endpoint_name} IS NOT ENABLED. You need to enable data capture for monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa90f039-97f8-42a2-887f-a2ce272c5be3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:14:37.160921Z",
     "iopub.status.busy": "2025-09-23T19:14:37.160657Z",
     "iopub.status.idle": "2025-09-23T19:14:37.167019Z",
     "shell.execute_reply": "2025-09-23T19:14:37.166134Z",
     "shell.execute_reply.started": "2025-09-23T19:14:37.160901Z"
    }
   },
   "source": [
    "### 2.3 Generate Test Traffic for Data Capture\n",
    "\n",
    "Before setting up monitoring, we need to generate some inference traffic to create captured data that we can use for baseline creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76f75b35-3d31-4650-82f3-a8cab1702856",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:15:00.624006Z",
     "iopub.status.busy": "2025-09-23T19:15:00.623683Z",
     "iopub.status.idle": "2025-09-23T19:15:00.684136Z",
     "shell.execute_reply": "2025-09-23T19:15:00.683015Z",
     "shell.execute_reply.started": "2025-09-23T19:15:00.623971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data files created with 10 features!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Create test_data directory\n",
    "os.makedirs('test_data', exist_ok=True)\n",
    "os.makedirs('model', exist_ok=True)\n",
    "\n",
    "# Create sample abalone data with 10 features \n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "\n",
    "# Generate synthetic abalone features \n",
    "data = {\n",
    "    'sex_M': np.random.choice([0, 1], n_samples),\n",
    "    'sex_F': np.random.choice([0, 1], n_samples),\n",
    "    'length': np.random.uniform(0.075, 0.815, n_samples),\n",
    "    'diameter': np.random.uniform(0.055, 0.650, n_samples),\n",
    "    'height': np.random.uniform(0.000, 1.130, n_samples),\n",
    "    'whole_weight': np.random.uniform(0.002, 2.826, n_samples),\n",
    "    'shucked_weight': np.random.uniform(0.001, 1.488, n_samples),\n",
    "    'viscera_weight': np.random.uniform(0.001, 0.760, n_samples),\n",
    "    'shell_weight': np.random.uniform(0.002, 1.005, n_samples),\n",
    "    'rings': np.random.randint(1, 30, n_samples)\n",
    "}\n",
    "\n",
    "# Create binary age labels (1 if rings > 10, 0 otherwise)\n",
    "age_labels = (data['rings'] > 10).astype(int)\n",
    "\n",
    "# Create validation.csv (label first, then features)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "validation_data = []\n",
    "for i in range(len(df)):\n",
    "    row = f\"{age_labels[i]},\" + \",\".join(map(str, df.iloc[i].values))\n",
    "    validation_data.append(row)\n",
    "\n",
    "with open('test_data/validation.csv', 'w') as f:\n",
    "    f.write('\\n'.join(validation_data))\n",
    "\n",
    "# Create test-dataset-input-cols.csv (features only, no labels)\n",
    "test_data = []\n",
    "for i in range(len(df)):\n",
    "    row = \",\".join(map(str, df.iloc[i].values))\n",
    "    test_data.append(row)\n",
    "\n",
    "with open('test_data/test-dataset-input-cols.csv', 'w') as f:\n",
    "    f.write('\\n'.join(test_data))\n",
    "\n",
    "# Create upload test file\n",
    "with open('test_data/upload-test-file.txt', 'w') as f:\n",
    "    f.write('test file for S3 upload')\n",
    "\n",
    "print(\"Sample data files created with 10 features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb55888e",
   "metadata": {},
   "source": [
    "### 2.4 Prepare S3 layout (baseline, ground truth, reports)\n",
    "\n",
    "We create/upload paths used by Model Monitor:\n",
    "- **Baseline data** (optional but recommended): validation set to compute baseline metrics\n",
    "- **Ground truth stream**: time-partitioned JSONL files for live monitoring\n",
    "- **Reports**: where the baseline + scheduled jobs write outputs\n",
    "\n",
    "> You will see paths like:\n",
    "- Baseline data: `s3://<bucket>/<prefix>/baseline/data/`\n",
    "- Baseline results: `s3://<bucket>/<prefix>/baseline/results/`\n",
    "- Ground truth: `s3://<bucket>/<prefix>/ground_truth_data/YYYY/MM/DD/HH/mmss.jsonl`\n",
    "- Reports: `s3://<bucket>/<prefix>/reports/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bf40091-af59-4068-b777-fadf27b84f63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:15:33.311609Z",
     "iopub.status.busy": "2025-09-23T19:15:33.311154Z",
     "iopub.status.idle": "2025-09-23T19:15:58.462593Z",
     "shell.execute_reply": "2025-09-23T19:15:58.461777Z",
     "shell.execute_reply.started": "2025-09-23T19:15:33.311584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................................\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name, sagemaker_session=session, serializer=CSVSerializer()\n",
    ")\n",
    "\n",
    "age_cutoff = 0.5  # Use 0.5 as threshold for binary classification\n",
    "validate_dataset = \"validation_with_predictions.csv\"\n",
    "\n",
    "limit = 50\n",
    "i = 0\n",
    "with open(f\"test_data/{validate_dataset}\", \"w\") as baseline_file:\n",
    "    baseline_file.write(\"probability,prediction,label\\n\")\n",
    "    with open(\"test_data/validation.csv\", \"r\") as f:\n",
    "        for row in f:\n",
    "            (label, input_cols) = row.split(\",\", 1)\n",
    "            try:\n",
    "                probability = float(predictor.predict(input_cols))\n",
    "                prediction = \"1\" if probability > age_cutoff else \"0\"\n",
    "                baseline_file.write(f\"{probability},{prediction},{label}\\n\")\n",
    "                i += 1\n",
    "                if i >= limit:\n",
    "                    break\n",
    "                print(\".\", end=\"\", flush=True)\n",
    "                sleep(0.5)\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting: {e}\")\n",
    "                continue\n",
    "print()\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf97b6e9-4ff0-48ac-98b0-271a7528ac85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:16:37.633599Z",
     "iopub.status.busy": "2025-09-23T19:16:37.633330Z",
     "iopub.status.idle": "2025-09-23T19:16:38.314352Z",
     "shell.execute_reply": "2025-09-23T19:16:38.313645Z",
     "shell.execute_reply.started": "2025-09-23T19:16:37.633579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "Success! You are all set to proceed.\n"
     ]
    }
   ],
   "source": [
    "# Upload some test files\n",
    "S3Uploader.upload(\"test_data/upload-test-file.txt\", f\"s3://{bucket}/test_upload\")\n",
    "print(\"Success! You are all set to proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9dde74-f635-4b82-a08c-d05fe5691db6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:52:13.025615Z",
     "iopub.status.busy": "2025-09-23T19:52:13.024468Z",
     "iopub.status.idle": "2025-09-23T19:52:13.035429Z",
     "shell.execute_reply": "2025-09-23T19:52:13.031336Z",
     "shell.execute_reply.started": "2025-09-23T19:52:13.025575Z"
    }
   },
   "source": [
    "### 2.5  Generate baseline predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22244337-db3f-47b7-be4b-738e6fb897b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:19:27.635833Z",
     "iopub.status.busy": "2025-09-23T19:19:27.635476Z",
     "iopub.status.idle": "2025-09-23T19:19:28.098784Z",
     "shell.execute_reply": "2025-09-23T19:19:28.098037Z",
     "shell.execute_reply.started": "2025-09-23T19:19:27.635809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline data uri: s3://amazon-sagemaker-006230620263-us-west-2-f717bf909848/dzd_51okpx3pb2okls/42utgzvtmcm8ls/dev/sagemaker/Abalone-ModelQualityMonitor-20201201/baselining/data\n",
      "Baseline results uri: s3://amazon-sagemaker-006230620263-us-west-2-f717bf909848/dzd_51okpx3pb2okls/42utgzvtmcm8ls/dev/sagemaker/Abalone-ModelQualityMonitor-20201201/baselining/results\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://amazon-sagemaker-006230620263-us-west-2-f717bf909848/dzd_51okpx3pb2okls/42utgzvtmcm8ls/dev/sagemaker/Abalone-ModelQualityMonitor-20201201/baselining/data/validation_with_predictions.csv'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate baseline predictions\n",
    "baseline_prefix = prefix + \"/baselining\"\n",
    "baseline_data_prefix = baseline_prefix + \"/data\"\n",
    "baseline_results_prefix = baseline_prefix + \"/results\"\n",
    "\n",
    "baseline_data_uri = f\"s3://{bucket}/{baseline_data_prefix}\"\n",
    "baseline_results_uri = f\"s3://{bucket}/{baseline_results_prefix}\"\n",
    "print(f\"Baseline data uri: {baseline_data_uri}\")\n",
    "print(f\"Baseline results uri: {baseline_results_uri}\")\n",
    "\n",
    "baseline_dataset_uri = S3Uploader.upload(f\"test_data/{validate_dataset}\", baseline_data_uri)\n",
    "baseline_dataset_uri\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47f5d06",
   "metadata": {},
   "source": [
    "## Section 3 - Create a baselining job with validation dataset predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587c5102-f438-4152-8e72-17ed743d256f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:53:36.294579Z",
     "iopub.status.busy": "2025-09-23T19:53:36.294299Z",
     "iopub.status.idle": "2025-09-23T19:53:36.297861Z",
     "shell.execute_reply": "2025-09-23T19:53:36.296957Z",
     "shell.execute_reply.started": "2025-09-23T19:53:36.294559Z"
    }
   },
   "source": [
    "### 3.1 Create a baselining job with validation dataset predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53165dea-48d6-4722-befa-a9f5aae402c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:19:53.991545Z",
     "iopub.status.busy": "2025-09-23T19:19:53.991137Z",
     "iopub.status.idle": "2025-09-23T19:19:54.896219Z",
     "shell.execute_reply": "2025-09-23T19:19:54.895501Z",
     "shell.execute_reply.started": "2025-09-23T19:19:53.991520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.MonitoringSchedule.MonitoringScheduleConfig.MonitoringJobDefinition.NetworkConfig.VpcConfig.Subnets\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.MonitoringSchedule.MonitoringScheduleConfig.MonitoringJobDefinition.NetworkConfig.VpcConfig.SecurityGroupIds\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import ModelQualityMonitor\n",
    "from sagemaker.model_monitor import EndpointInput\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "# Create the model quality monitoring object\n",
    "abalone_model_quality_monitor = ModelQualityMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6372b78e-f2fa-4a1e-bf27-e331b056ff20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:20:38.733439Z",
     "iopub.status.busy": "2025-09-23T19:20:38.733173Z",
     "iopub.status.idle": "2025-09-23T19:25:43.038884Z",
     "shell.execute_reply": "2025-09-23T19:25:43.037957Z",
     "shell.execute_reply.started": "2025-09-23T19:20:38.733420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................................................!"
     ]
    }
   ],
   "source": [
    "baseline_job_name = f\"smus-AIOps--model-baseline-job-{datetime.utcnow():%Y-%m-%d-%H%M}\"\n",
    "\n",
    "# Execute the baseline suggestion job\n",
    "job = abalone_model_quality_monitor.suggest_baseline(\n",
    "    job_name=baseline_job_name,\n",
    "    baseline_dataset=baseline_dataset_uri,\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    problem_type=\"BinaryClassification\",\n",
    "    inference_attribute=\"prediction\",\n",
    "    probability_attribute=\"probability\",\n",
    "    ground_truth_attribute=\"label\",\n",
    ")\n",
    "job.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef74993",
   "metadata": {},
   "source": [
    "### 3.2 Review baseline outputs\n",
    "\n",
    "Open the baseline S3 prefix. You should find:\n",
    "- `constraints.json` and/or `statistics.json` (schema and reference thresholds)\n",
    "- `model_quality_metrics.json` (per-class metrics for classification)\n",
    "\n",
    "If files are missing, check the job logs for parsing issues (e.g., CSV header/column names).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8aed9c12-ec90-400d-bc74-bc9eb54c7809",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:25:50.955834Z",
     "iopub.status.busy": "2025-09-23T19:25:50.955560Z",
     "iopub.status.idle": "2025-09-23T19:25:51.085632Z",
     "shell.execute_reply": "2025-09-23T19:25:51.085023Z",
     "shell.execute_reply.started": "2025-09-23T19:25:50.955813Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>confusion_matrix.0.0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion_matrix.0.1</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion_matrix.1.0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confusion_matrix.1.1</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall.value</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall.standard_deviation</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision.value</th>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision.standard_deviation</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy.value</th>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy.standard_deviation</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_best_constant_classifier.value</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_best_constant_classifier.standard_deviation</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_best_constant_classifier.value</th>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_best_constant_classifier.standard_deviation</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_best_constant_classifier.value</th>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_best_constant_classifier.standard_deviation</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_positive_rate.value</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_positive_rate.standard_deviation</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_negative_rate.value</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_negative_rate.standard_deviation</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false_positive_rate.value</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false_positive_rate.standard_deviation</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false_negative_rate.value</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false_negative_rate.standard_deviation</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>receiver_operating_characteristic_curve.false_positive_rates</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.058823529411764705...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>receiver_operating_characteristic_curve.true_positive_rates</th>\n",
       "      <td>[0.0, 0.030303030303030304, 0.0606060606060606...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_recall_curve.precisions</th>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_recall_curve.recalls</th>\n",
       "      <td>[0.0, 0.030303030303030304, 0.0606060606060606...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc.value</th>\n",
       "      <td>0.557932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc.standard_deviation</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>au_prc.value</th>\n",
       "      <td>0.722022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>au_prc.standard_deviation</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f0_5.value</th>\n",
       "      <td>0.708155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f0_5.standard_deviation</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1.value</th>\n",
       "      <td>0.795181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1.standard_deviation</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2.value</th>\n",
       "      <td>0.906593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2.standard_deviation</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f0_5_best_constant_classifier.value</th>\n",
       "      <td>0.708155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f0_5_best_constant_classifier.standard_deviation</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_best_constant_classifier.value</th>\n",
       "      <td>0.795181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_best_constant_classifier.standard_deviation</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2_best_constant_classifier.value</th>\n",
       "      <td>0.906593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2_best_constant_classifier.standard_deviation</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                    0\n",
       "confusion_matrix.0.0                                                                                0\n",
       "confusion_matrix.0.1                                                                               17\n",
       "confusion_matrix.1.0                                                                                0\n",
       "confusion_matrix.1.1                                                                               33\n",
       "recall.value                                                                                      1.0\n",
       "recall.standard_deviation                                                                        None\n",
       "precision.value                                                                                  0.66\n",
       "precision.standard_deviation                                                                     None\n",
       "accuracy.value                                                                                   0.66\n",
       "accuracy.standard_deviation                                                                      None\n",
       "recall_best_constant_classifier.value                                                             1.0\n",
       "recall_best_constant_classifier.standard_deviation                                               None\n",
       "precision_best_constant_classifier.value                                                         0.66\n",
       "precision_best_constant_classifier.standard_dev...                                               None\n",
       "accuracy_best_constant_classifier.value                                                          0.66\n",
       "accuracy_best_constant_classifier.standard_devi...                                               None\n",
       "true_positive_rate.value                                                                          1.0\n",
       "true_positive_rate.standard_deviation                                                            None\n",
       "true_negative_rate.value                                                                          0.0\n",
       "true_negative_rate.standard_deviation                                                            None\n",
       "false_positive_rate.value                                                                         1.0\n",
       "false_positive_rate.standard_deviation                                                           None\n",
       "false_negative_rate.value                                                                         0.0\n",
       "false_negative_rate.standard_deviation                                                           None\n",
       "receiver_operating_characteristic_curve.false_p...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.058823529411764705...\n",
       "receiver_operating_characteristic_curve.true_po...  [0.0, 0.030303030303030304, 0.0606060606060606...\n",
       "precision_recall_curve.precisions                   [1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666...\n",
       "precision_recall_curve.recalls                      [0.0, 0.030303030303030304, 0.0606060606060606...\n",
       "auc.value                                                                                    0.557932\n",
       "auc.standard_deviation                                                                           None\n",
       "au_prc.value                                                                                 0.722022\n",
       "au_prc.standard_deviation                                                                        None\n",
       "f0_5.value                                                                                   0.708155\n",
       "f0_5.standard_deviation                                                                          None\n",
       "f1.value                                                                                     0.795181\n",
       "f1.standard_deviation                                                                            None\n",
       "f2.value                                                                                     0.906593\n",
       "f2.standard_deviation                                                                            None\n",
       "f0_5_best_constant_classifier.value                                                          0.708155\n",
       "f0_5_best_constant_classifier.standard_deviation                                                 None\n",
       "f1_best_constant_classifier.value                                                            0.795181\n",
       "f1_best_constant_classifier.standard_deviation                                                   None\n",
       "f2_best_constant_classifier.value                                                            0.906593\n",
       "f2_best_constant_classifier.standard_deviation                                                   None"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_job = abalone_model_quality_monitor.latest_baselining_job\n",
    "\n",
    "# View metrics\n",
    "binary_metrics = baseline_job.baseline_statistics().body_dict[\"binary_classification_metrics\"]\n",
    "pd.json_normalize(binary_metrics).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46905867-39fd-456a-9d2a-e85523468616",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:25:53.597579Z",
     "iopub.status.busy": "2025-09-23T19:25:53.597307Z",
     "iopub.status.idle": "2025-09-23T19:25:53.678892Z",
     "shell.execute_reply": "2025-09-23T19:25:53.678115Z",
     "shell.execute_reply.started": "2025-09-23T19:25:53.597558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>comparison_operator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>1.0</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.66</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.66</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_positive_rate</th>\n",
       "      <td>1.0</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_negative_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false_positive_rate</th>\n",
       "      <td>1.0</td>\n",
       "      <td>GreaterThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false_negative_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>GreaterThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc</th>\n",
       "      <td>0.557932</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f0_5</th>\n",
       "      <td>0.708155</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.795181</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2</th>\n",
       "      <td>0.906593</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    threshold   comparison_operator\n",
       "recall                    1.0     LessThanThreshold\n",
       "precision                0.66     LessThanThreshold\n",
       "accuracy                 0.66     LessThanThreshold\n",
       "true_positive_rate        1.0     LessThanThreshold\n",
       "true_negative_rate        0.0     LessThanThreshold\n",
       "false_positive_rate       1.0  GreaterThanThreshold\n",
       "false_negative_rate       0.0  GreaterThanThreshold\n",
       "auc                  0.557932     LessThanThreshold\n",
       "f0_5                 0.708155     LessThanThreshold\n",
       "f1                   0.795181     LessThanThreshold\n",
       "f2                   0.906593     LessThanThreshold"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View constraints\n",
    "pd.DataFrame(baseline_job.suggested_constraints().body_dict[\"binary_classification_constraints\"]).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75adb27-de4d-42a4-be4f-98be7c1de745",
   "metadata": {},
   "source": [
    "### 3.3 Generate prediction data for Model Quality Monitoring\n",
    "\n",
    "Start generating some artificial traffic. The cell below starts a thread to send some traffic to the endpoint. Note that you need to stop the kernel to terminate this thread. If there is no traffic, the monitoring jobs are marked as Failed since there is no data to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ef5351c-22d5-40fb-815f-9a43736784a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:27:26.515795Z",
     "iopub.status.busy": "2025-09-23T19:27:26.515515Z",
     "iopub.status.idle": "2025-09-23T19:27:32.167693Z",
     "shell.execute_reply": "2025-09-23T19:27:32.166760Z",
     "shell.execute_reply.started": "2025-09-23T19:27:26.515776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating requests...\n",
      "..................................................\n",
      "Completed! Sent 50 requests.\n"
     ]
    }
   ],
   "source": [
    "def generate_traffic():\n",
    "    print(\"Generating requests...\")\n",
    "    \n",
    "    with open(\"test_data/test-dataset-input-cols.csv\", \"r\") as f:\n",
    "        rows = f.readlines()\n",
    "    \n",
    "    for i in range(50):\n",
    "        try:\n",
    "            row_index = i % len(rows)\n",
    "            payload = rows[row_index].rstrip(\"\\n\")\n",
    "            response = session.sagemaker_runtime_client.invoke_endpoint(\n",
    "                EndpointName=endpoint_name,\n",
    "                ContentType=\"text/csv\",\n",
    "                Body=payload,\n",
    "                InferenceId=str(i),\n",
    "            )[\"Body\"].read()\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            sleep(0.1)\n",
    "        except Exception as e:\n",
    "            print(f\"Request {i} failed: {e}\")\n",
    "    \n",
    "    print(f\"\\nCompleted! Sent 50 requests.\")\n",
    "\n",
    "generate_traffic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0fe9d0-d999-4f7f-aa96-8adc54eb2528",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:29:32.309805Z",
     "iopub.status.busy": "2025-09-23T19:29:32.309535Z",
     "iopub.status.idle": "2025-09-23T19:29:32.315935Z",
     "shell.execute_reply": "2025-09-23T19:29:32.315215Z",
     "shell.execute_reply.started": "2025-09-23T19:29:32.309786Z"
    }
   },
   "source": [
    "### 3.3 View captured data\n",
    "Now list the data capture files stored in Amazon S3. You should expect to see different files from different time periods organized based on the hour in which the invocation occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45e00c6a-09ee-4e80-b22e-fa213cc682c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:30:00.745977Z",
     "iopub.status.busy": "2025-09-23T19:30:00.745712Z",
     "iopub.status.idle": "2025-09-23T19:30:03.289883Z",
     "shell.execute_reply": "2025-09-23T19:30:03.289011Z",
     "shell.execute_reply.started": "2025-09-23T19:30:00.745957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "Found 18 captured files\n",
      "Latest file: 27-26-569-7aab0f89-fece-473d-b960-e1f1c6554762.jsonl\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "Records in file: 50\n",
      "Sample record:\n",
      "  Input: 0.0,0.0,0.5916396683746113,0.16515409265897868,0.5865624172039263,0.7410568508701986,1.2315206371720726,0.5115815726045536,0.2893791075739043,10.0\n",
      "  Output: 13.791704177856445\n",
      "  InferenceId: 0\n"
     ]
    }
   ],
   "source": [
    "# Check captured data\n",
    "import time\n",
    "time.sleep(30)  # Wait for data capture to process\n",
    "\n",
    "try:\n",
    "    # List captured files\n",
    "    captured_files = S3Downloader.list(f\"{data_capture_s3_uri}/{endpoint_name}\")\n",
    "    print(f\"Found {len(captured_files)} captured files\")\n",
    "    \n",
    "    if captured_files:\n",
    "        # Get latest file\n",
    "        latest_file = sorted(captured_files)[-1]\n",
    "        print(f\"Latest file: {latest_file.split('/')[-1]}\")\n",
    "        \n",
    "        # Read and show content\n",
    "        content = S3Downloader.read_file(latest_file)\n",
    "        lines = content.strip().split('\\n')\n",
    "        print(f\"Records in file: {len(lines)}\")\n",
    "        \n",
    "        # Show first record\n",
    "        if lines:\n",
    "            record = json.loads(lines[0])\n",
    "            input_data = record['captureData']['endpointInput']['data']\n",
    "            output_data = record['captureData']['endpointOutput']['data']\n",
    "            inference_id = record['eventMetadata'].get('inferenceId', 'N/A')\n",
    "            \n",
    "            print(f\"Sample record:\")\n",
    "            print(f\"  Input: {input_data}\")\n",
    "            print(f\"  Output: {output_data}\")\n",
    "            print(f\"  InferenceId: {inference_id}\")\n",
    "    else:\n",
    "        print(\"No captured files found yet\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error checking captured data: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19d58e1-f00b-45ea-889f-bf63b799b587",
   "metadata": {},
   "source": [
    "### 3.4 Generate synthetic ground truth\n",
    "Next, start generating ground truth data. The model quality job will fail if there's no ground truth data to merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa8b336e-2ad7-431b-b382-ec3e2ad2f7de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:31:35.960395Z",
     "iopub.status.busy": "2025-09-23T19:31:35.960119Z",
     "iopub.status.idle": "2025-09-23T19:31:35.982456Z",
     "shell.execute_reply": "2025-09-23T19:31:35.981600Z",
     "shell.execute_reply.started": "2025-09-23T19:31:35.960374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading 334 records to s3://amazon-sagemaker-006230620263-us-west-2-f717bf909848/dzd_51okpx3pb2okls/42utgzvtmcm8ls/dev/sagemaker/Abalone-ModelQualityMonitor-20201201/ground_truth_data/2025-09-23-19-12-49/2025/09/23/19/3135.jsonl\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def ground_truth_with_id(inference_id):\n",
    "    random.seed(inference_id)\n",
    "    rand = random.random()\n",
    "    return {\n",
    "        \"groundTruthData\": {\n",
    "            \"data\": \"1\" if rand < 0.3 else \"0\",\n",
    "            \"encoding\": \"CSV\",\n",
    "        },\n",
    "        \"eventMetadata\": {\n",
    "            \"eventId\": str(inference_id),\n",
    "        },\n",
    "        \"eventVersion\": \"0\",\n",
    "    }\n",
    "\n",
    "def upload_ground_truth(records, upload_time):\n",
    "    fake_records = [json.dumps(r) for r in records]\n",
    "    data_to_upload = \"\\n\".join(fake_records)\n",
    "    target_s3_uri = f\"{ground_truth_upload_path}/{upload_time:%Y/%m/%d/%H/%M%S}.jsonl\"\n",
    "    print(f\"Uploading {len(fake_records)} records to\", target_s3_uri)\n",
    "    S3Uploader.upload_string_as_file_body(data_to_upload, target_s3_uri)\n",
    "\n",
    "NUM_GROUND_TRUTH_RECORDS = 334\n",
    "\n",
    "def generate_fake_ground_truth_forever():\n",
    "    while True:\n",
    "        fake_records = [ground_truth_with_id(i) for i in range(NUM_GROUND_TRUTH_RECORDS)]\n",
    "        upload_ground_truth(fake_records, datetime.utcnow())\n",
    "        sleep(60 * 60)\n",
    "\n",
    "gt_thread = Thread(target=generate_fake_ground_truth_forever)\n",
    "gt_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34813c92-4140-4d94-b911-c8898315edf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:32:00.492008Z",
     "iopub.status.busy": "2025-09-23T19:32:00.491745Z",
     "iopub.status.idle": "2025-09-23T19:32:00.497806Z",
     "shell.execute_reply": "2025-09-23T19:32:00.496777Z",
     "shell.execute_reply.started": "2025-09-23T19:32:00.491989Z"
    }
   },
   "source": [
    "### 3.5 Create a monitoring schedule\n",
    "Now that you have the baseline information and ground truth labels, create a monitoring schedule to run model quality monitoring job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "671b37ce-5445-434f-98c2-0ad8cd0cf30a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:39:28.763983Z",
     "iopub.status.busy": "2025-09-23T19:39:28.763712Z",
     "iopub.status.idle": "2025-09-23T19:39:29.941387Z",
     "shell.execute_reply": "2025-09-23T19:39:29.940611Z",
     "shell.execute_reply.started": "2025-09-23T19:39:28.763963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New monitoring schedule created: None\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "\n",
    "\n",
    "monitor_schedule_name = (\n",
    "    f\"smus-AiOps-ModelQuality-monitoring-schedule-{datetime.utcnow():%Y-%m-%d-%H%M}\"\n",
    ")\n",
    "endpointInput = EndpointInput(\n",
    "    endpoint_name=endpoint_name,\n",
    "    probability_attribute=\"0\",\n",
    "    probability_threshold_attribute=0.5,\n",
    "    destination=\"/opt/ml/processing/input_data\",\n",
    ")\n",
    "\n",
    "# Now create the new schedule\n",
    "response = abalone_model_quality_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=monitor_schedule_name,\n",
    "    endpoint_input=endpointInput,\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    problem_type=\"BinaryClassification\",\n",
    "    ground_truth_input=ground_truth_upload_path,\n",
    "    constraints=baseline_job.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")\n",
    "\n",
    "print(f\"New monitoring schedule created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c0cfafa-0dc7-472f-8f57-d4ac54fbc7c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:40:20.826160Z",
     "iopub.status.busy": "2025-09-23T19:40:20.825889Z",
     "iopub.status.idle": "2025-09-23T19:40:20.984425Z",
     "shell.execute_reply": "2025-09-23T19:40:20.983793Z",
     "shell.execute_reply.started": "2025-09-23T19:40:20.826141Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MonitoringScheduleArn': 'arn:aws:sagemaker:us-west-2:006230620263:monitoring-schedule/smus-AiOps-ModelQuality-monitoring-schedule-2025-09-23-1939',\n",
       " 'MonitoringScheduleName': 'smus-AiOps-ModelQuality-monitoring-schedule-2025-09-23-1939',\n",
       " 'MonitoringScheduleStatus': 'Scheduled',\n",
       " 'MonitoringType': 'ModelQuality',\n",
       " 'CreationTime': datetime.datetime(2025, 9, 23, 19, 39, 29, 690000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2025, 9, 23, 19, 39, 34, 577000, tzinfo=tzlocal()),\n",
       " 'MonitoringScheduleConfig': {'ScheduleConfig': {'ScheduleExpression': 'cron(0 * ? * * *)'},\n",
       "  'MonitoringJobDefinitionName': 'model-quality-job-definition-2025-09-23-19-39-28-855',\n",
       "  'MonitoringType': 'ModelQuality'},\n",
       " 'EndpointName': 'dev-endpoint-20250918-141753',\n",
       " 'ResponseMetadata': {'RequestId': '8e45c127-cdd3-4ca2-9e01-ca98fef9d6db',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '8e45c127-cdd3-4ca2-9e01-ca98fef9d6db',\n",
       "   'strict-transport-security': 'max-age=47304000; includeSubDomains',\n",
       "   'x-frame-options': 'DENY',\n",
       "   'content-security-policy': \"frame-ancestors 'none'\",\n",
       "   'cache-control': 'no-cache, no-store, must-revalidate',\n",
       "   'x-content-type-options': 'nosniff',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '625',\n",
       "   'date': 'Tue, 23 Sep 2025 19:40:20 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You will see the monitoring schedule in the 'Scheduled' status\n",
    "abalone_model_quality_monitor.describe_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1c8061-c11e-4682-a79e-7ddf2ef0307b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:40:41.458013Z",
     "iopub.status.busy": "2025-09-23T19:40:41.457627Z",
     "iopub.status.idle": "2025-09-23T19:40:41.460910Z",
     "shell.execute_reply": "2025-09-23T19:40:41.460341Z",
     "shell.execute_reply.started": "2025-09-23T19:40:41.457987Z"
    }
   },
   "source": [
    "### 3.6 Examine monitoring schedule executions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c59b0fef-0214-4539-a9c0-8ec680c5e7ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:41:06.798314Z",
     "iopub.status.busy": "2025-09-23T19:41:06.798010Z",
     "iopub.status.idle": "2025-09-23T19:41:06.869014Z",
     "shell.execute_reply": "2025-09-23T19:41:06.868263Z",
     "shell.execute_reply.started": "2025-09-23T19:41:06.798290Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initially there will be no executions since the first execution happens at the top of the hour\n",
    "# Note that it is common for the execution to luanch upto 20 min after the hour.\n",
    "executions = abalone_model_quality_monitor.list_executions()\n",
    "executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6305720-c87d-4ebd-854a-e240ae48d1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the first execution of the monitoring_schedule\n",
    "print(\"Waiting for first execution\", end=\"\")\n",
    "while True:\n",
    "    execution = abalone_model_quality_monitor.describe_schedule().get(\n",
    "        \"LastMonitoringExecutionSummary\"\n",
    "    )\n",
    "    if execution:\n",
    "        break\n",
    "    print(\".\", end=\"\", flush=True)\n",
    "    sleep(10)\n",
    "print()\n",
    "print(\"Execution found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea2a4cb-0b6b-4c0f-9f74-bfc056916586",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_execution = churn_model_quality_monitor.list_executions()[-1]\n",
    "report_uri = latest_execution.describe()[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\n",
    "    \"S3Uri\"\n",
    "]\n",
    "print(\"Report Uri:\", report_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef93475-b90c-4338-bee6-38bbfd6be5c2",
   "metadata": {},
   "source": [
    "### 3.7 View violations generated by monitoring schedule\n",
    "If there are any violations compared to the baseline, they will be listed in the reports uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684e4cdd-aed7-48c1-97ae-e086643bc459",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = None\n",
    "violations = latest_execution.constraint_violations().body_dict[\"violations\"]\n",
    "violations_df = pd.json_normalize(violations)\n",
    "violations_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04367ab6",
   "metadata": {},
   "source": [
    "## Section 4 - Cleanup\n",
    "\n",
    "Stop the monitoring schedule and delete any sample endpoint/models you created to avoid charges.  \n",
    "Also consider emptying the test S3 prefixes if you no longer need the artifacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbfd9f5-3064-4cf6-af47-b67e16e5f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_model_quality_monitor.delete_monitoring_schedule()\n",
    "sleep(60)  # actually wait for the deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3135123b",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "- **\"No ground truth found\"**: Ensure your JSONL files are uploaded under the expected time partition and the keys (ID/timestamp) align with captured inference records.\n",
    "- **\"Could not parse columns\"**: Verify your schema and that `inference_attribute`, `probability_attribute`, and `ground_truth_attribute` match the column names in your files.\n",
    "- **\"No data capture path\"**: Re-create the endpoint config with data capture enabled and redeploy, or select a different endpoint that has capture on.\n",
    "- **Permissions**: The execution role must allow `s3:PutObject`, `s3:GetObject`, `logs:*`, and `sagemaker:*` for jobs/schedules.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
