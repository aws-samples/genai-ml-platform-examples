{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Model Monitoring \n",
    "\n",
    "<div class=\"alert alert-warning\"> This notebook has been last tested on a SageMaker Studio JupyterLab instance using the <code>SageMaker Distribution Image 3.0.1</code> and with the SageMaker Python SDK version <code>2.245.0</code></div>\n",
    "\n",
    "In this notebook you are going to use [Amazon SageMaker model monitor](https://aws.amazon.com/sagemaker/model-monitor/) to add continuous and automated [monitoring of the data quality](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-quality.html) for the traffic to your real-time SageMaker inference endpoints. You also implement [model monitoring](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality.html) to detect performance drift and model metric anomalies.\n",
    "\n",
    "Using Model Monitor integration with [Amazon EventBridge](https://aws.amazon.com/eventbridge/) you can implement automated response and remediation to any detected issues with data and model quality. For example, you can launch an automated model retraining if the model performance falls below a specific threshold.\n",
    "\n",
    "Additionally to data and model quality monitoring you can implement [bias drift](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-model-monitor-bias-drift.html) and [feature attribution drift](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-model-monitor-feature-attribution-drift.html) monitoring.\n",
    "    \n",
    "##  Context\n",
    "\n",
    "In this deployment module, you have:\n",
    "1. ‚úÖ **Pre-provisioned**: SageMaker Unified Studio domain with registered models\n",
    "2. ‚úÖ **Deployed**: SageMaker endpoint with data capture enabled - preprovsioned \n",
    "3. ‚úÖ **Tested**: Basic endpoint functionality in the previous notebook\n",
    "4. üéØ **Now**: Set up comprehensive model monitoring\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Lab 5.1: Model approved and triggered CDK for endpoint deployment\n",
    "- SageMaker endpoint with data capture enabled\n",
    "- **IAM Permissions**: Your execution role must have Model Monitor permissions\n",
    "\n",
    "### ‚ö†Ô∏è Important: IAM Permissions Required\n",
    "\n",
    "Your IAM role needs these additional permissions for Model Monitor:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"sagemaker:CreateDataQualityJobDefinition\",\n",
    "                \"sagemaker:CreateModelQualityJobDefinition\",\n",
    "                \"sagemaker:CreateMonitoringSchedule\",\n",
    "                \"sagemaker:DescribeMonitoringSchedule\",\n",
    "                \"sagemaker:ListMonitoringSchedules\",\n",
    "                \"sagemaker:StopMonitoringSchedule\",\n",
    "                \"sagemaker:DeleteMonitoringSchedule\",\n",
    "                \"sagemaker:CreateProcessingJob\",\n",
    "                \"sagemaker:DescribeProcessingJob\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**If you get AccessDeniedException errors:**\n",
    "1. Go to AWS Console ‚Üí IAM ‚Üí Roles\n",
    "2. Find your execution role (shown in setup section below)\n",
    "3. Add the above permissions as an inline policy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> üí°\n",
    "This notebook contains two parts:<br/>\n",
    "- Part 1: Monitor data quality<br/>\n",
    "- Part 2: Monitor model quality<br/>\n",
    "<br/>\n",
    "\n",
    "You need approximately between 60 and 90 minutes to go through this notebook. To optimize time you can execute both parts independently. For both parts you must execute all following sections up to the <strong>Part 1</storng>.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\"> Make sure you using <code>Python 3</code> kernel in JupyterLab for this notebook.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Monitoring Architecture\n",
    "\n",
    "Amazon SageMaker Model Monitor provides continuous monitoring capabilities:\n",
    "\n",
    "![Model Monitoring Architecture](images/model-monitoring-architecture.png)\n",
    "\n",
    "### Key Components:\n",
    "1. **Data Capture**: Real-time capture of inference requests and responses\n",
    "2. **Baseline Creation**: Statistical baseline from training data\n",
    "3. **Monitoring Jobs**: Scheduled analysis comparing live data to baseline\n",
    "4. **Violation Reports**: Automated detection of data/model drift\n",
    "5. **CloudWatch Integration**: Metrics and alerting\n",
    "6. **EventBridge Integration**: Automated responses to violations\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## How Model Monitor works\n",
    "Amazon SageMaker Model Monitor automatically monitors ML models in production and notifies you when quality issues arise. Model Monitor uses rules to detect drift in your models and data and alerts you when it happens. The following figure shows how this process works.\n",
    "\n",
    "\n",
    "The process for setting up and using the data monitoring:\n",
    "1. Enable the SageMaker endpoint to capture data from incoming requests to a trained ML model and the resulting model predictions\n",
    "2. Create a baseline from the dataset that was used to train the model. The baseline computes metrics and suggests constraints for the metrics. \n",
    "3. Create a monitoring schedule specifying what data to collect, how often to collect it, and how to analyze it. Data traffic to your model and predictions from the model are compared to the constraints, and are reported as violations if they are outside the constrained values. You can define multiple monitoring schedule per endpoint\n",
    "4. Inspect the reports, which compare the latest data with the baseline, and watch for any violations reported and for metrics and notifications from Amazon CloudWatch\n",
    "5. Implement observability for your ML models with Amazon CloudWatch and event-based architecture with Amazon EventBridge. You can automate data and model updates, model retraining, and user notification based on the data and model quality events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:40:14.201036Z",
     "iopub.status.busy": "2025-09-23T10:40:14.200711Z",
     "iopub.status.idle": "2025-09-23T10:40:17.318640Z",
     "shell.execute_reply": "2025-09-23T10:40:17.317387Z",
     "shell.execute_reply.started": "2025-09-23T10:40:14.201014Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q --use-pep517 sagemaker boto3 pandas numpy matplotlib seaborn tqdm jsonlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:40:17.320186Z",
     "iopub.status.busy": "2025-09-23T10:40:17.319915Z",
     "iopub.status.idle": "2025-09-23T10:40:22.637930Z",
     "shell.execute_reply": "2025-09-23T10:40:22.637130Z",
     "shell.execute_reply.started": "2025-09-23T10:40:17.320161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Fetched defaults config from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "‚úÖ SageMaker SDK version: 2.245.0\n",
      "‚úÖ Region: us-west-2\n",
      "‚úÖ Default bucket: amazon-sagemaker-006230620263-us-west-2-f717bf909848\n",
      "‚úÖ Role: arn:aws:iam::006230620263:role/datazone_usr_role_42utgzvtmcm8ls_aehrjybegqbp8w\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import jsonlines\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# SageMaker Model Monitor imports\n",
    "from sagemaker.model_monitor import (\n",
    "    DefaultModelMonitor,\n",
    "    DataCaptureConfig,\n",
    "    CronExpressionGenerator,\n",
    "    ModelQualityMonitor,\n",
    "    EndpointInput,\n",
    ")\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "# Initialize clients and session\n",
    "sm_client = boto3.client('sagemaker')\n",
    "s3_client = boto3.client('s3')\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# Handle execution role for both SageMaker and local environments\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    # For local development - replace with your actual SageMaker execution role ARN\n",
    "    print(\"‚ö†Ô∏è  Running locally - using default role pattern\")\n",
    "    print(\"   Update this with your actual SageMaker execution role ARN if needed\")\n",
    "    sts_client = boto3.client('sts')\n",
    "    account_id = sts_client.get_caller_identity()['Account']\n",
    "    role = f\"arn:aws:iam::{account_id}:role/SageMakerExecutionRole\"\n",
    "\n",
    "region = session.boto_region_name\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "print(f\"‚úÖ SageMaker SDK version: {sagemaker.__version__}\")\n",
    "print(f\"‚úÖ Region: {region}\")\n",
    "print(f\"‚úÖ Default bucket: {bucket}\")\n",
    "print(f\"‚úÖ Role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Discover and Inspect Deployed Endpoint\n",
    "\n",
    "First, let's find the endpoint deployed by our CDK stack and verify its data capture configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:40:27.375631Z",
     "iopub.status.busy": "2025-09-23T10:40:27.375007Z",
     "iopub.status.idle": "2025-09-23T10:40:27.696650Z",
     "shell.execute_reply": "2025-09-23T10:40:27.695593Z",
     "shell.execute_reply.started": "2025-09-23T10:40:27.375598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Found 2 InService endpoint(s):\n",
      "--------------------------------------------------------------------------------\n",
      "1. dev-endpoint-20250918-141753\n",
      "   Status: InService\n",
      "   Created: 2025-09-18 13:18:26.907000+00:00\n",
      "   Data Capture: ‚úÖ Enabled\n",
      "   Capture Status: Started\n",
      "   Sampling: 100%\n",
      "   S3 Location: s3://sagemaker-model-monitor-006230620263-us-west-2-dev/data-capture\n",
      "\n",
      "2. dev-endpoint-20250828-084146\n",
      "   Status: InService\n",
      "   Created: 2025-08-28 07:42:39.695000+00:00\n",
      "   Data Capture: ‚úÖ Enabled\n",
      "   Capture Status: Started\n",
      "   Sampling: 100%\n",
      "   S3 Location: s3://sagemaker-model-monitor-006230620263-us-west-2-dev/data-capture\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_deployed_endpoints():\n",
    "    \"\"\"Get all InService endpoints with data capture enabled\"\"\"\n",
    "    \n",
    "    endpoints = []\n",
    "    \n",
    "    try:\n",
    "        response = sm_client.list_endpoints(\n",
    "            SortBy='CreationTime',\n",
    "            SortOrder='Descending',\n",
    "            MaxResults=20\n",
    "        )\n",
    "        \n",
    "        for endpoint in response['Endpoints']:\n",
    "            if endpoint['EndpointStatus'] == 'InService':\n",
    "                # Get detailed endpoint info\n",
    "                endpoint_details = sm_client.describe_endpoint(\n",
    "                    EndpointName=endpoint['EndpointName']\n",
    "                )\n",
    "                \n",
    "                # Check if data capture is enabled\n",
    "                data_capture_config = endpoint_details.get('DataCaptureConfig', {})\n",
    "                \n",
    "                endpoints.append({\n",
    "                    'name': endpoint['EndpointName'],\n",
    "                    'status': endpoint['EndpointStatus'],\n",
    "                    'creation_time': endpoint['CreationTime'],\n",
    "                    'data_capture_enabled': data_capture_config.get('EnableCapture', False),\n",
    "                    'data_capture_status': data_capture_config.get('CaptureStatus', 'Not Configured'),\n",
    "                    'sampling_percentage': data_capture_config.get('CurrentSamplingPercentage', 0),\n",
    "                    's3_uri': data_capture_config.get('DestinationS3Uri', 'Not Configured')\n",
    "                })\n",
    "        \n",
    "        return endpoints\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error listing endpoints: {e}\")\n",
    "        return []\n",
    "\n",
    "# Get endpoints\n",
    "endpoints = get_deployed_endpoints()\n",
    "\n",
    "if endpoints:\n",
    "    print(f\"üìä Found {len(endpoints)} InService endpoint(s):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, ep in enumerate(endpoints, 1):\n",
    "        print(f\"{i}. {ep['name']}\")\n",
    "        print(f\"   Status: {ep['status']}\")\n",
    "        print(f\"   Created: {ep['creation_time']}\")\n",
    "        print(f\"   Data Capture: {'‚úÖ Enabled' if ep['data_capture_enabled'] else '‚ùå Disabled'}\")\n",
    "        if ep['data_capture_enabled']:\n",
    "            print(f\"   Capture Status: {ep['data_capture_status']}\")\n",
    "            print(f\"   Sampling: {ep['sampling_percentage']}%\")\n",
    "            print(f\"   S3 Location: {ep['s3_uri']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚ùå No InService endpoints found. Please deploy an endpoint first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:40:28.056869Z",
     "iopub.status.busy": "2025-09-23T10:40:28.056601Z",
     "iopub.status.idle": "2025-09-23T10:40:28.061503Z",
     "shell.execute_reply": "2025-09-23T10:40:28.060977Z",
     "shell.execute_reply.started": "2025-09-23T10:40:28.056849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Selected endpoint for monitoring: dev-endpoint-20250918-141753\n",
      "üìÅ Data capture location: s3://sagemaker-model-monitor-006230620263-us-west-2-dev/data-capture\n"
     ]
    }
   ],
   "source": [
    "# Select the endpoint to monitor\n",
    "if endpoints:\n",
    "    # Auto-select the first endpoint with data capture enabled\n",
    "    monitoring_endpoint = None\n",
    "    for ep in endpoints:\n",
    "        if ep['data_capture_enabled']:\n",
    "            monitoring_endpoint = ep\n",
    "            break\n",
    "    \n",
    "    if monitoring_endpoint:\n",
    "        endpoint_name = monitoring_endpoint['name']\n",
    "        data_capture_s3_uri = monitoring_endpoint['s3_uri']\n",
    "        \n",
    "        print(f\"üéØ Selected endpoint for monitoring: {endpoint_name}\")\n",
    "        print(f\"üìÅ Data capture location: {data_capture_s3_uri}\")\n",
    "    else:\n",
    "        print(\"‚ùå No endpoints with data capture enabled found.\")\n",
    "        print(\"Please ensure your CDK deployment includes data capture configuration.\")\n",
    "        endpoint_name = None\n",
    "else:\n",
    "    endpoint_name = None\n",
    "    print(\"‚ùå No endpoints available for monitoring.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:40:30.124961Z",
     "iopub.status.busy": "2025-09-23T10:40:30.124679Z",
     "iopub.status.idle": "2025-09-23T10:40:30.228202Z",
     "shell.execute_reply": "2025-09-23T10:40:30.227455Z",
     "shell.execute_reply.started": "2025-09-23T10:40:30.124937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the data capture configuration for the endpoint dev-endpoint-20250918-141753\n",
      "{\n",
      "  \"EnableCapture\": true,\n",
      "  \"CaptureStatus\": \"Started\",\n",
      "  \"CurrentSamplingPercentage\": 100,\n",
      "  \"DestinationS3Uri\": \"s3://sagemaker-model-monitor-006230620263-us-west-2-dev/data-capture\"\n",
      "}\n",
      "Data capture S3 url: s3://sagemaker-model-monitor-006230620263-us-west-2-dev/data-capture\n"
     ]
    }
   ],
   "source": [
    "# Get the data capture configuration for the endpoint\n",
    "# endpoint_name = \"model-deploy-16-21-26-26-staging\" # must be set before, but you can use any suitable endpoint\n",
    "\n",
    "if not endpoint_name:\n",
    "    print(f\"You must have at least on endpoint with data capture configuration enabled!\")\n",
    "else:\n",
    "    print(f\"Checking the data capture configuration for the endpoint {endpoint_name}\")\n",
    "    data_capture_config = sm_client.describe_endpoint(EndpointName=endpoint_name)['DataCaptureConfig']\n",
    "    data_capture_s3_url = data_capture_config['DestinationS3Uri']\n",
    "    data_capture_bucket = data_capture_s3_url.split('/')[2]\n",
    "    data_capture_prefix = '/'.join(data_capture_s3_url.split('/')[3:])\n",
    "\n",
    "    print(json.dumps(data_capture_config, indent=2))\n",
    "    print(f\"Data capture S3 url: {data_capture_s3_url}\")\n",
    "    \n",
    "    if not data_capture_config['EnableCapture']:\n",
    "        print(f\"Data capture config for the endpoint {endpoint_name} IS NOT ENABLED. You need to enable data capture for monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Test Traffic for Data Capture\n",
    "\n",
    "Before setting up monitoring, we need to generate some inference traffic to create captured data that we can use for baseline creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:40:36.230862Z",
     "iopub.status.busy": "2025-09-23T10:40:36.230597Z",
     "iopub.status.idle": "2025-09-23T10:40:40.947780Z",
     "shell.execute_reply": "2025-09-23T10:40:40.946723Z",
     "shell.execute_reply.started": "2025-09-23T10:40:36.230843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Generating 30 test requests to dev-endpoint-20250918-141753...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:04<00:00,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Traffic generation complete:\n",
      "   ‚úÖ Successful requests: 30\n",
      "   ‚ùå Failed requests: 0\n",
      "\n",
      "üí° Data capture is enabled, so these requests are now available for monitoring analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_test_traffic(endpoint_name, num_requests=50):\n",
    "    \"\"\"Generate test traffic to create captured data for monitoring\"\"\"\n",
    "    \n",
    "    if not endpoint_name:\n",
    "        print(\"‚ùå No endpoint available for traffic generation\")\n",
    "        return False\n",
    "    \n",
    "    runtime_client = boto3.client('sagemaker-runtime')\n",
    "    \n",
    "    print(f\"üöÄ Generating {num_requests} test requests to {endpoint_name}...\")\n",
    "    \n",
    "    successful_requests = 0\n",
    "    failed_requests = 0\n",
    "    \n",
    "    for i in tqdm(range(num_requests), desc=\"Sending requests\"):\n",
    "        try:\n",
    "            # Generate realistic test data for abalone dataset\n",
    "            sex = np.random.choice([0, 1, 2])\n",
    "            length = np.random.normal(0.5, 0.1)\n",
    "            diameter = length * np.random.uniform(0.7, 0.9)\n",
    "            height = length * np.random.uniform(0.15, 0.25)\n",
    "            whole_weight = length * diameter * height * np.random.uniform(8, 12)\n",
    "            shucked_weight = whole_weight * np.random.uniform(0.3, 0.5)\n",
    "            viscera_weight = whole_weight * np.random.uniform(0.1, 0.2)\n",
    "            shell_weight = whole_weight * np.random.uniform(0.2, 0.4)\n",
    "            \n",
    "            test_data = f\"{sex},{length:.3f},{diameter:.3f},{height:.3f},{whole_weight:.3f},{shucked_weight:.3f},{viscera_weight:.3f},{shell_weight:.3f},0.0,0.0\"\n",
    "            \n",
    "            # Send request\n",
    "            response = runtime_client.invoke_endpoint(\n",
    "                EndpointName=endpoint_name,\n",
    "                ContentType='text/csv',\n",
    "                Body=test_data\n",
    "            )\n",
    "            \n",
    "            successful_requests += 1\n",
    "            time.sleep(0.1)  # Small delay\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_requests += 1\n",
    "            if failed_requests <= 3:\n",
    "                print(f\"\\n‚ùå Request {i+1} failed: {e}\")\n",
    "    \n",
    "    print(f\"\\nüìä Traffic generation complete:\")\n",
    "    print(f\"   ‚úÖ Successful requests: {successful_requests}\")\n",
    "    print(f\"   ‚ùå Failed requests: {failed_requests}\")\n",
    "    \n",
    "    if successful_requests > 0:\n",
    "        print(f\"\\nüí° Data capture is enabled, so these requests are now available for monitoring analysis.\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Generate test traffic\n",
    "if endpoint_name:\n",
    "    traffic_generated = generate_test_traffic(endpoint_name, num_requests=30)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping traffic generation - no endpoint available\")\n",
    "    traffic_generated = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Captured Data is Available\n",
    "\n",
    "Let's wait a moment and then check that our traffic has been captured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:40:40.951949Z",
     "iopub.status.busy": "2025-09-23T10:40:40.951578Z",
     "iopub.status.idle": "2025-09-23T10:41:11.468345Z",
     "shell.execute_reply": "2025-09-23T10:41:11.467644Z",
     "shell.execute_reply.started": "2025-09-23T10:40:40.951924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Waiting for data capture to process requests...\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "üìä Found 15 captured data files\n",
      "üìÅ Location: s3://sagemaker-model-monitor-006230620263-us-west-2-dev/data-capture\n",
      "\n",
      "üìã Recent files (10):\n",
      "  1. 08-59-272-45c5a082-9938-4b62-b27e-d4d83d021e1f.jsonl\n",
      "  2. 26-26-810-ab62a649-66b1-4a77-8bd2-95d0d2c1d4d7.jsonl\n",
      "  3. 42-03-176-550bd082-8191-4b9a-afc1-74c5b78a6337.jsonl\n",
      "  4. 49-52-915-9f462381-9dd1-47a0-89bc-7182b01c76da.jsonl\n",
      "  5. 52-51-780-01ee6223-8f2b-46ee-91cb-ebbd36c37c62.jsonl\n",
      "  6. 34-44-026-73285976-8594-4a4e-b8dc-32f419b2feae.jsonl\n",
      "  7. 56-17-023-29dc6e17-995a-4469-b068-31192b27353d.jsonl\n",
      "  8. 20-29-410-896912ca-75f0-49d7-8785-b8467dc37e78.jsonl\n",
      "  9. 19-20-152-70e6017c-4de5-41b5-9282-2feacc6fcd47.jsonl\n",
      "  10. 56-44-921-c3cbadea-46a9-4ced-9d8a-8d4f4170c60e.jsonl\n",
      "\n",
      "‚úÖ Data capture is working! Found 10 files.\n",
      "üìä We can now proceed to create monitoring baselines using this captured data.\n"
     ]
    }
   ],
   "source": [
    "def inspect_captured_data(s3_uri, max_files=10):\n",
    "    \"\"\"Inspect captured data files in S3\"\"\"\n",
    "    \n",
    "    if not s3_uri or s3_uri == 'Not Configured':\n",
    "        print(\"‚ùå No data capture S3 URI configured\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # List captured data files\n",
    "        captured_files = S3Downloader.list(s3_uri)\n",
    "        \n",
    "        if not captured_files:\n",
    "            print(f\"üì≠ No captured data files found in {s3_uri}\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"üìä Found {len(captured_files)} captured data files\")\n",
    "        print(f\"üìÅ Location: {s3_uri}\")\n",
    "        \n",
    "        # Show recent files\n",
    "        recent_files = sorted(captured_files, reverse=True)[:max_files]\n",
    "        print(f\"\\nüìã Recent files ({len(recent_files)}):\")\n",
    "        for i, file_path in enumerate(recent_files, 1):\n",
    "            file_name = file_path.split('/')[-1]\n",
    "            print(f\"  {i}. {file_name}\")\n",
    "        \n",
    "        return recent_files\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error inspecting captured data: {e}\")\n",
    "        return []\n",
    "\n",
    "# Wait for data capture to process and then check\n",
    "if traffic_generated:\n",
    "    print(\"‚è≥ Waiting for data capture to process requests...\")\n",
    "    time.sleep(30)  # Wait for data capture\n",
    "    \n",
    "    # Check captured data\n",
    "    captured_files = inspect_captured_data(data_capture_s3_uri)\n",
    "    \n",
    "    if captured_files:\n",
    "        print(f\"\\n‚úÖ Data capture is working! Found {len(captured_files)} files.\")\n",
    "        print(\"üìä We can now proceed to create monitoring baselines using this captured data.\")\n",
    "        has_captured_data = True\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No captured data found yet. Monitoring will use synthetic baselines.\")\n",
    "        has_captured_data = False\n",
    "else:\n",
    "    has_captured_data = False\n",
    "    captured_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T10:41:11.470642Z",
     "iopub.status.busy": "2025-09-23T10:41:11.470470Z",
     "iopub.status.idle": "2025-09-23T10:41:12.046323Z",
     "shell.execute_reply": "2025-09-23T10:41:12.043399Z",
     "shell.execute_reply.started": "2025-09-23T10:41:11.470623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading latest captured file: 56-44-921-c3cbadea-46a9-4ced-9d8a-8d4f4170c60e.jsonl\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "\n",
      "üìÑ Content of the capture file:\n",
      "--------------------------------------------------------------------------------\n",
      "{\n",
      "  \"captureData\": {\n",
      "    \"endpointInput\": {\n",
      "      \"observedContentType\": \"text/csv\",\n",
      "      \"mode\": \"INPUT\",\n",
      "      \"data\": \"0,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,0.0,0.0\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    },\n",
      "    \"endpointOutput\": {\n",
      "      \"observedContentType\": \"text/csv; charset=utf-8\",\n",
      "      \"mode\": \"OUTPUT\",\n",
      "      \"data\": \"7.912803649902344\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    }\n",
      "  },\n",
      "  \"eventMetadata\": {\n",
      "    \"eventId\": \"fd57342a-e2d1-4bea-a879-3afdcd979fd7\",\n",
      "    \"inferenceTime\": \"2025-09-20T08:56:44Z\"\n",
      "  },\n",
      "  \"eventVersion\": \"0\"\n",
      "}\n",
      "\n",
      "======================================== SECOND RECORD ========================================\n",
      "{\n",
      "  \"captureData\": {\n",
      "    \"endpointInput\": {\n",
      "      \"observedContentType\": \"text/csv\",\n",
      "      \"mode\": \"INPUT\",\n",
      "      \"data\": \"0,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,0.0,0.0\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    },\n",
      "    \"endpointOutput\": {\n",
      "      \"observedContentType\": \"text/csv; charset=utf-8\",\n",
      "      \"mode\": \"OUTPUT\",\n",
      "      \"data\": \"9.499584197998047\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    }\n",
      "  },\n",
      "  \"eventMetadata\": {\n",
      "    \"eventId\": \"4d40b3f2-2709-4a05-a081-1732bb613d78\",\n",
      "    \"inferenceTime\": \"2025-09-20T08:56:45Z\"\n",
      "  },\n",
      "  \"eventVersion\": \"0\"\n",
      "}\n",
      "--------------------------------------------------------------------------------\n",
      "‚úÖ File downloaded to: ./tmp/56-44-921-c3cbadea-46a9-4ced-9d8a-8d4f4170c60e.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Download a capture data file and print its content\n",
    "if captured_files:\n",
    "    file_key = captured_files[-1]  # Get the latest file\n",
    "    local_path = \"./tmp\"\n",
    "    \n",
    "    # Create tmp directory if it doesn't exist\n",
    "    import os\n",
    "    os.makedirs(local_path, exist_ok=True)\n",
    "    \n",
    "    print(f\"üì• Downloading latest captured file: {file_key.split('/')[-1]}\")\n",
    "    \n",
    "    # Download the file\n",
    "    S3Downloader.download(file_key, local_path)\n",
    "    \n",
    "    print(f\"\\nüìÑ Content of the capture file:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Read the jsonl file and show the first object\n",
    "    import jsonlines\n",
    "    import json\n",
    "    \n",
    "    local_file_path = f\"{local_path}/{file_key.split('/')[-1]}\"\n",
    "    \n",
    "    with jsonlines.open(local_file_path) as reader:\n",
    "        first_record = reader.read()\n",
    "        print(json.dumps(first_record, indent=2))\n",
    "        \n",
    "        # Optionally show second record if available\n",
    "        try:\n",
    "            second_record = reader.read()\n",
    "            print(\"\\n\" + \"=\"*40 + \" SECOND RECORD \" + \"=\"*40)\n",
    "            print(json.dumps(second_record, indent=2))\n",
    "        except:\n",
    "            print(f\"\\n(Only one record found in file)\")\n",
    "            \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"‚úÖ File downloaded to: {local_file_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No captured files available. Run the data capture inspection cell first.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1- Data Quality Monitoring\n",
    "\n",
    "Now that we have captured data, let's set up data quality monitoring.\n",
    "\n",
    "\n",
    "In this part you learn how to setup data quality monitoring for SageMaker real-time endpoints.\n",
    "\n",
    "To enable inference data quality monitoring and evaluation you must:\n",
    "1. Enable [data capture](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-capture.html)\n",
    "1. [Create a baseline](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-create-baseline.html) with which you compare the realtime traffic\n",
    "1. Once a baseline is ready, [schedule monitoring jobs](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-scheduling.html) to continously evaluate and compare against the baseline\n",
    "1. [See and interpret the results](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-interpreting-results.html) of monitoring jobs\n",
    "1. [Integrate data quality monitoring](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-interpreting-cloudwatch.html) with Amazon CloudWatch\n",
    "\n",
    "![Data Monitoring Architecture](images/data-monitoring-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create Baseline Dataset\n",
    "\n",
    "\n",
    "The whole dataset with which you trained and tested the model is usually a good baseline dataset. Note that the baseline dataset data schema and the inference dataset schema should exactly match (i.e. the number and order of the features).\n",
    "\n",
    "From the baseline dataset you can ask Amazon SageMaker to suggest a set of baseline _constraints_ and generate descriptive _statistics_ to explore the data. Model Monitor provides a [built-in container](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-pre-built-container.html) that provides the ability to suggest the constraints automatically for CSV and flat JSON input. This `sagemaker-model-monitor-analyzer` container also provides you with a range of model monitoring capabilities, including constraint validation against a baseline, and emitting Amazon CloudWatch metrics. This container is based on Spark and is built with [Deequ](https://github.com/awslabs/deequ). \n",
    "\n",
    "<div class=\"alert alert-info\"> üí° <strong> All column names in your baseline dataset must be compliant with Spark. For column names, use only lowercase characters, and _ as the only special character. </strong>\n",
    "</div>\n",
    "\n",
    "We'll create a baseline dataset for data quality monitoring using captured data if available, or synthetic data as fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T14:10:15.905406Z",
     "iopub.status.busy": "2025-09-21T14:10:15.905104Z",
     "iopub.status.idle": "2025-09-21T14:10:16.461408Z",
     "shell.execute_reply": "2025-09-21T14:10:16.460569Z",
     "shell.execute_reply.started": "2025-09-21T14:10:15.905384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Creating baseline from captured inference data...\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "‚úÖ Extracted 12 samples from captured data\n",
      "üìä Created baseline with 12 samples\n",
      "‚úÖ Baseline dataset uploaded to: s3://amazon-sagemaker-006230620263-us-west-2-f717bf909848/model-monitor/dev-endpoint-20250918-141753/baseline/baseline.csv\n"
     ]
    }
   ],
   "source": [
    "def create_baseline_from_captured_data():\n",
    "    \"\"\"Create baseline from captured inference data if available\"\"\"\n",
    "    \n",
    "    if not has_captured_data or not captured_files:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(\"üìä Creating baseline from captured inference data...\")\n",
    "        \n",
    "        # Download and parse captured data\n",
    "        sample_file = captured_files[0]\n",
    "        captured_content = S3Downloader.read_file(sample_file)\n",
    "        \n",
    "        # Parse JSON lines and extract input data\n",
    "        baseline_data = []\n",
    "        for line in captured_content.strip().split('\\n'):\n",
    "            record = json.loads(line)\n",
    "            if 'captureData' in record and 'endpointInput' in record['captureData']:\n",
    "                input_data = record['captureData']['endpointInput']['data']\n",
    "                baseline_data.append(input_data)\n",
    "        \n",
    "        if baseline_data:\n",
    "            print(f\"‚úÖ Extracted {len(baseline_data)} samples from captured data\")\n",
    "            return baseline_data\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No input data found in captured files\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing captured data: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_synthetic_baseline():\n",
    "    \"\"\"Create synthetic baseline data as fallback\"\"\"\n",
    "    \n",
    "    print(\"üìä Creating synthetic baseline dataset...\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = 100\n",
    "    baseline_data = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        sex = np.random.choice([0, 1, 2])\n",
    "        length = np.random.normal(0.5, 0.1)\n",
    "        diameter = length * np.random.uniform(0.7, 0.9)\n",
    "        height = length * np.random.uniform(0.15, 0.25)\n",
    "        whole_weight = length * diameter * height * np.random.uniform(8, 12)\n",
    "        shucked_weight = whole_weight * np.random.uniform(0.3, 0.5)\n",
    "        viscera_weight = whole_weight * np.random.uniform(0.1, 0.2)\n",
    "        shell_weight = whole_weight * np.random.uniform(0.2, 0.4)\n",
    "        \n",
    "        baseline_data.append(f\"{sex},{length:.3f},{diameter:.3f},{height:.3f},{whole_weight:.3f},{shucked_weight:.3f},{viscera_weight:.3f},{shell_weight:.3f},0.0,0.0\")\n",
    "    \n",
    "    return baseline_data\n",
    "\n",
    "# Create baseline dataset\n",
    "baseline_data = create_baseline_from_captured_data()\n",
    "if not baseline_data:\n",
    "    baseline_data = create_synthetic_baseline()\n",
    "\n",
    "print(f\"üìä Created baseline with {len(baseline_data)} samples\")\n",
    "\n",
    "# Upload baseline dataset to S3\n",
    "baseline_s3_prefix = f\"model-monitor/{endpoint_name}/baseline\"\n",
    "baseline_s3_uri = f\"s3://{bucket}/{baseline_s3_prefix}/baseline.csv\"\n",
    "\n",
    "# Save baseline dataset\n",
    "baseline_csv = \"\\n\".join(baseline_data)\n",
    "s3_client.put_object(\n",
    "    Bucket=bucket,\n",
    "    Key=f\"{baseline_s3_prefix}/baseline.csv\",\n",
    "    Body=baseline_csv\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Baseline dataset uploaded to: {baseline_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create Baseline Job\n",
    "\n",
    "Create a baseline job to establish statistical constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T12:57:48.509671Z",
     "iopub.status.busy": "2025-09-21T12:57:48.509255Z",
     "iopub.status.idle": "2025-09-21T13:03:17.239497Z",
     "shell.execute_reply": "2025-09-21T13:03:17.238558Z",
     "shell.execute_reply.started": "2025-09-21T12:57:48.509645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.MonitoringSchedule.MonitoringScheduleConfig.MonitoringJobDefinition.NetworkConfig.VpcConfig.Subnets\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.MonitoringSchedule.MonitoringScheduleConfig.MonitoringJobDefinition.NetworkConfig.VpcConfig.SecurityGroupIds\n",
      "üöÄ Starting baseline job: data-quality-baseline-dev-endpoint-20250918-141753-1758459469\n",
      "üìä Input data: s3://amazon-sagemaker-006230620263-us-west-2-f717bf909848/model-monitor/dev-endpoint-20250918-141753/baseline/baseline.csv\n",
      "üìÅ Output location: s3://amazon-sagemaker-006230620263-us-west-2-f717bf909848/model-monitor/dev-endpoint-20250918-141753/baseline/results\n",
      ".............\u001b[34m2025-09-21 12:59:58.142093: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-09-21 12:59:58.142129: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2025-09-21 12:59:59.751254: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-09-21 12:59:59.751288: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2025-09-21 12:59:59.751310: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-38-202-150.us-west-2.compute.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2025-09-21 12:59:59.751587: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:01,377 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:006230620263:processing-job/data-quality-baseline-dev-endpoint-20250918-141753-1758459469', 'ProcessingJobName': 'data-quality-baseline-dev-endpoint-20250918-141753-1758459469', 'Environment': {'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '159807026194.dkr.ecr.us-west-2.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://amazon-sagemaker-006230620263-us-west-2-f717bf909848/model-monitor/dev-endpoint-20250918-141753/baseline/baseline.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://amazon-sagemaker-006230620263-us-west-2-f717bf909848/model-monitor/dev-endpoint-20250918-141753/baseline/results', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': {'SecurityGroupIds': ['sg-0f64dd177befa9219'], 'Subnets': ['subnet-0cd410bbb33b2ceaa', 'subnet-0a51efacd44b59211', 'subnet-016f25025c835e9db']}, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::006230620263:role/datazone_usr_role_42utgzvtmcm8ls_aehrjybegqbp8w', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:01,377 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:01,377 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:01,377 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:01,377 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:01,377 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:01,452 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:01,454 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:01,455 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0', 'topology': None}\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:01,468 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:01,468 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:01,468 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,086 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.38.202.150\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-\u001b[0m\n",
      "\u001b[34myarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_392\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,094 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,098 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-0e11bbb8-4a45-4a4d-ba1b-bb08d5267968\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,643 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,655 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,656 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,658 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,662 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,662 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,662 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,663 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,694 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,705 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,705 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,709 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,712 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Sep 21 13:00:02\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,713 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,713 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,715 INFO util.GSet: 2.0% max memory 3.1 GB = 62.9 MB\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,715 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,749 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,753 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,753 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,753 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,753 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,753 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,753 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,753 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,753 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,753 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,753 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,753 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,779 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,779 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,779 INFO util.GSet: 1.0% max memory 3.1 GB = 31.4 MB\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,779 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,781 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,781 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,781 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,781 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,785 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,788 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,789 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,789 INFO util.GSet: 0.25% max memory 3.1 GB = 7.9 MB\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,789 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,822 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,822 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,822 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,825 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,825 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,827 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,827 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,827 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 965.7 KB\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,827 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,848 INFO namenode.FSImage: Allocated new BlockPoolId: BP-260419337-10.38.202.150-1758459602842\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,861 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,867 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,945 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,958 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,962 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.38.202.150\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:02,975 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:05,035 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:05,036 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:07,104 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:07,104 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:09,190 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:09,190 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:11,281 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:11,282 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:13,379 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:13,380 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:23,385 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:25,013 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:25,490 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:25,534 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:25,550 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,169 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,195 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,195 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,196 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,196 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,234 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11384, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,248 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,250 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,299 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,300 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,300 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,300 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,301 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,626 INFO util.Utils: Successfully started service 'sparkDriver' on port 46267.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,657 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,694 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,714 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,715 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,747 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,771 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-15622d9b-d3d4-4d13-8a80-fdfa0921713b\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,789 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,829 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:26,864 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.38.202.150:46267/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1758459626163\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:27,364 INFO client.RMProxy: Connecting to ResourceManager at /10.38.202.150:8032\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:28,142 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:28,142 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:28,150 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15524 MB per container)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:28,150 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:28,151 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:28,151 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:28,158 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:28,240 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:30,485 INFO yarn.Client: Uploading resource file:/tmp/spark-4cade3d2-effd-4c3a-a0a3-17d1eeba1045/__spark_libs__4209639801987342387.zip -> hdfs://10.38.202.150/user/root/.sparkStaging/application_1758459608729_0001/__spark_libs__4209639801987342387.zip\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:31,711 INFO yarn.Client: Uploading resource file:/tmp/spark-4cade3d2-effd-4c3a-a0a3-17d1eeba1045/__spark_conf__6709528997830894629.zip -> hdfs://10.38.202.150/user/root/.sparkStaging/application_1758459608729_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:31,763 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:31,763 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:31,763 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:31,763 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:31,763 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:31,796 INFO yarn.Client: Submitting application application_1758459608729_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:32,025 INFO impl.YarnClientImpl: Submitted application application_1758459608729_0001\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:33,032 INFO yarn.Client: Application report for application_1758459608729_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:33,037 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Sun Sep 21 13:00:32 +0000 2025] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1758459631911\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1758459608729_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:34,047 INFO yarn.Client: Application report for application_1758459608729_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:35,051 INFO yarn.Client: Application report for application_1758459608729_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:36,054 INFO yarn.Client: Application report for application_1758459608729_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:37,059 INFO yarn.Client: Application report for application_1758459608729_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:37,572 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1758459608729_0001), /proxy/application_1758459608729_0001\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:38,063 INFO yarn.Client: Application report for application_1758459608729_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:38,064 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.38.202.150\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1758459631911\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1758459608729_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:38,065 INFO cluster.YarnClientSchedulerBackend: Application application_1758459608729_0001 has started running.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:38,076 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40123.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:38,076 INFO netty.NettyBlockTransferService: Server created on 10.38.202.150:40123\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:38,078 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:38,087 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.38.202.150, 40123, None)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:38,090 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.38.202.150:40123 with 1458.6 MiB RAM, BlockManagerId(driver, 10.38.202.150, 40123, None)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:38,093 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.38.202.150, 40123, None)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:38,094 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.38.202.150, 40123, None)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:38,218 INFO util.log: Logging initialized @14662ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:39,108 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:42,948 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.38.202.150:43360) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:43,140 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:45331 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 45331, None)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:57,309 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:57,522 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:57,580 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:57,586 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:58,600 INFO datasources.InMemoryFileIndex: It took 38 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:58,779 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 417.1 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:59,092 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:59,095 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.38.202.150:40123 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:59,099 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:59,498 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:59,501 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:59,505 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 1559\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:59,561 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:59,581 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:59,582 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:59,582 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:59,584 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:59,590 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:59,619 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:59,624 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:59,625 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.38.202.150:40123 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:59,626 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:59,644 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:59,645 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:59,697 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4621 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:00:59,965 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:45331 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:00,714 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:45331 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:01,035 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1356 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:01,037 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:01,044 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.432 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:01,048 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:01,048 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:01,050 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.488902 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:01,260 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.38.202.150:40123 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:01,266 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:45331 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,412 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,415 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,418 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: string, _c1: string, _c2: string, _c3: string, _c4: string ... 8 more fields>\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,614 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.7 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,631 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,632 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.38.202.150:40123 (size: 39.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,633 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,650 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,699 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,700 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,700 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,700 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,704 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,712 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,769 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,777 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,778 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.38.202.150:40123 (size: 7.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,779 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,779 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,779 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,784 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4949 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:03,857 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:45331 (size: 7.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:04,574 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:45331 (size: 39.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:04,663 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:45331 (size: 3.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:04,773 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 992 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:04,773 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:04,774 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 1.058 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:04,774 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:04,774 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:04,775 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 1.075946 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:05,035 INFO codegen.CodeGenerator: Code generated in 196.064105 ms\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:05,351 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.38.202.150:40123 in memory (size: 7.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:05,359 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:45331 in memory (size: 7.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:05,705 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:05,885 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:05,890 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:05,891 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:05,891 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:05,894 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:05,897 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:05,930 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 113.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:05,936 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:05,937 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.38.202.150:40123 (size: 34.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:05,944 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:05,946 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:05,946 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:05,956 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4938 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:05,998 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:45331 (size: 34.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,342 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1388 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,342 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,345 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.445 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,346 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,346 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,347 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,347 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,422 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,425 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,425 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,425 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,426 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,427 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,440 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 166.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,442 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 45.7 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,443 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.38.202.150:40123 (size: 45.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,443 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,444 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,444 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,447 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,471 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:45331 (size: 45.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,520 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.38.202.150:43360\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,858 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 412 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,859 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.425 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,859 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,860 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,860 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,860 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.437910 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:07,893 INFO codegen.CodeGenerator: Code generated in 23.605176 ms\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:08,232 INFO codegen.CodeGenerator: Code generated in 39.563736 ms\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:08,318 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:08,319 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:08,319 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:08,319 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:08,320 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:08,323 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:08,340 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 37.4 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:08,342 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 16.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:08,343 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.38.202.150:40123 (size: 16.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:08,344 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:08,345 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:08,345 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:08,358 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4949 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:08,378 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:45331 (size: 16.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:08,637 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 280 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:08,637 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:08,638 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 0.314 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:08,638 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:08,639 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:08,639 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 0.321497 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,189 INFO codegen.CodeGenerator: Code generated in 160.051252 ms\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,199 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,199 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,199 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,199 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,201 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,202 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,210 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 73.2 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,212 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.5 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,213 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.38.202.150:40123 (size: 23.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,214 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,215 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,215 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,217 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4938 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,235 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:45331 (size: 23.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,317 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 101 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,317 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,318 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.115 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,319 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,320 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,320 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,320 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,529 INFO codegen.CodeGenerator: Code generated in 121.369042 ms\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,543 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,544 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,545 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,545 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,545 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,546 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,554 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 66.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,556 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,559 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.38.202.150:40123 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,560 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,561 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,561 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,563 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,589 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:45331 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,597 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.38.202.150:43360\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,714 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 151 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,714 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,716 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.164 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,717 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,718 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,720 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.177482 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,843 INFO codegen.CodeGenerator: Code generated in 75.262895 ms\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,969 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,976 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,977 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,977 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,978 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,978 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,982 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,992 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 29.7 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,994 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 13.7 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,995 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.38.202.150:40123 (size: 13.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,996 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,996 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,997 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:09,998 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4938 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:10,018 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:45331 (size: 13.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,344 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 1346 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,345 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,346 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 1.363 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,347 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,348 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,348 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,348 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,349 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,351 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,353 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,354 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.38.202.150:40123 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,354 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,355 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,355 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,357 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,375 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:45331 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,384 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.38.202.150:43360\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,431 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 75 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,431 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,433 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.083 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,433 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,434 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,434 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 1.461590 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,604 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,604 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,604 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,604 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,605 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,606 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,614 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 82.5 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,616 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,619 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.38.202.150:40123 (size: 27.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,620 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,621 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,621 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,623 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4938 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,637 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:45331 (size: 27.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,832 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 210 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,832 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,833 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.224 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,833 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,833 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,833 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,833 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,912 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,913 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,914 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,914 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,914 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,915 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,933 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 167.1 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,940 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 45.7 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,941 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.38.202.150:40123 (size: 45.7 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,942 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,942 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,952 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,954 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,971 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:45331 (size: 45.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:11,989 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.38.202.150:43360\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,120 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 165 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,120 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,121 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.203 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,121 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,121 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,122 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.209681 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,249 INFO codegen.CodeGenerator: Code generated in 20.170662 ms\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,289 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,291 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,291 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,292 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,292 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,294 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,317 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 37.3 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,324 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,326 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.38.202.150:40123 (size: 15.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,327 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,328 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,328 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,331 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4949 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,346 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:45331 (size: 15.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,440 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 109 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,440 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,441 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.146 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,442 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,442 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,442 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.152498 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,833 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.38.202.150:40123 in memory (size: 13.7 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,833 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:45331 in memory (size: 13.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,896 INFO codegen.CodeGenerator: Code generated in 76.418244 ms\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,899 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:45331 in memory (size: 34.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,903 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.38.202.150:40123 in memory (size: 34.7 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,907 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,907 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,907 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,907 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,908 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,908 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,914 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 72.8 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,916 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 23.2 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,917 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.38.202.150:40123 (size: 23.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,918 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,918 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,918 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,921 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4938 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,955 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.38.202.150:40123 in memory (size: 45.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,967 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:45331 (size: 23.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:12,970 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:45331 in memory (size: 45.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,047 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:45331 in memory (size: 16.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,066 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.38.202.150:40123 in memory (size: 16.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,091 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.38.202.150:40123 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,111 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:45331 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,128 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 207 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,128 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,129 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.220 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,129 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,129 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,129 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,129 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,153 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.38.202.150:40123 in memory (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,185 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:45331 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,232 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.38.202.150:40123 in memory (size: 23.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,235 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:45331 in memory (size: 23.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,260 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.38.202.150:40123 in memory (size: 27.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,279 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:45331 in memory (size: 27.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,321 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.38.202.150:40123 in memory (size: 15.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,341 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:45331 in memory (size: 15.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,401 INFO codegen.CodeGenerator: Code generated in 139.95006 ms\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,432 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.38.202.150:40123 in memory (size: 45.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,435 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:45331 in memory (size: 45.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,436 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,454 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,454 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,454 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,455 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,455 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,459 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 66.2 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,462 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,462 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.38.202.150:40123 (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,463 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,463 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,463 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,465 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,479 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:45331 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,485 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.38.202.150:43360\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,554 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 89 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,554 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,555 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.097 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,557 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,558 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,558 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.115642 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,695 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,698 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,699 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,700 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,700 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,700 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,701 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,707 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 29.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,716 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 13.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,717 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.38.202.150:40123 (size: 13.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,718 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,721 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,722 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,723 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4938 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,737 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:45331 (size: 13.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,793 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 70 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,794 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,795 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.092 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,795 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,795 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,795 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,795 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,795 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,797 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,798 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,799 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.38.202.150:40123 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,799 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,800 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,800 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,805 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,826 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:45331 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,831 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.38.202.150:43360\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,858 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 52 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,858 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,859 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.063 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,859 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,859 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:13,861 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.164108 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,082 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,123 INFO codegen.CodeGenerator: Code generated in 14.936501 ms\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,129 INFO scheduler.DAGScheduler: Registering RDD 86 (count at StatsGenerator.scala:66) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,130 INFO scheduler.DAGScheduler: Got map stage job 14 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,130 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,131 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,131 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,132 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,135 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 21.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,137 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 10.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,138 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.38.202.150:40123 (size: 10.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,139 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,139 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,140 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,142 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4938 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,154 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:45331 (size: 10.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,271 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 130 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,271 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,273 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (count at StatsGenerator.scala:66) finished in 0.139 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,273 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,273 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,274 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,274 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,299 INFO codegen.CodeGenerator: Code generated in 16.365901 ms\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,315 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,317 INFO scheduler.DAGScheduler: Got job 15 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,319 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,320 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,320 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,321 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,323 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 11.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,326 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,327 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.38.202.150:40123 (size: 5.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,329 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,330 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,330 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,332 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,345 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:45331 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,353 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.38.202.150:43360\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,374 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 42 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,374 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,376 INFO scheduler.DAGScheduler: ResultStage 22 (count at StatsGenerator.scala:66) finished in 0.053 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,377 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,377 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,378 INFO scheduler.DAGScheduler: Job 15 finished: count at StatsGenerator.scala:66, took 0.062153 s\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,574 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,586 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,617 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,618 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,623 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,642 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,706 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,709 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,713 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,718 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,785 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,785 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,786 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,813 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,814 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-21b25926-c543-489f-bb9f-3c440a73b5a1\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,827 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-4cade3d2-effd-4c3a-a0a3-17d1eeba1045\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,919 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2025-09-21 13:01:14,919 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n",
      "‚úÖ Baseline job started successfully!\n",
      "‚è≥ This will take approximately 10-15 minutes...\n"
     ]
    }
   ],
   "source": [
    "# Create DefaultModelMonitor instance\n",
    "data_quality_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Define S3 paths for baseline job outputs\n",
    "baseline_job_name = f\"data-quality-baseline-{endpoint_name}-{int(time.time())}\"\n",
    "baseline_results_s3_uri = f\"s3://{bucket}/{baseline_s3_prefix}/results\"\n",
    "\n",
    "print(f\"üöÄ Starting baseline job: {baseline_job_name}\")\n",
    "print(f\"üìä Input data: {baseline_s3_uri}\")\n",
    "print(f\"üìÅ Output location: {baseline_results_s3_uri}\")\n",
    "\n",
    "# Start baseline job\n",
    "try:\n",
    "    data_quality_monitor.suggest_baseline(\n",
    "        baseline_dataset=baseline_s3_uri,\n",
    "        dataset_format=DatasetFormat.csv(header=False),\n",
    "        output_s3_uri=baseline_results_s3_uri,\n",
    "        job_name=baseline_job_name\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Baseline job started successfully!\")\n",
    "    print(\"‚è≥ This will take approximately 10-15 minutes...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error starting baseline job: {e}\")\n",
    "    baseline_job_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:03:17.242233Z",
     "iopub.status.busy": "2025-09-21T13:03:17.241890Z",
     "iopub.status.idle": "2025-09-21T13:03:17.313077Z",
     "shell.execute_reply": "2025-09-21T13:03:17.310949Z",
     "shell.execute_reply.started": "2025-09-21T13:03:17.242207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Monitoring baseline job progress...\n",
      "‚è±Ô∏è  Status: Completed | Elapsed: 0m 0s\n",
      "\n",
      "üèÅ Baseline job completed!\n",
      "‚úÖ Baseline creation successful!\n"
     ]
    }
   ],
   "source": [
    "# Monitor baseline job progress\n",
    "if baseline_job_name:\n",
    "    print(\"üìä Monitoring baseline job progress...\")\n",
    "    \n",
    "    # Wait for job completion with progress updates\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            job_desc = sm_client.describe_processing_job(ProcessingJobName=baseline_job_name)\n",
    "            status = job_desc['ProcessingJobStatus']\n",
    "            \n",
    "            elapsed_time = int(time.time() - start_time)\n",
    "            print(f\"\\r‚è±Ô∏è  Status: {status} | Elapsed: {elapsed_time//60}m {elapsed_time%60}s\", end=\"\", flush=True)\n",
    "            \n",
    "            if status in ['Completed', 'Failed', 'Stopped']:\n",
    "                print(f\"\\n\\nüèÅ Baseline job {status.lower()}!\")\n",
    "                \n",
    "                if status == 'Completed':\n",
    "                    print(\"‚úÖ Baseline creation successful!\")\n",
    "                    baseline_job_completed = True\n",
    "                else:\n",
    "                    print(f\"‚ùå Baseline job failed with status: {status}\")\n",
    "                    if 'FailureReason' in job_desc:\n",
    "                        print(f\"Failure reason: {job_desc['FailureReason']}\")\n",
    "                    baseline_job_completed = False\n",
    "                break\n",
    "                \n",
    "            time.sleep(30)  # Check every 30 seconds\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n‚è∏Ô∏è  Monitoring interrupted. Job continues running in background.\")\n",
    "            baseline_job_completed = None\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error monitoring job: {e}\")\n",
    "            baseline_job_completed = None\n",
    "            break\n",
    "else:\n",
    "    baseline_job_completed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the generated statistics and constraints\n",
    "After the baselining jobs finished, it saves the baseline statistics to the statistics.json file and the suggested baseline constraints to the constraints.json file in the location you specify with output_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:03:29.004065Z",
     "iopub.status.busy": "2025-09-21T13:03:29.003530Z",
     "iopub.status.idle": "2025-09-21T13:03:29.044964Z",
     "shell.execute_reply": "2025-09-21T13:03:29.044015Z",
     "shell.execute_reply.started": "2025-09-21T13:03:29.004037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ProcessingInputs': [{'InputName': 'baseline_dataset_input',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://amazon-sagemaker-006230620263-us-west-2-f717bf909848/model-monitor/dev-endpoint-20250918-141753/baseline/baseline.csv',\n",
       "    'LocalPath': '/opt/ml/processing/input/baseline_dataset_input',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}}],\n",
       " 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output',\n",
       "    'S3Output': {'S3Uri': 's3://amazon-sagemaker-006230620263-us-west-2-f717bf909848/model-monitor/dev-endpoint-20250918-141753/baseline/results',\n",
       "     'LocalPath': '/opt/ml/processing/output',\n",
       "     'S3UploadMode': 'EndOfJob'},\n",
       "    'AppManaged': False}]},\n",
       " 'ProcessingJobName': 'data-quality-baseline-dev-endpoint-20250918-141753-1758459469',\n",
       " 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1,\n",
       "   'InstanceType': 'ml.m5.xlarge',\n",
       "   'VolumeSizeInGB': 20}},\n",
       " 'StoppingCondition': {'MaxRuntimeInSeconds': 3600},\n",
       " 'AppSpecification': {'ImageUri': '159807026194.dkr.ecr.us-west-2.amazonaws.com/sagemaker-model-monitor-analyzer'},\n",
       " 'Environment': {'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}',\n",
       "  'dataset_source': '/opt/ml/processing/input/baseline_dataset_input',\n",
       "  'output_path': '/opt/ml/processing/output',\n",
       "  'publish_cloudwatch_metrics': 'Disabled'},\n",
       " 'NetworkConfig': {'EnableInterContainerTrafficEncryption': False,\n",
       "  'EnableNetworkIsolation': False,\n",
       "  'VpcConfig': {'SecurityGroupIds': ['sg-0f64dd177befa9219'],\n",
       "   'Subnets': ['subnet-0cd410bbb33b2ceaa',\n",
       "    'subnet-0a51efacd44b59211',\n",
       "    'subnet-016f25025c835e9db']}},\n",
       " 'RoleArn': 'arn:aws:iam::006230620263:role/datazone_usr_role_42utgzvtmcm8ls_aehrjybegqbp8w',\n",
       " 'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:006230620263:processing-job/data-quality-baseline-dev-endpoint-20250918-141753-1758459469',\n",
       " 'ProcessingJobStatus': 'Completed',\n",
       " 'ExitMessage': 'Completed: Job completed successfully with no violations.',\n",
       " 'ProcessingEndTime': datetime.datetime(2025, 9, 21, 13, 1, 25, tzinfo=tzlocal()),\n",
       " 'ProcessingStartTime': datetime.datetime(2025, 9, 21, 12, 58, 26, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2025, 9, 21, 13, 2, 52, 925000, tzinfo=tzlocal()),\n",
       " 'CreationTime': datetime.datetime(2025, 9, 21, 12, 57, 51, 330000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '89bc614e-278e-4c1a-a473-957e22b46597',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '89bc614e-278e-4c1a-a473-957e22b46597',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2257',\n",
       "   'date': 'Sun, 21 Sep 2025 13:03:29 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_quality_monitor.describe_latest_baselining_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:03:34.005166Z",
     "iopub.status.busy": "2025-09-21T13:03:34.004853Z",
     "iopub.status.idle": "2025-09-21T13:03:35.186241Z",
     "shell.execute_reply": "2025-09-21T13:03:35.185292Z",
     "shell.execute_reply.started": "2025-09-21T13:03:34.005142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-21 13:01:24       1987 constraints.json\n",
      "2025-09-21 13:01:24      20012 statistics.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls {baseline_results_s3_uri}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:03:39.297825Z",
     "iopub.status.busy": "2025-09-21T13:03:39.297494Z",
     "iopub.status.idle": "2025-09-21T13:03:39.302681Z",
     "shell.execute_reply": "2025-09-21T13:03:39.301646Z",
     "shell.execute_reply.started": "2025-09-21T13:03:39.297799Z"
    }
   },
   "outputs": [],
   "source": [
    "data_statistics_s3_url = f\"{baseline_results_s3_uri}/statistics.json\"\n",
    "data_constraints_s3_url = f\"{baseline_results_s3_uri}/constraints.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy statistics and constraints JSON files to the Studio EFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:03:45.847240Z",
     "iopub.status.busy": "2025-09-21T13:03:45.846953Z",
     "iopub.status.idle": "2025-09-21T13:03:48.357098Z",
     "shell.execute_reply": "2025-09-21T13:03:48.355964Z",
     "shell.execute_reply.started": "2025-09-21T13:03:45.847220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://amazon-sagemaker-006230620263-us-west-2-f717bf909848/model-monitor/dev-endpoint-20250918-141753/baseline/results/constraints.json to tmp/constraints.json\n",
      "download: s3://amazon-sagemaker-006230620263-us-west-2-f717bf909848/model-monitor/dev-endpoint-20250918-141753/baseline/results/statistics.json to tmp/statistics.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {data_constraints_s3_url} ./tmp/\n",
    "!aws s3 cp {data_statistics_s3_url} ./tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:03:49.479610Z",
     "iopub.status.busy": "2025-09-21T13:03:49.479288Z",
     "iopub.status.idle": "2025-09-21T13:03:49.680748Z",
     "shell.execute_reply": "2025-09-21T13:03:49.679124Z",
     "shell.execute_reply.started": "2025-09-21T13:03:49.479584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"version\" : 0.0,\n",
      "  \"features\" : [ {\n",
      "    \"name\" : \"_c0\",\n",
      "    \"inferred_type\" : \"Integral\",\n",
      "    \"completeness\" : 1.0,\n",
      "    \"num_constraints\" : {\n",
      "      \"is_non_negative\" : true\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c1\",\n",
      "    \"inferred_type\" : \"Fractional\",\n",
      "    \"completeness\" : 1.0,\n",
      "    \"num_constraints\" : {\n",
      "      \"is_non_negative\" : true\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c2\",\n",
      "    \"inferred_type\" : \"Fractional\",\n",
      "    \"completeness\" : 1.0,\n"
     ]
    }
   ],
   "source": [
    "!head -20 tmp/constraints.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:03:53.247899Z",
     "iopub.status.busy": "2025-09-21T13:03:53.247335Z",
     "iopub.status.idle": "2025-09-21T13:03:53.418137Z",
     "shell.execute_reply": "2025-09-21T13:03:53.416984Z",
     "shell.execute_reply.started": "2025-09-21T13:03:53.247640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"version\" : 0.0,\n",
      "  \"dataset\" : {\n",
      "    \"item_count\" : 30\n",
      "  },\n",
      "  \"features\" : [ {\n",
      "    \"name\" : \"_c0\",\n",
      "    \"inferred_type\" : \"Integral\",\n",
      "    \"numerical_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 30,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"mean\" : 1.0666666666666667,\n",
      "      \"sum\" : 32.0,\n",
      "      \"std_dev\" : 0.8137703743822469,\n",
      "      \"min\" : 0.0,\n",
      "      \"max\" : 2.0,\n",
      "      \"approximate_num_distinct_values\" : 3,\n",
      "      \"completeness\" : 1.0,\n"
     ]
    }
   ],
   "source": [
    "!head -20 tmp/statistics.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the generated JSON as Pandas DataFrame and see the content of statistics.json and constaints.json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:03:59.692666Z",
     "iopub.status.busy": "2025-09-21T13:03:59.692053Z",
     "iopub.status.idle": "2025-09-21T13:03:59.856793Z",
     "shell.execute_reply": "2025-09-21T13:03:59.855843Z",
     "shell.execute_reply.started": "2025-09-21T13:03:59.692631Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>numerical_statistics.common.num_present</th>\n",
       "      <th>numerical_statistics.common.num_missing</th>\n",
       "      <th>numerical_statistics.mean</th>\n",
       "      <th>numerical_statistics.sum</th>\n",
       "      <th>numerical_statistics.std_dev</th>\n",
       "      <th>numerical_statistics.min</th>\n",
       "      <th>numerical_statistics.max</th>\n",
       "      <th>numerical_statistics.approximate_num_distinct_values</th>\n",
       "      <th>numerical_statistics.completeness</th>\n",
       "      <th>numerical_statistics.distribution.kll.buckets</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.c</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.k</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_c0</td>\n",
       "      <td>Integral</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1.066667</td>\n",
       "      <td>32.000</td>\n",
       "      <td>0.813770</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.2, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_c1</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.519600</td>\n",
       "      <td>15.588</td>\n",
       "      <td>0.119817</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.735</td>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.282, 'upper_bound': 0.3273,...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.618, 0.662, 0.735, 0.479, 0.472, 0.467, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_c2</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.412900</td>\n",
       "      <td>12.387</td>\n",
       "      <td>0.093845</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.586</td>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.235, 'upper_bound': 0.2701,...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.441, 0.527, 0.538, 0.407, 0.332, 0.333, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_c3</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.099367</td>\n",
       "      <td>2.981</td>\n",
       "      <td>0.025073</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.167</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.045, 'upper_bound': 0.0572,...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.093, 0.105, 0.167, 0.076, 0.087, 0.112, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_c4</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.234167</td>\n",
       "      <td>7.025</td>\n",
       "      <td>0.138781</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.541</td>\n",
       "      <td>31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.034, 'upper_bound': 0.0847,...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.24, 0.342, 0.541, 0.147, 0.158, 0.191, 0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name inferred_type  numerical_statistics.common.num_present  \\\n",
       "0  _c0      Integral                                       30   \n",
       "1  _c1    Fractional                                       30   \n",
       "2  _c2    Fractional                                       30   \n",
       "3  _c3    Fractional                                       30   \n",
       "4  _c4    Fractional                                       30   \n",
       "\n",
       "   numerical_statistics.common.num_missing  numerical_statistics.mean  \\\n",
       "0                                        0                   1.066667   \n",
       "1                                        0                   0.519600   \n",
       "2                                        0                   0.412900   \n",
       "3                                        0                   0.099367   \n",
       "4                                        0                   0.234167   \n",
       "\n",
       "   numerical_statistics.sum  numerical_statistics.std_dev  \\\n",
       "0                    32.000                      0.813770   \n",
       "1                    15.588                      0.119817   \n",
       "2                    12.387                      0.093845   \n",
       "3                     2.981                      0.025073   \n",
       "4                     7.025                      0.138781   \n",
       "\n",
       "   numerical_statistics.min  numerical_statistics.max  \\\n",
       "0                     0.000                     2.000   \n",
       "1                     0.282                     0.735   \n",
       "2                     0.235                     0.586   \n",
       "3                     0.045                     0.167   \n",
       "4                     0.034                     0.541   \n",
       "\n",
       "   numerical_statistics.approximate_num_distinct_values  \\\n",
       "0                                                  3      \n",
       "1                                                 30      \n",
       "2                                                 29      \n",
       "3                                                 28      \n",
       "4                                                 31      \n",
       "\n",
       "   numerical_statistics.completeness  \\\n",
       "0                                1.0   \n",
       "1                                1.0   \n",
       "2                                1.0   \n",
       "3                                1.0   \n",
       "4                                1.0   \n",
       "\n",
       "       numerical_statistics.distribution.kll.buckets  \\\n",
       "0  [{'lower_bound': 0.0, 'upper_bound': 0.2, 'cou...   \n",
       "1  [{'lower_bound': 0.282, 'upper_bound': 0.3273,...   \n",
       "2  [{'lower_bound': 0.235, 'upper_bound': 0.2701,...   \n",
       "3  [{'lower_bound': 0.045, 'upper_bound': 0.0572,...   \n",
       "4  [{'lower_bound': 0.034, 'upper_bound': 0.0847,...   \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.parameters.c  \\\n",
       "0                                               0.64           \n",
       "1                                               0.64           \n",
       "2                                               0.64           \n",
       "3                                               0.64           \n",
       "4                                               0.64           \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.parameters.k  \\\n",
       "0                                             2048.0           \n",
       "1                                             2048.0           \n",
       "2                                             2048.0           \n",
       "3                                             2048.0           \n",
       "4                                             2048.0           \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.data  \n",
       "0  [[0.0, 0.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0,...  \n",
       "1  [[0.618, 0.662, 0.735, 0.479, 0.472, 0.467, 0....  \n",
       "2  [[0.441, 0.527, 0.538, 0.407, 0.332, 0.333, 0....  \n",
       "3  [[0.093, 0.105, 0.167, 0.076, 0.087, 0.112, 0....  \n",
       "4  [[0.24, 0.342, 0.541, 0.147, 0.158, 0.191, 0.1...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_job = data_quality_monitor.latest_baselining_job\n",
    "statistics_df = pd.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "statistics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:04:08.079639Z",
     "iopub.status.busy": "2025-09-21T13:04:08.079334Z",
     "iopub.status.idle": "2025-09-21T13:04:08.199053Z",
     "shell.execute_reply": "2025-09-21T13:04:08.198152Z",
     "shell.execute_reply.started": "2025-09-21T13:04:08.079613Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>completeness</th>\n",
       "      <th>num_constraints.is_non_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_c0</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_c1</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_c2</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_c3</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_c4</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>_c5</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>_c6</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>_c7</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>_c8</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>_c9</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name inferred_type  completeness  num_constraints.is_non_negative\n",
       "0  _c0      Integral           1.0                             True\n",
       "1  _c1    Fractional           1.0                             True\n",
       "2  _c2    Fractional           1.0                             True\n",
       "3  _c3    Fractional           1.0                             True\n",
       "4  _c4    Fractional           1.0                             True\n",
       "5  _c5    Fractional           1.0                             True\n",
       "6  _c6    Fractional           1.0                             True\n",
       "7  _c7    Fractional           1.0                             True\n",
       "8  _c8    Fractional           1.0                             True\n",
       "9  _c9    Fractional           1.0                             True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints_df = pd.json_normalize(\n",
    "    baseline_job.suggested_constraints().body_dict[\"features\"]\n",
    ")\n",
    "constraints_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset the baselining job suggest three constraints:\n",
    "\n",
    "1. DataType\n",
    "2. Completeness\n",
    "3. Is non-negative\n",
    "Additionally, the Model Monitor prebuilt container does missing and extra column check, baseline drift check, and categorical values check. Refer to Developer Guide for more details.\n",
    "\n",
    "In a real-world project you can add your own constraints the data must comply with.\n",
    "\n",
    "Next you schedule and run a monitoring job to validate incoming data against these constraints and statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Create Data Quality Monitoring Schedule\n",
    "\n",
    "Set up automated monitoring to run on a schedule:\n",
    "With a monitoring schedule, SageMaker launches processing jobs at a specified frequency to analyze the data collected during a given period. SageMaker provides a [built-in container](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-pre-built-container.html) for performing analysis on tabular datasets. In the processing job, SageMaker compares the dataset for the current analysis with the baseline statistics and constraints and generates a violations report. In addition, CloudWatch metrics are emitted for each data feature under analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:19:59.800635Z",
     "iopub.status.busy": "2025-09-21T13:19:59.800330Z",
     "iopub.status.idle": "2025-09-21T13:20:00.776056Z",
     "shell.execute_reply": "2025-09-21T13:20:00.774896Z",
     "shell.execute_reply.started": "2025-09-21T13:19:59.800611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating data quality monitoring schedule: dev-endpoint-20250918-141753-data-quality-schedule\n",
      "‚úÖ Data quality monitoring schedule created successfully\n",
      "üìä Schedule: Hourly monitoring\n",
      "üìÅ Reports will be stored at: s3://amazon-sagemaker-006230620263-us-west-2-f717bf909848/model-monitor/dev-endpoint-20250918-141753/data-quality-reports\n"
     ]
    }
   ],
   "source": [
    "# Create data quality monitoring schedule\n",
    "data_quality_schedule_name = f\"{endpoint_name}-data-quality-schedule\"\n",
    "data_quality_reports_uri = f\"s3://{bucket}/model-monitor/{endpoint_name}/data-quality-reports\"\n",
    "\n",
    "print(f\"üîÑ Creating data quality monitoring schedule: {data_quality_schedule_name}\")\n",
    "\n",
    "try:\n",
    "    data_quality_monitor.create_monitoring_schedule(\n",
    "        monitor_schedule_name=data_quality_schedule_name,\n",
    "        endpoint_input=EndpointInput(\n",
    "            endpoint_name=endpoint_name,\n",
    "            destination=\"/opt/ml/processing/input\"\n",
    "        ),\n",
    "        output_s3_uri=data_quality_reports_uri,\n",
    "        schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "        enable_cloudwatch_metrics=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Data quality monitoring schedule created successfully\")\n",
    "    print(f\"üìä Schedule: Hourly monitoring\")\n",
    "    print(f\"üìÅ Reports will be stored at: {data_quality_reports_uri}\")\n",
    "    data_quality_schedule_created = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating data quality monitoring schedule: {e}\")\n",
    "    print(\"üí° This might be due to existing schedule or permission issues.\")\n",
    "    data_quality_schedule_created = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Launch a Manual Monitoring Job\n",
    "\n",
    "You can launch a monitoring job manually and don't wait until a configured data monitor schedule execution. You created an hourly schedule, so you need to wait until you cross the hour boundary to see some schedule executions.\n",
    "\n",
    "Since the Model Monitor uses a built-in container and a SageMaker processing job to run analysis of the captured data, you can manually configure and run a monitoring job.\n",
    "\n",
    "This utils folder contains an implementation of a helper function to manually run a monitoring job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:20:39.563671Z",
     "iopub.status.busy": "2025-09-21T13:20:39.563314Z",
     "iopub.status.idle": "2025-09-21T13:20:39.570861Z",
     "shell.execute_reply": "2025-09-21T13:20:39.570071Z",
     "shell.execute_reply.started": "2025-09-21T13:20:39.563647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imported monitoring utilities\n"
     ]
    }
   ],
   "source": [
    "# Import the monitoring utilities\n",
    "import sys\n",
    "sys.path.append('./utils')\n",
    "from monitoring_utils import run_model_monitor_job\n",
    "\n",
    "print(\"‚úÖ Imported monitoring utilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate non-compliant traffic\n",
    "Now generate traffic that will trigger the violation in the model monitor data quality check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:26:26.698195Z",
     "iopub.status.busy": "2025-09-21T13:26:26.697792Z",
     "iopub.status.idle": "2025-09-21T13:26:59.891655Z",
     "shell.execute_reply": "2025-09-21T13:26:59.890082Z",
     "shell.execute_reply.started": "2025-09-21T13:26:26.698163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Generating drift traffic to test monitoring...\n",
      "üö® Generating 12 drift-inducing requests...\n",
      "   This data is intentionally different from baseline to trigger violations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending drift traffic: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:03<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Generated 12 drift requests\n",
      "‚ö†Ô∏è This should trigger violations in monitoring analysis\n",
      "\n",
      "‚è≥ Waiting for data capture to process...\n"
     ]
    }
   ],
   "source": [
    "# Generate drift traffic first to test monitoring\n",
    "def generate_drift_traffic(endpoint_name, num_requests=15):\n",
    "    \"\"\"Generate traffic that will likely trigger monitoring violations\"\"\"\n",
    "    \n",
    "    if not endpoint_name:\n",
    "        print(\"‚ùå No endpoint available for drift simulation\")\n",
    "        return False\n",
    "    \n",
    "    runtime_client = boto3.client('sagemaker-runtime')\n",
    "    \n",
    "    print(f\"üö® Generating {num_requests} drift-inducing requests...\")\n",
    "    print(\"   This data is intentionally different from baseline to trigger violations\")\n",
    "    \n",
    "    successful_requests = 0\n",
    "    \n",
    "    for i in tqdm(range(num_requests), desc=\"Sending drift traffic\"):\n",
    "        try:\n",
    "            # Generate data that's significantly different from baseline\n",
    "            if i % 3 == 0:\n",
    "                # Extremely large values (outliers)\n",
    "                test_data = \"2,2.0,1.8,0.8,5.0,2.5,1.2,1.8,0.0,0.0\"\n",
    "            elif i % 3 == 1:\n",
    "                # Extremely small values (outliers)\n",
    "                test_data = \"0,0.05,0.04,0.01,0.01,0.005,0.002,0.003,0.0,0.0\"\n",
    "            else:\n",
    "                # Negative values (constraint violations)\n",
    "                test_data = \"1,-0.5,0.4,0.1,0.6,-0.2,0.1,0.2,0.0,0.0\"\n",
    "            \n",
    "            runtime_client.invoke_endpoint(\n",
    "                EndpointName=endpoint_name,\n",
    "                ContentType='text/csv',\n",
    "                Body=test_data\n",
    "            )\n",
    "            \n",
    "            successful_requests += 1\n",
    "            time.sleep(0.2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Request {i+1} failed: {e}\")\n",
    "    \n",
    "    print(f\"\\nüìä Generated {successful_requests} drift requests\")\n",
    "    print(\"‚ö†Ô∏è This should trigger violations in monitoring analysis\")\n",
    "    return successful_requests > 0\n",
    "\n",
    "# Generate drift traffic to test monitoring\n",
    "if endpoint_name:\n",
    "    print(\"üéØ Generating drift traffic to test monitoring...\")\n",
    "    drift_generated = generate_drift_traffic(endpoint_name, num_requests=12)\n",
    "    \n",
    "    if drift_generated:\n",
    "        print(\"\\n‚è≥ Waiting for data capture to process...\")\n",
    "        time.sleep(30)\n",
    "        print(\"\\n‚è≥ Completed\")\n",
    "else:\n",
    "    drift_generated = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Captured Drift Data\n",
    "\n",
    "Let's examine the captured data to see the drift traffic we just generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:28:34.258851Z",
     "iopub.status.busy": "2025-09-21T13:28:34.258568Z",
     "iopub.status.idle": "2025-09-21T13:28:35.243991Z",
     "shell.execute_reply": "2025-09-21T13:28:35.242982Z",
     "shell.execute_reply.started": "2025-09-21T13:28:34.258830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Examining captured drift data...\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "üìÑ Latest capture file: 26-26-810-ab62a649-66b1-4a77-8bd2-95d0d2c1d4d7.jsonl\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "üìä File contains 12 inference records\n",
      "\n",
      "üîç Showing last 10 records (likely drift data):\n",
      "\n",
      "üìù Record 1:\n",
      "   Input:  1,-0.5,0.4,0.1,0.6,-0.2,0.1,0.2,0.0,0.0\n",
      "   Output: 9.007847785949707\n",
      "   ‚ö†Ô∏è VIOLATION: Negative values detected!\n",
      "\n",
      "üìù Record 2:\n",
      "   Input:  2,2.0,1.8,0.8,5.0,2.5,1.2,1.8,0.0,0.0\n",
      "   Output: 9.617115020751953\n",
      "   üö® DRIFT: Extremely large values detected!\n",
      "\n",
      "üìù Record 3:\n",
      "   Input:  0,0.05,0.04,0.01,0.01,0.005,0.002,0.003,0.0,0.0\n",
      "   Output: 9.942182540893555\n",
      "   üö® DRIFT: Extremely small values detected!\n",
      "\n",
      "üìù Record 4:\n",
      "   Input:  1,-0.5,0.4,0.1,0.6,-0.2,0.1,0.2,0.0,0.0\n",
      "   Output: 9.007847785949707\n",
      "   ‚ö†Ô∏è VIOLATION: Negative values detected!\n",
      "\n",
      "üìù Record 5:\n",
      "   Input:  2,2.0,1.8,0.8,5.0,2.5,1.2,1.8,0.0,0.0\n",
      "   Output: 9.617115020751953\n",
      "   üö® DRIFT: Extremely large values detected!\n",
      "\n",
      "üìù Record 6:\n",
      "   Input:  0,0.05,0.04,0.01,0.01,0.005,0.002,0.003,0.0,0.0\n",
      "   Output: 9.942182540893555\n",
      "   üö® DRIFT: Extremely small values detected!\n",
      "\n",
      "üìù Record 7:\n",
      "   Input:  1,-0.5,0.4,0.1,0.6,-0.2,0.1,0.2,0.0,0.0\n",
      "   Output: 9.007847785949707\n",
      "   ‚ö†Ô∏è VIOLATION: Negative values detected!\n",
      "\n",
      "üìù Record 8:\n",
      "   Input:  2,2.0,1.8,0.8,5.0,2.5,1.2,1.8,0.0,0.0\n",
      "   Output: 9.617115020751953\n",
      "   üö® DRIFT: Extremely large values detected!\n",
      "\n",
      "üìù Record 9:\n",
      "   Input:  0,0.05,0.04,0.01,0.01,0.005,0.002,0.003,0.0,0.0\n",
      "   Output: 9.942182540893555\n",
      "   üö® DRIFT: Extremely small values detected!\n",
      "\n",
      "üìù Record 10:\n",
      "   Input:  1,-0.5,0.4,0.1,0.6,-0.2,0.1,0.2,0.0,0.0\n",
      "   Output: 9.007847785949707\n",
      "   ‚ö†Ô∏è VIOLATION: Negative values detected!\n",
      "\n",
      "üí° The drift data shows intentionally problematic values that should trigger violations!\n"
     ]
    }
   ],
   "source": [
    "def show_captured_drift_data(s3_uri, max_records=3):\n",
    "    \"\"\"Show the latest captured data including drift traffic\"\"\"\n",
    "    \n",
    "    if not s3_uri or not drift_generated:\n",
    "        print(\"‚ö†Ô∏è No drift data to examine\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        print(\"üîç Examining captured drift data...\")\n",
    "        \n",
    "        # Get latest captured files\n",
    "        captured_files = S3Downloader.list(s3_uri)\n",
    "        \n",
    "        if not captured_files:\n",
    "            print(\"üì≠ No captured files found\")\n",
    "            return\n",
    "        \n",
    "        # Get the most recent file (should contain drift data)\n",
    "        latest_file = sorted(captured_files)[-1]\n",
    "        print(f\"üìÑ Latest capture file: {latest_file.split('/')[-1]}\")\n",
    "        \n",
    "        # Download and parse the file\n",
    "        file_content = S3Downloader.read_file(latest_file)\n",
    "        lines = file_content.strip().split('\\n')\n",
    "        \n",
    "        print(f\"üìä File contains {len(lines)} inference records\")\n",
    "        print(f\"\\nüîç Showing last {min(max_records, len(lines))} records (likely drift data):\")\n",
    "        \n",
    "        # Show the last few records (most likely to be drift data)\n",
    "        for i, line in enumerate(lines[-max_records:], 1):\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                \n",
    "                # Extract input and output data\n",
    "                if 'captureData' in record:\n",
    "                    capture_data = record['captureData']\n",
    "                    \n",
    "                    # Get input data\n",
    "                    input_data = capture_data.get('endpointInput', {}).get('data', 'N/A')\n",
    "                    \n",
    "                    # Get output data\n",
    "                    output_data = capture_data.get('endpointOutput', {}).get('data', 'N/A')\n",
    "                    \n",
    "                    print(f\"\\nüìù Record {i}:\")\n",
    "                    print(f\"   Input:  {input_data}\")\n",
    "                    print(f\"   Output: {output_data}\")\n",
    "                    \n",
    "                    # Analyze if this looks like drift data\n",
    "                    if input_data != 'N/A':\n",
    "                        values = input_data.split(',')\n",
    "                        if len(values) >= 3:\n",
    "                            try:\n",
    "                                # Check for extreme values that indicate drift\n",
    "                                val1, val2, val3 = float(values[1]), float(values[2]), float(values[3])\n",
    "                                if val1 > 1.5 or val2 > 1.5:\n",
    "                                    print(f\"   üö® DRIFT: Extremely large values detected!\")\n",
    "                                elif val1 < 0 or val2 < 0 or val3 < 0:\n",
    "                                    print(f\"   ‚ö†Ô∏è VIOLATION: Negative values detected!\")\n",
    "                                elif val1 < 0.1 and val2 < 0.1:\n",
    "                                    print(f\"   üö® DRIFT: Extremely small values detected!\")\n",
    "                                else:\n",
    "                                    print(f\"   ‚úÖ Normal range values\")\n",
    "                            except ValueError:\n",
    "                                print(f\"   ‚ùì Could not analyze values\")\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"   ‚ùå Could not parse record {i}\")\n",
    "        \n",
    "        print(f\"\\nüí° The drift data shows intentionally problematic values that should trigger violations!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error examining drift data: {e}\")\n",
    "\n",
    "# Show captured drift data\n",
    "if drift_generated:\n",
    "    show_captured_drift_data(data_capture_s3_uri, max_records=10)\n",
    "else:\n",
    "    print(\"üí° No drift traffic generated to examine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Manual Monitoring Job with Util Function\n",
    "\n",
    "Now let's use the utility function to manually run a monitoring job on the captured data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:29:19.756964Z",
     "iopub.status.busy": "2025-09-21T13:29:19.756679Z",
     "iopub.status.idle": "2025-09-21T13:29:22.305361Z",
     "shell.execute_reply": "2025-09-21T13:29:22.304492Z",
     "shell.execute_reply.started": "2025-09-21T13:29:19.756943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running manual monitoring job using utility function...\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.ProcessingJob.NetworkConfig.VpcConfig.Subnets\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.ProcessingJob.NetworkConfig.VpcConfig.SecurityGroupIds\n",
      "üìÅ Reports will be saved to: s3://amazon-sagemaker-006230620263-us-west-2-f717bf909848/model-monitor/dev-endpoint-20250918-141753/manual-reports\n",
      "‚è≥ Job is running in background - will analyze drift data\n"
     ]
    }
   ],
   "source": [
    "# Run manual monitoring job using the utility function\n",
    "if baseline_job_completed and drift_generated:\n",
    "    print(\"üöÄ Running manual monitoring job using utility function...\")\n",
    "    \n",
    "    # Define paths for the monitoring job\n",
    "    manual_reports_path = f\"s3://{bucket}/model-monitor/{endpoint_name}/manual-reports\"\n",
    "    \n",
    "    try:\n",
    "        # Use the utility function to run monitoring job\n",
    "        processing_job = run_model_monitor_job(\n",
    "            region=region,\n",
    "            instance_type='ml.m5.xlarge',\n",
    "            role=role,\n",
    "            data_capture_path=data_capture_s3_uri,\n",
    "            statistics_path=data_statistics_s3_url,\n",
    "            constraints_path=data_constraints_s3_url,\n",
    "            reports_path=manual_reports_path,\n",
    "            instance_count=1,\n",
    "            publish_cloudwatch_metrics='Disabled',\n",
    "            wait=False,  # Don't wait for completion\n",
    "            logs=False\n",
    "        )\n",
    "        \n",
    "        \n",
    "        print(f\"üìÅ Reports will be saved to: {manual_reports_path}\")\n",
    "        print(\"‚è≥ Job is running in background - will analyze drift data\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running manual monitoring job: {e}\")\n",
    "        manual_job_name = None\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping manual monitoring job - prerequisites not met\")\n",
    "    print(f\"   Baseline completed: {baseline_job_completed if 'baseline_job_completed' in locals() else False}\")\n",
    "    print(f\"   Drift generated: {drift_generated}\")\n",
    "    manual_job_name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check Latest Captured Data and Monitoring Results\n",
    "\n",
    "Let's examine the latest captured data and check for any monitoring violations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:38:58.958388Z",
     "iopub.status.busy": "2025-09-21T13:38:58.958089Z",
     "iopub.status.idle": "2025-09-21T13:38:59.035666Z",
     "shell.execute_reply": "2025-09-21T13:38:59.035078Z",
     "shell.execute_reply.started": "2025-09-21T13:38:58.958365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Checking monitoring schedule executions...\n",
      "üìä Found 2 recent execution(s):\n",
      "[{'MonitoringScheduleName': 'dev-endpoint-20250918-141753-data-quality-schedule', 'ScheduledTime': datetime.datetime(2025, 9, 21, 13, 0, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2025, 9, 21, 13, 2, 23, 286000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2025, 9, 21, 13, 8, 12, 811000, tzinfo=tzlocal()), 'MonitoringExecutionStatus': 'CompletedWithViolations', 'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:006230620263:processing-job/model-monitoring-202509211300-f944e75d79c63a5ad7055637', 'EndpointName': 'dev-endpoint-20250918-141753', 'MonitoringJobDefinitionName': 'data-quality-job-definition-2025-09-20-18-04-24-772', 'MonitoringType': 'DataQuality'}, {'MonitoringScheduleName': 'dev-endpoint-20250918-141753-data-quality-schedule', 'ScheduledTime': datetime.datetime(2025, 9, 21, 12, 0, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2025, 9, 21, 12, 2, 23, 346000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2025, 9, 21, 12, 8, 9, 751000, tzinfo=tzlocal()), 'MonitoringExecutionStatus': 'Failed', 'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:006230620263:processing-job/model-monitoring-202509211200-00052dcb4faeebe21785eeec', 'EndpointName': 'dev-endpoint-20250918-141753', 'FailureReason': 'AlgorithmError: Error: Could not find any jsonl file under directory /opt/ml/processing/input. Please verify if the provided dataset path is correct or if data capturing in your Endpoint is turned on., exit code: 255', 'MonitoringJobDefinitionName': 'data-quality-job-definition-2025-09-20-18-04-24-772', 'MonitoringType': 'DataQuality'}]\n",
      "   ‚Ä¢ Status: CompletedWithViolations | Scheduled: 2025-09-21 13:00:00+00:00\n",
      "   ‚Ä¢ Status: Failed | Scheduled: 2025-09-21 12:00:00+00:00\n",
      "     ‚ùå Execution failed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check monitoring schedule executions\n",
    "def check_monitoring_executions(schedule_name):\n",
    "    \"\"\"Check recent monitoring schedule executions\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = sm_client.list_monitoring_executions(\n",
    "            MonitoringScheduleName=schedule_name,\n",
    "            MaxResults=2\n",
    "        )\n",
    "        \n",
    "        executions = response.get('MonitoringExecutionSummaries', [])\n",
    "        \n",
    "        if executions:\n",
    "            print(f\"üìä Found {len(executions)} recent execution(s):\")\n",
    "            print(executions)\n",
    "            for execution in executions:\n",
    "                status = execution.get('MonitoringExecutionStatus', 'Unknown')\n",
    "                scheduled_time = execution.get('ScheduledTime', 'Unknown')\n",
    "                \n",
    "                print(f\"   ‚Ä¢ Status: {status} | Scheduled: {scheduled_time}\")\n",
    "                \n",
    "                if status == 'Completed':\n",
    "                    print(\"     ‚úÖ Execution completed - check S3 for reports\")\n",
    "                elif status == 'Failed':\n",
    "                    print(\"     ‚ùå Execution failed\")\n",
    "                elif status == 'InProgress':\n",
    "                    print(\"     ‚è≥ Execution in progress\")\n",
    "        else:\n",
    "            print(\"üì≠ No executions found yet - monitoring jobs run on schedule\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking executions: {e}\")\n",
    "\n",
    "# Check monitoring schedule executions\n",
    "if data_quality_schedule_created:\n",
    "    print(\"\\nüîç Checking monitoring schedule executions...\")\n",
    "    check_monitoring_executions(data_quality_schedule_name)\n",
    "else:\n",
    "    print(\"üí° No monitoring schedule to check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View Monitoring Reports and Violations\n",
    "\n",
    "Let's examine any monitoring reports that have been generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:39:43.234434Z",
     "iopub.status.busy": "2025-09-21T13:39:43.234137Z",
     "iopub.status.idle": "2025-09-21T13:39:44.270559Z",
     "shell.execute_reply": "2025-09-21T13:39:44.269738Z",
     "shell.execute_reply.started": "2025-09-21T13:39:43.234412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Examining reports in: s3://amazon-sagemaker-006230620263-us-west-2-f717bf909848/model-monitor/dev-endpoint-20250918-141753/data-quality-reports\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "üìÅ Found 2 report files:\n",
      "  ‚Ä¢ constraint_violations.json\n",
      "  ‚Ä¢ constraint_violations.json\n",
      "\n",
      "üîç Analyzing violations in: constraint_violations.json\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3Bucket\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.Session.DefaultS3ObjectKeyPrefix\n",
      "‚ö†Ô∏è Found 1 violations:\n",
      "\n",
      "1. Feature: Extra columns\n",
      "   Check: extra_column_check\n",
      "   Issue: There are extra columns in current dataset. Number of columns in current dataset...\n",
      "\n",
      "üéØ Success! Monitoring detected data drift as expected.\n"
     ]
    }
   ],
   "source": [
    "def examine_monitoring_reports(reports_s3_uri):\n",
    "    \"\"\"Examine monitoring reports and violations\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"üìä Examining reports in: {reports_s3_uri}\")\n",
    "        \n",
    "        # List report files\n",
    "        report_files = S3Downloader.list(reports_s3_uri)\n",
    "        \n",
    "        if not report_files:\n",
    "            print(\"üì≠ No report files found yet\")\n",
    "            print(\"üí° Reports appear after monitoring jobs complete (may take time)\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üìÅ Found {len(report_files)} report files:\")\n",
    "        for file_path in report_files[-5:]:  # Show last 5\n",
    "            file_name = file_path.split('/')[-1]\n",
    "            print(f\"  ‚Ä¢ {file_name}\")\n",
    "        \n",
    "        # Look for constraint violations\n",
    "        violation_files = [f for f in report_files if 'constraint_violations.json' in f]\n",
    "        \n",
    "        if violation_files:\n",
    "            latest_violations = violation_files[-1]  # Most recent\n",
    "            print(f\"\\nüîç Analyzing violations in: {latest_violations.split('/')[-1]}\")\n",
    "            \n",
    "            violations_content = S3Downloader.read_file(latest_violations)\n",
    "            violations_data = json.loads(violations_content)\n",
    "            \n",
    "            violations_list = violations_data.get('violations', [])\n",
    "            \n",
    "            if violations_list:\n",
    "                print(f\"‚ö†Ô∏è Found {len(violations_list)} violations:\")\n",
    "                \n",
    "                for i, violation in enumerate(violations_list[:3], 1):  # Show first 3\n",
    "                    feature = violation.get('feature_name', 'Unknown')\n",
    "                    check_type = violation.get('constraint_check_type', 'Unknown')\n",
    "                    description = violation.get('description', 'No description')\n",
    "                    \n",
    "                    print(f\"\\n{i}. Feature: {feature}\")\n",
    "                    print(f\"   Check: {check_type}\")\n",
    "                    print(f\"   Issue: {description[:80]}...\" if len(description) > 80 else f\"   Issue: {description}\")\n",
    "                \n",
    "                print(f\"\\nüéØ Success! Monitoring detected data drift as expected.\")\n",
    "            else:\n",
    "                print(\"‚úÖ No violations detected - data within baseline constraints\")\n",
    "        else:\n",
    "            print(\"üìã No violation files found - monitoring may still be processing\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error examining reports: {e}\")\n",
    "\n",
    "# Examine monitoring reports\n",
    "if data_quality_schedule_created:\n",
    "    examine_monitoring_reports(data_quality_reports_uri)\n",
    "else:\n",
    "    print(\"üí° No monitoring reports to examine yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:39:50.279362Z",
     "iopub.status.busy": "2025-09-21T13:39:50.279071Z",
     "iopub.status.idle": "2025-09-21T13:39:50.458336Z",
     "shell.execute_reply": "2025-09-21T13:39:50.457405Z",
     "shell.execute_reply.started": "2025-09-21T13:39:50.279341Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MonitoringScheduleArn': 'arn:aws:sagemaker:us-west-2:006230620263:monitoring-schedule/dev-endpoint-20250918-141753-data-quality-schedule',\n",
       " 'MonitoringScheduleName': 'dev-endpoint-20250918-141753-data-quality-schedule',\n",
       " 'MonitoringScheduleStatus': 'Scheduled',\n",
       " 'MonitoringType': 'DataQuality',\n",
       " 'CreationTime': datetime.datetime(2025, 9, 21, 13, 20, 0, 617000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2025, 9, 21, 13, 20, 8, 635000, tzinfo=tzlocal()),\n",
       " 'MonitoringScheduleConfig': {'ScheduleConfig': {'ScheduleExpression': 'cron(0 * ? * * *)'},\n",
       "  'MonitoringJobDefinitionName': 'data-quality-job-definition-2025-09-21-13-19-59-803',\n",
       "  'MonitoringType': 'DataQuality'},\n",
       " 'EndpointName': 'dev-endpoint-20250918-141753',\n",
       " 'LastMonitoringExecutionSummary': {'MonitoringScheduleName': 'dev-endpoint-20250918-141753-data-quality-schedule',\n",
       "  'ScheduledTime': datetime.datetime(2025, 9, 21, 13, 0, tzinfo=tzlocal()),\n",
       "  'CreationTime': datetime.datetime(2025, 9, 21, 13, 2, 23, 286000, tzinfo=tzlocal()),\n",
       "  'LastModifiedTime': datetime.datetime(2025, 9, 21, 13, 8, 12, 811000, tzinfo=tzlocal()),\n",
       "  'MonitoringExecutionStatus': 'CompletedWithViolations',\n",
       "  'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:006230620263:processing-job/model-monitoring-202509211300-f944e75d79c63a5ad7055637',\n",
       "  'EndpointName': 'dev-endpoint-20250918-141753'},\n",
       " 'ResponseMetadata': {'RequestId': 'b73079c0-86b9-43a7-81a5-7c518764e640',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'b73079c0-86b9-43a7-81a5-7c518764e640',\n",
       "   'strict-transport-security': 'max-age=47304000; includeSubDomains',\n",
       "   'x-frame-options': 'DENY',\n",
       "   'content-security-policy': \"frame-ancestors 'none'\",\n",
       "   'cache-control': 'no-cache, no-store, must-revalidate',\n",
       "   'x-content-type-options': 'nosniff',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1109',\n",
       "   'date': 'Sun, 21 Sep 2025 13:39:50 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "while data_quality_monitor.describe_schedule()[\"MonitoringScheduleStatus\"] != \"Scheduled\":\n",
    "    print(f\"Waiting until data monitoring schedule status becomes Scheduled\")\n",
    "    time.sleep(3)\n",
    "\n",
    "data_quality_monitor.describe_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:42:25.430747Z",
     "iopub.status.busy": "2025-09-21T13:42:25.430471Z",
     "iopub.status.idle": "2025-09-21T13:42:25.434690Z",
     "shell.execute_reply": "2025-09-21T13:42:25.433950Z",
     "shell.execute_reply.started": "2025-09-21T13:42:25.430727Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get S3 url for the latest monitoring job output\n",
    "def get_latest_monitoring_report_s3_url(job_name):\n",
    "    monitor_job = sm_client.list_processing_jobs(\n",
    "        NameContains=job_name,\n",
    "        SortOrder='Descending',\n",
    "        MaxResults=2\n",
    "    )['ProcessingJobSummaries'][0]['ProcessingJobName']\n",
    "\n",
    "    monitoring_job_output_s3_url = sm_client.describe_processing_job(\n",
    "        ProcessingJobName=monitor_job\n",
    "    )['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']\n",
    "\n",
    "    print(f\"Latest monitoring report S3 url: {monitoring_job_output_s3_url}\")\n",
    "    \n",
    "    return monitoring_job_output_s3_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:42:26.242392Z",
     "iopub.status.busy": "2025-09-21T13:42:26.242105Z",
     "iopub.status.idle": "2025-09-21T13:42:26.309801Z",
     "shell.execute_reply": "2025-09-21T13:42:26.308937Z",
     "shell.execute_reply.started": "2025-09-21T13:42:26.242370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest monitoring report S3 url: s3://amazon-sagemaker-006230620263-us-west-2-f717bf909848/model-monitor/dev-endpoint-20250918-141753/manual-reports/e\n"
     ]
    }
   ],
   "source": [
    "manual_monitoring_job_output_s3_url = get_latest_monitoring_report_s3_url(\"sagemaker-model-monitor-analyzer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:42:28.048627Z",
     "iopub.status.busy": "2025-09-21T13:42:28.048335Z",
     "iopub.status.idle": "2025-09-21T13:42:29.188923Z",
     "shell.execute_reply": "2025-09-21T13:42:29.187931Z",
     "shell.execute_reply.started": "2025-09-21T13:42:28.048604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-21 13:32:43        274 constraint_violations.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls {manual_monitoring_job_output_s3_url}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:43:45.838539Z",
     "iopub.status.busy": "2025-09-21T13:43:45.838253Z",
     "iopub.status.idle": "2025-09-21T13:43:45.843100Z",
     "shell.execute_reply": "2025-09-21T13:43:45.842226Z",
     "shell.execute_reply.started": "2025-09-21T13:43:45.838515Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper to load a json file from S3\n",
    "def load_json_from_file(file_s3_url):\n",
    "    bucket = file_s3_url.split('/')[2]\n",
    "    key = '/'.join(file_s3_url.split('/')[3:])\n",
    "    print(f\"Load JSON from: {bucket}/{key}\")\n",
    "    \n",
    "    return json.loads(\n",
    "        s3_client.get_object(Bucket=bucket, \n",
    "                      Key=key)[\"Body\"].read().decode(\"utf-8\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:43:46.457052Z",
     "iopub.status.busy": "2025-09-21T13:43:46.456759Z",
     "iopub.status.idle": "2025-09-21T13:43:46.583008Z",
     "shell.execute_reply": "2025-09-21T13:43:46.582251Z",
     "shell.execute_reply.started": "2025-09-21T13:43:46.457030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load JSON from: amazon-sagemaker-006230620263-us-west-2-f717bf909848/model-monitor/dev-endpoint-20250918-141753/manual-reports/e/constraint_violations.json\n"
     ]
    }
   ],
   "source": [
    "violations = load_json_from_file(f\"{manual_monitoring_job_output_s3_url}/constraint_violations.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:43:48.262483Z",
     "iopub.status.busy": "2025-09-21T13:43:48.262203Z",
     "iopub.status.idle": "2025-09-21T13:43:48.273784Z",
     "shell.execute_reply": "2025-09-21T13:43:48.273094Z",
     "shell.execute_reply.started": "2025-09-21T13:43:48.262461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>constraint_check_type</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Extra columns</td>\n",
       "      <td>extra_column_check</td>\n",
       "      <td>There are extra columns in current dataset. Nu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature_name constraint_check_type  \\\n",
       "0  Extra columns    extra_column_check   \n",
       "\n",
       "                                         description  \n",
       "0  There are extra columns in current dataset. Nu...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.json_normalize(violations[\"violations\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:43:50.160710Z",
     "iopub.status.busy": "2025-09-21T13:43:50.160415Z",
     "iopub.status.idle": "2025-09-21T13:43:51.491210Z",
     "shell.execute_reply": "2025-09-21T13:43:51.490271Z",
     "shell.execute_reply.started": "2025-09-21T13:43:50.160687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://amazon-sagemaker-006230620263-us-west-2-f717bf909848/model-monitor/dev-endpoint-20250918-141753/manual-reports/e/constraint_violations.json to tmp/constraint_violations.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {manual_monitoring_job_output_s3_url}/constraint_violations.json ./tmp/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T13:43:52.980705Z",
     "iopub.status.busy": "2025-09-21T13:43:52.980395Z",
     "iopub.status.idle": "2025-09-21T13:43:53.152588Z",
     "shell.execute_reply": "2025-09-21T13:43:53.151357Z",
     "shell.execute_reply.started": "2025-09-21T13:43:52.980680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"violations\" : [ {\n",
      "    \"feature_name\" : \"Extra columns\",\n",
      "    \"constraint_check_type\" : \"extra_column_check\",\n",
      "    \"description\" : \"There are extra columns in current dataset. Number of columns in current dataset: 11, Number of columns in baseline constraints: 10\"\n",
      "  } ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "!head ./tmp/constraint_violations.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary and Next Steps\n",
    "\n",
    "Congratulations! You have successfully set up model monitoring following the correct flow:\n",
    "\n",
    "1. ‚úÖ **Discovered endpoints** with data capture enabled\n",
    "2. ‚úÖ **Generated traffic** to create captured data\n",
    "3. ‚úÖ **Created baselines** from captured data (or synthetic fallback)\n",
    "4. ‚úÖ **Set up monitoring schedules** for automated analysis\n",
    "\n",
    "This follows the same pattern as the reference notebook and represents best practices for model monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Cleanup Resources \n",
    "\n",
    "### 6.1 Stop Monitoring Schedules\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T20:04:12.955858Z",
     "iopub.status.busy": "2025-09-23T20:04:12.955594Z",
     "iopub.status.idle": "2025-09-23T20:04:12.963481Z",
     "shell.execute_reply": "2025-09-23T20:04:12.962228Z",
     "shell.execute_reply.started": "2025-09-23T20:04:12.955839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Cleanup code is ready but commented out for safety.\n",
      "   Uncomment the lines above if you want to clean up resources.\n"
     ]
    }
   ],
   "source": [
    "def cleanup_monitoring_resources():\n",
    "    \"\"\"Clean up monitoring resources to avoid ongoing costs\"\"\"\n",
    "    \n",
    "    print(\"üßπ CLEANING UP MONITORING RESOURCES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    cleanup_actions = []\n",
    "    \n",
    "    # Stop data quality monitoring schedule\n",
    "    if 'data_quality_schedule_name' in locals() and data_quality_schedule_created:\n",
    "        try:\n",
    "            data_quality_monitor.stop_monitoring_schedule()\n",
    "            cleanup_actions.append(f\"‚úÖ Stopped data quality schedule: {data_quality_schedule_name}\")\n",
    "        except Exception as e:\n",
    "            cleanup_actions.append(f\"‚ùå Error stopping data quality schedule: {e}\")\n",
    "    \n",
    "    # Delete monitoring schedules\n",
    "    try:\n",
    "        schedules = sm_client.list_monitoring_schedules(\n",
    "            EndpointName=endpoint_name if endpoint_name else \"dummy\"\n",
    "        )['MonitoringScheduleSummaries']\n",
    "        \n",
    "        for schedule in schedules:\n",
    "            schedule_name = schedule['MonitoringScheduleName']\n",
    "            try:\n",
    "                sm_client.delete_monitoring_schedule(\n",
    "                    MonitoringScheduleName=schedule_name\n",
    "                )\n",
    "                cleanup_actions.append(f\"‚úÖ Deleted schedule: {schedule_name}\")\n",
    "            except Exception as e:\n",
    "                cleanup_actions.append(f\"‚ùå Error deleting {schedule_name}: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        cleanup_actions.append(f\"‚ùå Error listing schedules: {e}\")\n",
    "    \n",
    "    # Show cleanup results\n",
    "    if cleanup_actions:\n",
    "        print(\"\\nüìã Cleanup Results:\")\n",
    "        for action in cleanup_actions:\n",
    "            print(f\"   {action}\")\n",
    "    else:\n",
    "        print(\"üí° No monitoring resources found to clean up\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è Note: This does not delete:\")\n",
    "    print(f\"   ‚Ä¢ S3 data (baselines, reports, captured data)\")\n",
    "    print(f\"   ‚Ä¢ CloudWatch alarms\")\n",
    "    print(f\"   ‚Ä¢ EventBridge rules\")\n",
    "    print(f\"   ‚Ä¢ The endpoint itself\")\n",
    "    \n",
    "    return len([a for a in cleanup_actions if \"‚úÖ\" in a])\n",
    "\n",
    "# Uncomment the lines below to run cleanup\n",
    "# cleanup_count = cleanup_monitoring_resources()\n",
    "# print(f\"\\nüéØ Cleaned up {cleanup_count} monitoring resources\")\n",
    "\n",
    "print(\"üí° Cleanup code is ready but commented out for safety.\")\n",
    "print(\"   Uncomment the lines above if you want to clean up resources.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
