{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Model Endpoint Testing\n",
    "\n",
    "This notebook demonstrates how to test the deployed model endpoint from Lab 4.\n",
    "\n",
    "## Prerequisites\n",
    "- Complete Lab 4 with approved and deployed model\n",
    "- Endpoint should be in 'InService' status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries and Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "def invoke_endpoint(endpoint_name, payload):\n",
    "    \"\"\"\n",
    "    Invoke a SageMaker endpoint with the given payload.\n",
    "    \n",
    "    Args:\n",
    "        endpoint_name (str): Name of the SageMaker endpoint\n",
    "        payload: The data to send to the endpoint (can be JSON, image bytes, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        The response from the endpoint\n",
    "    \"\"\"\n",
    "    # Create a SageMaker runtime client\n",
    "    runtime_client = boto3.client('sagemaker-runtime')\n",
    "    \n",
    "    try:\n",
    "        # Call the endpoint\n",
    "        response = runtime_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='application/json',  # Adjust based on your endpoint's requirements\n",
    "            Body=json.dumps(payload) if isinstance(payload, (dict, list)) else payload\n",
    "        )\n",
    "        \n",
    "        # Get the response body\n",
    "        response_body = response['Body'].read().decode('utf-8')\n",
    "        \n",
    "        # Parse the response if it's JSON\n",
    "        try:\n",
    "            return json.loads(response_body)\n",
    "        except json.JSONDecodeError:\n",
    "            return response_body\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking endpoint: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Endpoint and Payload\n",
    "\n",
    "**IMPORTANT**: Replace the endpoint name with your actual endpoint name from Lab 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured endpoint: meta-textgeneration-llama-3-2-3b-2025-11-28-16-58-14-582\n",
      "Test prompt: ### Instruction: What is Amazon SageMaker in one sentence?### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the payload\n",
    "payload = {\n",
    "    \"inputs\": \"\"\"### Instruction: What is Amazon SageMaker in one sentence?### Response:\\n\"\"\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"return_full_text\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Specify your endpoint name - REPLACE WITH YOUR ACTUAL ENDPOINT NAME\n",
    "endpoint_name = \"fine-tuning-mlops-project-p-XXXXXXXXX\"  # Replace with your actual endpoint name\n",
    "\n",
    "print(f\"Configured endpoint: {endpoint_name}\")\n",
    "print(f\"Test prompt: {payload['inputs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Response from endpoint:\n",
      "{\n",
      "  \"generated_text\": \"Amazon SageMaker is a fully managed service that provides developers and data scientists with the ability to build, train, and deploy machine learning models quickly and easily.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Invoke the endpoint and get the response\n",
    "try:\n",
    "    response = invoke_endpoint(endpoint_name, payload)\n",
    "    print(\"‚úÖ Response from endpoint:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to get response: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test with Additional Prompts\n",
    "\n",
    "Test the model with different prompts to validate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing multiple prompts...\n",
      "\n",
      "üß™ Test 1: What is machine learning?\n",
      "--------------------------------------------------\n",
      "‚úÖ Response:\n",
      "{\n",
      "  \"generated_text\": \"Machine learning is a subfield of artificial intelligence (AI) that focuses on the development of computer programs that can access data and use it to learn for themselves. Machine learning algorithms build a model based on sample data, identified trends and patterns, and make predictions or decisions with minimal human intervention.\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üß™ Test 2: Explain the benefits of MLOps.\n",
      "--------------------------------------------------\n",
      "‚úÖ Response:\n",
      "{\n",
      "  \"generated_text\": \"MLOps is a term that refers to the application of DevOps principles to machine learning (ML) and artificial intelligence (AI) development. It is a set of practices and tools that help organizations build, deploy, and maintain ML and AI models in production. MLOps is a relatively new field, but it has already gained traction in the industry. Here are some of the key benefits of MLOps:\\n\\n1. Increased model reliability: MLOps helps ensure that ML models are reliable and accurate by implementing best practices such as model validation, hyperparameter tuning, and model monitoring. This helps organizations avoid costly mistakes and ensure that\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üß™ Test 3: How does model governance work?\n",
      "--------------------------------------------------\n",
      "‚úÖ Response:\n",
      "{\n",
      "  \"generated_text\": \"Model governance is a process that ensures that models are developed, tested, and deployed in a way that is consistent with the organization's goals and values. It involves a set of practices and procedures that are designed to ensure that models are developed and deployed in a way that is consistent with the organization's goals and values. Model governance is a critical component of any organization's risk management strategy, as it helps to ensure that models are developed and deployed in a way that is consistent with the organization's risk management goals and values.\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üß™ Test 4: What is Amazon SageMaker?\n",
      "--------------------------------------------------\n",
      "‚úÖ Response:\n",
      "{\n",
      "  \"generated_text\": \"Amazon SageMaker is a fully managed service that provides everything you need to build, train, and deploy machine learning models. It is a fully managed service that provides everything you need to build, train, and deploy machine learning models. It is a fully managed service that provides everything you need to build, train, and deploy machine learning models. It is a fully managed service that provides everything you need to build, train, and deploy machine learning models. It is a fully managed service that provides everything you need to build, train, and deploy machine learning models. It is a fully managed service that provides everything you need to build, train,\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Additional test prompts\n",
    "test_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain the benefits of MLOps.\",\n",
    "    \"How does model governance work?\",\n",
    "    \"What is Amazon SageMaker?\"\n",
    "]\n",
    "\n",
    "print(\"Testing multiple prompts...\\n\")\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"üß™ Test {i}: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Create payload for this prompt\n",
    "    test_payload = {\n",
    "        \"inputs\": f\"inputs:### Instruction: {prompt}### Response:\\n\",\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 128,\n",
    "            \"top_p\": 0.9,\n",
    "            \"temperature\": 0.6,\n",
    "            \"return_full_text\": False,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = invoke_endpoint(endpoint_name, test_payload)\n",
    "        print(\"‚úÖ Response:\")\n",
    "        print(json.dumps(response, indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Custom Test\n",
    "\n",
    "Test with your own custom prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Custom Test: Your custom test prompt here\n",
      "--------------------------------------------------\n",
      "‚úÖ Response:\n",
      "{\n",
      "  \"generated_text\": \"The following is an input that contains an instruction paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is the difference between a 3D printer and a 3D scanner?\\n\\n### Input:\\nA 3D scanner is a device that captures a 3D image of an object. A 3D printer is a device that creates a 3D object from a 3D image.\\n\\n### Response:\\nA 3D scanner is a device that captures a 3D image of an object. A 3D printer is a device that creates a 3D object\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Enter your custom prompt here\n",
    "custom_prompt = \"Your custom test prompt here\"\n",
    "\n",
    "# Create custom payload\n",
    "custom_payload = {\n",
    "    \"inputs\": f\"inputs:### Instruction: {custom_prompt}### Response:\\n\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"return_full_text\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"üîß Custom Test: {custom_prompt}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    response = invoke_endpoint(endpoint_name, custom_payload)\n",
    "    print(\"‚úÖ Response:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Endpoint Status (Optional)\n",
    "\n",
    "Check the endpoint status and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Endpoint Status: InService\n",
      "üìç Endpoint ARN: arn:aws:sagemaker:us-west-2:975050068730:endpoint/meta-textgeneration-llama-3-2-3b-2025-11-28-16-58-14-582\n",
      "üïê Creation Time: 2025-11-28 16:58:16.383000+00:00\n",
      "üïê Last Modified: 2025-11-28 17:04:58.446000+00:00\n",
      "\n",
      "‚úÖ Endpoint is ready and operational!\n"
     ]
    }
   ],
   "source": [
    "# Create SageMaker client to check endpoint status\n",
    "sagemaker = boto3.client('sagemaker')\n",
    "\n",
    "try:\n",
    "    response = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "    print(f\"üìä Endpoint Status: {response['EndpointStatus']}\")\n",
    "    print(f\"üìç Endpoint ARN: {response['EndpointArn']}\")\n",
    "    print(f\"üïê Creation Time: {response['CreationTime']}\")\n",
    "    print(f\"üïê Last Modified: {response['LastModifiedTime']}\")\n",
    "    \n",
    "    if response['EndpointStatus'] == 'InService':\n",
    "        print(\"\\n‚úÖ Endpoint is ready and operational!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Endpoint status: {response['EndpointStatus']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error checking endpoint status: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "üéâ **Congratulations!** You have successfully tested your deployed model endpoint.\n",
    "\n",
    "### What You've Accomplished:\n",
    "\n",
    "‚úÖ **Model Deployment Validation**: Confirmed the endpoint is accessible and responding\n",
    "‚úÖ **Inference Testing**: Successfully executed inference requests with various prompts\n",
    "‚úÖ **Response Validation**: Verified the model generates appropriate responses\n",
    "‚úÖ **End-to-End Pipeline**: Validated the complete MLOps workflow from training to deployment\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "üîß **Payload Structure**: The model expects JSON with `inputs` and `parameters` fields\n",
    "üìä **Response Format**: The model returns structured JSON responses\n",
    "‚ö° **Performance**: The endpoint processes requests efficiently\n",
    "üîí **Governance**: The model went through proper approval workflow before deployment\n",
    "\n",
    "Your model is now ready for production use with proper governance controls in place!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
