{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Deployed LLaMA Fine-tuned Model Endpoint\n",
    "\n",
    "This notebook provides examples for testing your deployed SageMaker endpoint.\n",
    "\n",
    "## Prerequisites\n",
    "- Model has been approved in Model Registry\n",
    "- Deployment pipeline has completed\n",
    "- Endpoint is in 'InService' status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Initialize clients\n",
    "sm_client = boto3.client('sagemaker')\n",
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "print(\"‚úì Clients initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Your Endpoint\n",
    "\n",
    "List all endpoints to find yours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all endpoints\n",
    "response = sm_client.list_endpoints(\n",
    "    SortBy='CreationTime',\n",
    "    SortOrder='Descending',\n",
    "    MaxResults=10\n",
    ")\n",
    "\n",
    "print(\"Available Endpoints:\")\n",
    "print(\"-\" * 80)\n",
    "for endpoint in response['Endpoints']:\n",
    "    print(f\"Name: {endpoint['EndpointName']}\")\n",
    "    print(f\"Status: {endpoint['EndpointStatus']}\")\n",
    "    print(f\"Created: {endpoint['CreationTime']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your endpoint name here\n",
    "ENDPOINT_NAME = \"your-endpoint-name-here\"  # Replace with your actual endpoint name\n",
    "\n",
    "print(f\"Using endpoint: {ENDPOINT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Endpoint Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check endpoint status\n",
    "response = sm_client.describe_endpoint(EndpointName=ENDPOINT_NAME)\n",
    "print(f\"Endpoint Name: {response['EndpointName']}\")\n",
    "print(f\"Status: {response['EndpointStatus']}\")\n",
    "\n",
    "# Get instance details from ProductionVariants\n",
    "variant = response['ProductionVariants'][0]\n",
    "print(f\"Variant Name: {variant['VariantName']}\")\n",
    "print(f\"Current Instance Count: {variant.get('CurrentInstanceCount', 'N/A')}\")\n",
    "print(f\"Desired Instance Count: {variant.get('DesiredInstanceCount', 'N/A')}\")\n",
    "\n",
    "# To get InstanceType, you need to describe the endpoint config\n",
    "config_name = response['EndpointConfigName']\n",
    "config_response = sm_client.describe_endpoint_config(EndpointConfigName=config_name)\n",
    "print(f\"Instance Type: {config_response['ProductionVariants'][0]['InstanceType']}\")\n",
    "\n",
    "if response['EndpointStatus'] == 'InService':\n",
    "    print(\"\\n‚úÖ Endpoint is ready for inference!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Endpoint is {response['EndpointStatus']}. Please wait for it to be InService.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_endpoint(prompt, max_tokens=150, temperature=0.7, show_prompt=True):\n",
    "    \"\"\"\n",
    "    Invoke the SageMaker endpoint with a prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input prompt text\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (0.0-1.0)\n",
    "        show_prompt: Whether to display the prompt\n",
    "    \n",
    "    Returns:\n",
    "        Generated text\n",
    "    \"\"\"\n",
    "    # Prepare payload\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": 0.95,\n",
    "            \"do_sample\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if show_prompt:\n",
    "        print(\"üìù Prompt:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(prompt[:200] + \"...\" if len(prompt) > 200 else prompt)\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Invoke endpoint\n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName=ENDPOINT_NAME,\n",
    "        ContentType='application/json',\n",
    "        Body=json.dumps(payload)\n",
    "    )\n",
    "    \n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    # Parse response\n",
    "    result = json.loads(response['Body'].read().decode())\n",
    "    \n",
    "    # Extract generated text\n",
    "    if isinstance(result, list) and len(result) > 0:\n",
    "        generated_text = result[0].get('generated_text', '')\n",
    "    elif isinstance(result, dict):\n",
    "        generated_text = result.get('generated_text', result.get('outputs', ''))\n",
    "    else:\n",
    "        generated_text = str(result)\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è Latency: {latency:.2f}s\")\n",
    "    print(\"\\nüìÑ Generated Response:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(generated_text)\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Summarization Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Summarize the following text in 2-3 sentences.\n",
    "\n",
    "### Input:\n",
    "Machine learning is a subset of artificial intelligence that focuses on the development of algorithms and statistical models that enable computers to improve their performance on a specific task through experience. Unlike traditional programming where explicit instructions are provided, machine learning systems learn patterns from data and make decisions with minimal human intervention. This technology powers many modern applications including recommendation systems, image recognition, and natural language processing.\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "response = invoke_endpoint(prompt, max_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Answer the following question based on the context provided.\n",
    "\n",
    "### Input:\n",
    "Context: The Amazon rainforest, also known as Amazonia, is a moist broadleaf tropical rainforest in the Amazon biome that covers most of the Amazon basin of South America. The basin covers 7,000,000 square kilometers, of which 5,500,000 square kilometers are covered by the rainforest.\n",
    "\n",
    "Question: How large is the Amazon rainforest?\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "response = invoke_endpoint(prompt, max_tokens=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Instruction Following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Write a professional email to a customer apologizing for a delayed shipment.\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "response = invoke_endpoint(prompt, max_tokens=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Custom Prompt\n",
    "\n",
    "Try your own prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize this prompt\n",
    "custom_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Explain quantum computing in simple terms.\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "response = invoke_endpoint(custom_prompt, max_tokens=150, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing Example\n",
    "\n",
    "Process multiple prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "What is the capital of France?\n",
    "\n",
    "### Response:\n",
    "\"\"\",\n",
    "    \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "List 3 benefits of exercise.\n",
    "\n",
    "### Response:\n",
    "\"\"\",\n",
    "    \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Translate 'Hello, how are you?' to Spanish.\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing prompt {i}/{len(prompts)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    result = invoke_endpoint(prompt, max_tokens=100, show_prompt=False)\n",
    "    results.append(result)\n",
    "    time.sleep(0.5)  # Small delay between requests\n",
    "\n",
    "print(f\"\\n‚úÖ Processed {len(results)} prompts successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Testing\n",
    "\n",
    "Measure latency across multiple requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "test_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "What is machine learning?\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "num_requests = 5\n",
    "latencies = []\n",
    "\n",
    "print(f\"Running {num_requests} requests to measure latency...\\n\")\n",
    "\n",
    "for i in range(num_requests):\n",
    "    payload = {\n",
    "        \"inputs\": test_prompt,\n",
    "        \"parameters\": {\"max_new_tokens\": 50}\n",
    "    }\n",
    "    \n",
    "    start = time.time()\n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName=ENDPOINT_NAME,\n",
    "        ContentType='application/json',\n",
    "        Body=json.dumps(payload)\n",
    "    )\n",
    "    latency = time.time() - start\n",
    "    latencies.append(latency)\n",
    "    print(f\"Request {i+1}: {latency:.2f}s\")\n",
    "\n",
    "print(f\"\\nüìä Latency Statistics:\")\n",
    "print(f\"  Average: {statistics.mean(latencies):.2f}s\")\n",
    "print(f\"  Median: {statistics.median(latencies):.2f}s\")\n",
    "print(f\"  Min: {min(latencies):.2f}s\")\n",
    "print(f\"  Max: {max(latencies):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "‚ö†Ô∏è **Warning:** This will delete your endpoint and stop all inference capabilities.\n",
    "\n",
    "Only run this if you want to delete the endpoint to save costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete endpoint\n",
    "# sm_client.delete_endpoint(EndpointName=ENDPOINT_NAME)\n",
    "# print(f\"‚úì Endpoint {ENDPOINT_NAME} deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- ‚úÖ Finding and checking endpoint status\n",
    "- ‚úÖ Invoking endpoint with different prompts\n",
    "- ‚úÖ Batch processing multiple requests\n",
    "- ‚úÖ Measuring inference latency\n",
    "\n",
    "### Next Steps:\n",
    "1. Integrate endpoint into your application\n",
    "2. Monitor CloudWatch metrics\n",
    "3. Set up auto-scaling if needed\n",
    "4. Configure alarms for monitoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
