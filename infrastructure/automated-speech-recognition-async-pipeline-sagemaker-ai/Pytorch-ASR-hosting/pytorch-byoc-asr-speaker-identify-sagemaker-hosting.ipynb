{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db53f808-9fbd-408d-a6a0-d18200733876",
   "metadata": {},
   "source": [
    "# NVIDIA Parakeet ASR Model Deployment on Amazon SageMaker AI with speaker diarization\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to deploy the [**NVIDIA Parakeet TDT 0.6B v2**](https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2) model for Automatic Speech Recognition (ASR) tasks using Amazon SageMaker with both real-time and asynchronous inference capabilities.\n",
    "\n",
    "### About NVIDIA Parakeet TDT 0.6B v2\n",
    "\n",
    "The **Parakeet TDT (Transducer-Decoder-Transducer) 0.6B v2** is a state-of-the-art neural speech recognition model developed by NVIDIA:\n",
    "\n",
    "- **Architecture**: Transformer-based transducer model optimized for streaming ASR\n",
    "- **Model Size**: 600 million parameters, balancing accuracy and efficiency\n",
    "- **Performance**: Excellent accuracy on diverse speech datasets with low latency\n",
    "- **Language Support**: Primarily English, with robust performance across accents\n",
    "- **Optimization**: Built with NVIDIA NeMo framework for production deployment\n",
    "\n",
    "### Speaker diarization with pyannote\n",
    "\n",
    "In this example, we use [pyannote speaker diarization](https://huggingface.co/pyannote/speaker-diarization-3.1) to identify speakers and match the speakers with the transcriptions. It is a speaker diarization pipeline (version 3.1) by pyannote that processes mono audio files sampled at 16kHz to identify and separate different speakers in audio recordings. \n",
    "\n",
    "It's an improvement over version 3.0 as it runs in pure PyTorch without onnxruntime, making deployment easier and potentially faster. The model can automatically handle stereo/multi-channel audio conversion to mono and resampling to 16kHz.\n",
    "\n",
    "We hosted both the parakeet and pyannote on the same endpoint, the basic process is:\n",
    "\n",
    "parakeet for (transcription + timestamp) --> pyannote for (speaker + timestamp) --> use the time stamp to match them up\n",
    "\n",
    "\n",
    "### SageMaker Asynchronous Inference Benefits\n",
    "\n",
    "**Asynchronous inference** is particularly well-suited for ASR workloads:\n",
    "\n",
    "1. **Long Processing Times**: Audio transcription can take several seconds to one hour\n",
    "2. **Variable Input Sizes**: Audio files range from seconds to hours in duration\n",
    "3. **Managed queuing**: Efficiently handle multiple audio files simultaneously\n",
    "4. **Cost Optimization**: Pay only for actual processing time, with automatic scaling to zero\n",
    "5. **Large File Support**: Handle audio files up to 1GB in size\n",
    "6. **Event driven pipeline**: Async endpoint supports SNS which helps with building a event driven pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a75f20-21ff-4d05-a0e8-50a6ceaa49c2",
   "metadata": {},
   "source": [
    "## Prerequisites and Setup\n",
    "\n",
    "**❗If you run this notebook in SageMaker Studio, please consider building the custom docker container from alternative services. This notebook was tested using SageMaker notebook instance with ml.g5.2xlarge and EBS volume 100G.**\n",
    "\n",
    "### Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12501f18-d807-4337-b4e9-7e1c2d7590df",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install datasets\n",
    "%pip install sagemaker==2.246.0 huggingface_hub\n",
    "%pip install librosa -q\n",
    "%pip install soundfile -q\n",
    "%pip install libcst==1.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9229164",
   "metadata": {},
   "source": [
    "Also install the nemo toolkit for asr, it will take a few mins for the installation to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f708b2d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install nemo_toolkit['asr']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab1118a-a0fa-4140-a4c8-7725f512f772",
   "metadata": {},
   "source": [
    "**❗Please restart the kernel before executing the cells below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fec1136-d7b0-402f-b959-d0302680c508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import sagemaker\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import soundfile as sf\n",
    "import nemo.collections.asr as nemo_asr\n",
    "\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56301238",
   "metadata": {},
   "source": [
    "## Configuration Setup\n",
    "\n",
    "Initialize the basic configuration for SageMaker deployment:\n",
    "\n",
    "- **SageMaker Session**: Manages interactions with SageMaker services\n",
    "- **S3 Bucket**: Default bucket for storing model artifacts and data\n",
    "- **IAM Role**: Execution role with necessary permissions for SageMaker\n",
    "- **S3 Prefixes**: Organized folder structure for model artifacts\n",
    "- **Runtime Client**: For invoking endpoints after deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ea851-3d8c-42d6-bcbe-334d7cde01ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic configurations\n",
    "sess = sagemaker.session.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'parakeet-asr-speaker'\n",
    "role = sagemaker.get_execution_role()\n",
    "s3_model_prefix = (\n",
    "    \"hf-asr-models/nvidia-asr-speaker\"  # folder within bucket where code artifact will go\n",
    ")\n",
    "# below boto3 clients are for invoking asynchronous endpoint \n",
    "sm_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "region = sess.boto_region_name\n",
    "account_id = sess.account_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb0f2b-1dd6-4624-ad4b-b65a19937d2d",
   "metadata": {},
   "source": [
    "## Model Preparation and Testing (Optional)\n",
    "\n",
    "### Local Model Testing\n",
    "\n",
    "Before deploying to SageMaker, we'll first test the Parakeet model locally to ensure it works correctly. This step helps validate:\n",
    "\n",
    "- Model loading and initialization\n",
    "- Audio file processing capabilities\n",
    "\n",
    "### Download Model from HuggingFace Hub\n",
    "\n",
    "Download the NVIDIA Parakeet model from HuggingFace Hub for SageMaker deployment:\n",
    "\n",
    "- **Model Repository**: `nvidia/parakeet-tdt-0.6b-v2`\n",
    "- **File Filtering**: Only download necessary files (*.json, *.safetensors, *.nemo)\n",
    "- **Local Caching**: Store model files locally for packaging\n",
    "- **LFS Support**: Handle large model files using Git LFS\n",
    "\n",
    "#### Uncomment below cells to load the parakeet model to local notebook and test the model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2376d9-c0d1-4129-b082-7ac72c7ba1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import snapshot_download\n",
    "# from pathlib import Path\n",
    "# import os\n",
    "# import sagemaker\n",
    "# import jinja2\n",
    "\n",
    "# parakeet = \"nvidia/parakeet-tdt-0.6b-v2\"\n",
    "\n",
    "# # - This will download the model into the current directory where ever the jupyter notebook is running\n",
    "# local_model_path = Path(\".\")\n",
    "# local_model_path.mkdir(exist_ok=True)\n",
    "# model_name = parakeet\n",
    "# # Only download pytorch checkpoint files\n",
    "# allow_patterns = [\"*.json\", \"*.safetensors\", \"*.nemo\"]\n",
    "\n",
    "# # - Leverage the snapshot library to donload the model since the model is stored in repository using LFS\n",
    "# model_download_path = snapshot_download(\n",
    "#     repo_id=model_name,\n",
    "#     cache_dir=local_model_path,\n",
    "#     allow_patterns=allow_patterns,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6071c7-dfac-4114-ab17-c71bfaf5df66",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = nemo_asr.models.ASRModel.restore_from(restore_path=model_download_path + \"/parakeet-tdt-0.6b-v2.nemo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a9318a-f79f-4fe7-aafd-7f32aa8d8c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model.transcribe(['../data/test.wav'])\n",
    "# print(output[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a6e606",
   "metadata": {},
   "source": [
    "### Package Model for SageMaker\n",
    "\n",
    "Create a compressed archive of the model files for SageMaker deployment. Note that, we can also load the model directly from huggingface during the endpoint creation during our development stage. This way we don't need to archive the model artifacts and upload to s3. \n",
    "\n",
    "In this example, we choose to use this way and load both models during the endpoint creation time. **Please note that, Pyannote models on HuggingFace are gated models that require authentication and user agreement before access.** To use the model, you need to go to the huggingface model page and accept the terms. \n",
    "\n",
    "❗Also please make sure to update your **huggingface token** in the 'inference.py' file, see below, to make sure the model is able to be downloaded for the endpoint.\n",
    "\n",
    "\n",
    "For production use case, consider switching to [pyannoteAI](https://www.pyannote.ai/) for better and faster options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2b2ea183-b07c-4885-a2c0-51204ec28e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "import boto3\n",
    "import tempfile\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.collections.asr.models import EncDecCTCModelBPE\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Load and return the model\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    # model = EncDecCTCModelBPE.restore_from(restore_path=\"parakeet-tdt-0.6b-v2.nemo\").to(DEVICE)\n",
    "    model = EncDecCTCModelBPE.from_pretrained(\"nvidia/parakeet-tdt-0.6b-v2\").to(DEVICE)\n",
    "    \n",
    "    # Enable local attention\n",
    "    model.change_attention_model(\"rel_pos_local_attn\", [128, 128])  # local attn\n",
    "     \n",
    "    # Enable chunking for subsampling module\n",
    "    model.change_subsampling_conv_chunking_factor(1)  # 1 = auto select\n",
    "    print(f'parakeet model has been loaded to this device: {model.device.type}')\n",
    "    models.append(model)\n",
    "    \n",
    "    # Load diarization model\n",
    "    model_diarization = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\",\n",
    "        use_auth_token=\"<your huggingface token>\" ## <--- Make sure to update your huggingface token here\n",
    "    ).to(DEVICE)\n",
    "    print(f'pyannote model has been loaded to this device: {model.device.type}')\n",
    "    models.append(model_diarization)\n",
    "    return models\n",
    "\n",
    "def transform_fn(model, request_body, request_content_type, response_content_type=\"application/json\"):\n",
    "    \"\"\"\n",
    "    Transform the input data and generate a transcription result\n",
    "    \"\"\"\n",
    "    logging.info(\"Check out the request_body type: %s\", type(request_body))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    file = io.BytesIO(request_body)\n",
    "    tfile = tempfile.NamedTemporaryFile(delete=True)\n",
    "    tfile.write(file.read())\n",
    "\n",
    "\n",
    "\n",
    "    logging.info(\"Start generating the transcription ...\")\n",
    "    transcription = model[0].transcribe([tfile.name], timestamps=True)\n",
    "    logging.info(\"Transcription generation completed.\")\n",
    "    result = transcription[0]\n",
    "    text = result.text\n",
    "    \n",
    "    # Create segments\n",
    "    segments = []\n",
    "    if hasattr(result, 'timestamp') and 'segment' in result.timestamp:\n",
    "        for i, stamp in enumerate(result.timestamp['segment']):\n",
    "            segments.append({\n",
    "                \"id\": i,\n",
    "                \"start\": stamp['start'],\n",
    "                \"end\": stamp['end'],\n",
    "                \"text\": stamp['segment'],\n",
    "                \"speaker\": None\n",
    "            })\n",
    "    else:\n",
    "        # Single segment for entire transcription\n",
    "        segments.append({\n",
    "            \"id\": 0,\n",
    "            \"start\": 0.0,\n",
    "            \"end\": len(text.split()) / 2.0,\n",
    "            \"text\": text,\n",
    "            \"speaker\": None\n",
    "        })\n",
    "    diarization = model[1](tfile.name, num_speakers=None)\n",
    "\n",
    "    # Extract speaker segments\n",
    "    speaker_segments = []\n",
    "    speakers = set()\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        speaker_id = f\"SPEAKER_{speaker}\" if not str(speaker).startswith(\"SPEAKER_\") else str(speaker)\n",
    "        speaker_segments.append({\n",
    "            \"start\": turn.start,\n",
    "            \"end\": turn.end,\n",
    "            \"speaker\": speaker_id\n",
    "        })\n",
    "        speakers.add(speaker_id)\n",
    "    \n",
    "    # Sort by start time\n",
    "    speaker_segments.sort(key=lambda x: x['start'])\n",
    "    \n",
    "    print(f\"Found {len(speakers)} speakers in {len(speaker_segments)} segments\")\n",
    "\n",
    "    print(\"Merging transcription with speaker information...\")\n",
    "    for segment in segments:\n",
    "        start = segment['start']\n",
    "        end = segment['end']\n",
    "        \n",
    "        # Find overlapping speaker segments\n",
    "        overlapping = []\n",
    "        for spk_segment in speaker_segments:\n",
    "            overlap_start = max(start, spk_segment['start'])\n",
    "            overlap_end = min(end, spk_segment['end'])\n",
    "            \n",
    "            if overlap_end > overlap_start:\n",
    "                duration = overlap_end - overlap_start\n",
    "                overlapping.append((spk_segment['speaker'], duration))\n",
    "        \n",
    "        # Assign speaker with most overlap\n",
    "        if overlapping:\n",
    "            overlapping.sort(key=lambda x: x[1], reverse=True)\n",
    "            segment['speaker'] = overlapping[0][0]\n",
    "        else:\n",
    "            segment['speaker'] = \"unknown\"\n",
    "    \n",
    "    #Format final text with speaker labels\n",
    "    text_parts = []\n",
    "    previous_speaker = None\n",
    "    \n",
    "    for segment in segments:\n",
    "        text = segment['text'].strip()\n",
    "        speaker = segment['speaker']\n",
    "        \n",
    "        if speaker and speaker != previous_speaker:\n",
    "            # Extract speaker number for cleaner display\n",
    "            try:\n",
    "                if speaker.startswith(\"SPEAKER_\"):\n",
    "                    speaker_num = int(speaker.split(\"_\")[-1]) + 1\n",
    "                    text = f\"Speaker {speaker_num}: {text}\"\n",
    "                else:\n",
    "                    text = f\"{speaker}: {text}\"\n",
    "            except (ValueError, IndexError):\n",
    "                text = f\"Speaker: {text}\"\n",
    "            previous_speaker = speaker\n",
    "        \n",
    "        text_parts.append(text)\n",
    "    \n",
    "    final_text = \" \".join(text_parts)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logging.info(\"Elapsed time: %s seconds\", elapsed_time)\n",
    "    # if hasattr(result[0], 'text'):\n",
    "    #     text_result = [hyp.text for hyp in result]\n",
    "    # else:\n",
    "    #     text_result = result\n",
    "    text_result = {\n",
    "            \"text\": final_text,\n",
    "            \"segments\": segments,\n",
    "            \"num_speakers\": len(speakers),\n",
    "            \"total_segments\": len(segments)\n",
    "        }\n",
    "    return json.dumps(text_result), response_content_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dcbba8-1058-4d3c-88cd-6a7bad23f4b8",
   "metadata": {},
   "source": [
    "We directly packaged the entry point script 'inference.py' under the code folder in the model.tar.gz. You can specify the `SAGEMAKER_PROGRAM` and `SAGEMAKER_SUBMIT_DIRECTORY` during model creation, SageMaker will not repackage the model tarball file with entrypoint script and can directly use this inference.py file at the endpoint. Otherwise, you can provide the `entry_point` and `source_dir` parameters when creating the PyTorch model object, and during the endpoint creation time, SageMaker will repackage these files within the model.tar.gz for the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83adfc6a-f0f4-4285-add8-311ae7ced626",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -cvzf model.tar.gz code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1afbb5-8bd8-479d-a3c5-9e08931fe2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = sess.upload_data('model.tar.gz', bucket=bucket, key_prefix=s3_model_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9da4e8",
   "metadata": {},
   "source": [
    "### Create Custom Docker Image by extending from AWS prebuilt PyTorch container\n",
    "\n",
    "Build a custom Docker image optimized for NVIDIA Parakeet ASR model:\n",
    "\n",
    "**Base Image**: PyTorch inference container with GPU support\n",
    "- `pytorch-inference:2.6.0-gpu-py312-cu124-ubuntu22.04-sagemaker`\n",
    "\n",
    "**System Dependencies**:\n",
    "- **ffmpeg**: Audio format conversion and processing\n",
    "- **libsndfile1**: Audio file I/O library\n",
    "\n",
    "**Python Dependencies**:\n",
    "- **nemo_toolkit[asr]**: NVIDIA NeMo framework for ASR\n",
    "- **ffmpeg-python**: Python wrapper for ffmpeg\n",
    "- **soundfile**: Audio file reading/writing\n",
    "- **pyannote.audio**: Pyannote model package\n",
    "- **pydub**\n",
    "- **pydantic**\n",
    "\n",
    "Note that the notebook was tested in 'us-west-2', if you are using other region, please check the prebuilt image uri in that region from the [available images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327538f4-f49b-4bac-9c03-bf0df848326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "# SageMaker PyTorch image\n",
    "FROM 763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:2.6.0-gpu-py312-cu124-ubuntu22.04-sagemaker\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    ffmpeg \\\n",
    "    libsndfile1 \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN pip install nemo_toolkit[asr] ffmpeg-python soundfile cuda-python \n",
    "RUN pip install pyannote.audio pydub pydantic typing-extensions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c995fcf",
   "metadata": {},
   "source": [
    "### Configuring Docker Storage on Amazon SageMaker Notebook Instances\n",
    "Amazon SageMaker notebook instances come with a 5 GB Amazon EBS storage volume by default, but Docker uses the instance's root volume for storing images and containers. When building multiple Docker images, the root volume has limited disk space that can quickly run out of space, causing \"no space left on device\" errors.\n",
    "\n",
    "To solve this storage limitation, you can redirect Docker to use the larger EBS volume by modifying the Docker daemon configuration. Note that the EBS volume specified for the SageMaker Notebook instance is under path `/home/ec2-user/SageMaker`, but when you go to terminal the default path is `/home/ec2-user`.\n",
    "\n",
    "From the terminal, you can stop the Docker service and edit the Docker daemon configuration.\n",
    "\n",
    "```\n",
    "# Stop the Docker service\n",
    "sudo systemctl stop docker\n",
    "\n",
    "# Create the new Docker data directory on the EBS volume\n",
    "sudo mkdir -p /home/ec2-user/SageMaker/docker\n",
    "\n",
    "# Create or edit the Docker daemon configuration file\n",
    "sudo nano /etc/docker/daemon.json\n",
    "```\n",
    "\n",
    "Add the following configuration to /etc/docker/daemon.json:\n",
    "```json\n",
    "{\n",
    "    \"runtimes\": {\n",
    "        \"nvidia\": {\n",
    "            \"args\": [],\n",
    "            \"path\": \"nvidia-container-runtime\"\n",
    "        }\n",
    "    },\n",
    "    \"data-root\": \"/home/ec2-user/SageMaker/docker\"\n",
    "}\n",
    "```\n",
    "\n",
    "Restart the docker service\n",
    "```\n",
    "# Start the Docker service with new configuration\n",
    "sudo systemctl start docker\n",
    "\n",
    "# Verify Docker is running with correct data directory\n",
    "docker info | grep \"Docker Root Dir\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a30252",
   "metadata": {},
   "source": [
    "### Build and Push Docker Image to ECR\n",
    "\n",
    "Build the custom Docker image and push it to Amazon Elastic Container Registry (ECR):\n",
    "\n",
    "**Process Overview**:\n",
    "1. **ECR Repository**: Create or verify ECR repository exists\n",
    "2. **Docker Build**: Build the custom image with all dependencies\n",
    "3. **Authentication**: Login to ECR using AWS credentials\n",
    "4. **Tag and Push**: Tag the image and push to ECR\n",
    "\n",
    "**Repository Naming**: `asr-sagemaker` for easy identification\n",
    "\n",
    "Note that, you need to make sure the IAM role has proper permission to access the ECR service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b1956e-8048-466d-bb0e-766252a0ac3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\n",
    "REGION=$(aws configure get region)\n",
    "REPOSITORY_NAME=asr-sagemaker-speaker\n",
    "\n",
    "# login to access the base image from the prebuilt images\n",
    "aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin 763104351884.dkr.ecr.$REGION.amazonaws.com\n",
    "\n",
    "# Create ECR repository if needed\n",
    "if aws ecr describe-repositories --repository-names \"${REPOSITORY_NAME}\" &>/dev/null; then\n",
    "    echo \"Repository ${REPOSITORY_NAME} already exists\"\n",
    "else\n",
    "    echo \"Creating ECR repository ${REPOSITORY_NAME}...\"\n",
    "    aws ecr create-repository \\\n",
    "        --repository-name \"${REPOSITORY_NAME}\" \\\n",
    "        --region \"${REGION}\"\n",
    "fi\n",
    "\n",
    "#build docker image and push to ECR repository\n",
    "docker build -t $REPOSITORY_NAME .\n",
    "aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com\n",
    "docker tag $REPOSITORY_NAME:latest $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$REPOSITORY_NAME:latest\n",
    "docker push $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$REPOSITORY_NAME:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc49067-13c1-4512-b453-aead1c46164c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate a unique model name and provide image uri\n",
    "\n",
    "id = int(time.time())\n",
    "model_name = f'parakeet-model-{id}'\n",
    "\n",
    "# !Please change the image URI for the region that you are using: e.g. us-east-1\n",
    "image = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/asr-sagemaker-speaker:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b06cc44-3c20-4815-8323-d6f1cf9e9dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.local import LocalSession\n",
    "\n",
    "local_session = LocalSession()\n",
    "local_session.config = {'local': {'local_code': True}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10a2c82",
   "metadata": {},
   "source": [
    "### Create PyTorch Model Object\n",
    "\n",
    "Create a SageMaker PyTorchModel with specific environment variables setup for async workloads:\n",
    "\n",
    "**Key Configuration Parameters**:\n",
    "- **SAGEMAKER_MODEL_SERVER_WORKERS**: set the number of torch worker that will load the the number of model copied into GPU memory\n",
    "- **TS_DEFAULT_RESPONSE_TIMEOUT**: time out setting for Torch server worker, for long audio processing you can set it to a higher number\n",
    "- **TS_MAX_REQUEST_SIZE**: byte size values for request, set to 1G for async endpoint\n",
    "- **TS_MAX_RESPONSE_SIZE**: byte size values for response\n",
    "- **SAGEMAKER_PROGRAM**: Points to inference.py script\n",
    "\n",
    "**Session Selection**: Switch between local testing and cloud deployment\n",
    "SageMaker local session is a feature in the SageMaker Python SDK that allows you to create estimators and run training, processing, and inference jobs locally using Docker containers instead of managed AWS infrastructure, providing a fast way to test and debug your machine learning scripts before scaling to production. You can see more examples in this [github repo](https://github.com/aws-samples/amazon-sagemaker-local-mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9682718-6ddf-4a75-a99e-7df8ba9a9c03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a PyTorchModel for deployment\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "parakeet_model = PyTorchModel(\n",
    "    # entry_point=\"inference.py\",\n",
    "    # source_dir=\"code\",\n",
    "    model_data=model_uri,\n",
    "    image_uri=image,\n",
    "    role=role,\n",
    "    name=model_name,\n",
    "    env={\"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
    "         \"TS_MAX_REQUEST_SIZE\": \"1073741824\",\n",
    "         \"TS_MAX_RESPONSE_SIZE\": \"1073741824\",\n",
    "         \"TS_DEFAULT_RESPONSE_TIMEOUT\": \"300\",\n",
    "         \"SAGEMAKER_SUBMIT_DIRECTORY\":\"/opt/ml/model/code\",\n",
    "         \"SAGEMAKER_PROGRAM\":\"inference.py\",\n",
    "        },\n",
    "    # sagemaker_session=local_session # used for local test\n",
    "    sagemaker_session=sess  # used for actual endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee32e99-edb5-43da-9bd4-015d4477933d",
   "metadata": {},
   "source": [
    "### Real-time inference \n",
    "\n",
    "Set up data serialization for audio file processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25574fd6-d888-47b3-beb4-ab9a441d648a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serializers import DataSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Define serializers and deserializer\n",
    "audio_serializer = DataSerializer(content_type=\"audio/x-audio\")\n",
    "deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2077ac87",
   "metadata": {},
   "source": [
    "### Deploy Real-time Endpoint\n",
    "\n",
    "Deploy the model as a real-time inference endpoint. Note that if you choose to use the local SageMaker Session when creating the model object, change the `instance_type` to `local_gpu` to be able to quickly test the endpoint from local SageMaker notebook instance for fast testing. If you are going to deploy the model to an async endpoint, please make sure you create the PyTorchModel object with the actual sagemaker session. In this case, it will be `sess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a97d22-3990-4024-b859-1df12da7741d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Deploy the model for real-time inference locally or remotely\n",
    "endpoint_name = f'parakeet-realtime-endpoint-speaker-identify-{id}'\n",
    "\n",
    "real_time_predictor = parakeet_model.deploy(\n",
    "    initial_instance_count=1,  # number of instances\n",
    "    # instance_type=\"local_gpu\",\n",
    "    instance_type=\"ml.g5.xlarge\",  # instance type\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=audio_serializer,\n",
    "    deserializer=deserializer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bb24c1",
   "metadata": {},
   "source": [
    "### Test Real-time Inference\n",
    "\n",
    "Test the deployed real-time endpoint with a sample audio file:\n",
    "\n",
    "- **Input**: Audio file path (automatically serialized)\n",
    "- **Processing**: Synchronous transcription\n",
    "- **Output**: JSON response with transcription results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8635750c-1d6e-48b6-ba28-3122924392da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Perform real-time inference\n",
    "audio_path = \"../data/medical-diarization.wav\"\n",
    "response = real_time_predictor.predict(data=audio_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f6e846-08e7-4265-a9ec-76bb9ba8c0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"=\" * 50)\n",
    "print(\"TRANSCRIPTION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Number of speakers: {response['num_speakers']}\")\n",
    "print(f\"Total segments: {response['total_segments']}\")\n",
    "print(\"\\nFull text with speaker labels:\")\n",
    "print(\"-\" * 30)\n",
    "print(response['text'])\n",
    "\n",
    "print(\"\\nDetailed segments:\")\n",
    "print(\"-\" * 30)\n",
    "for segment in response['segments']:\n",
    "    speaker = segment['speaker'] or 'Unknown'\n",
    "    print(f\"[{segment['start']:.2f}s - {segment['end']:.2f}s] {speaker}: {segment['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfac6e3",
   "metadata": {},
   "source": [
    "Uncomment below cell to delete the endpoint once you finish testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfdd88c-8ded-408c-b1ba-aa9b273d1d2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## optional: Delete real-time inference endpoint\n",
    "real_time_predictor.delete_endpoint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579c16fd-99e0-4178-8845-10a0fdaf02ac",
   "metadata": {},
   "source": [
    "## Asynchronous Inference Deployment\n",
    "\n",
    "Set up asynchronous inference configuration with comprehensive monitoring:\n",
    "\n",
    "**Configuration Components**:\n",
    "- **Output Path**: S3 location for storing transcription results\n",
    "- **Concurrency**: Maximum concurrent invocations per instance (4 for optimal GPU usage)\n",
    "- **SNS Notifications**: Real-time alerts for job completion status\n",
    "- **Failure Path**: Separate S3 location for failed job artifacts\n",
    "\n",
    "**SNS Topics**: Configure separate topics for success and failure notifications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d8d055",
   "metadata": {},
   "source": [
    "### Create SNS Topics for Async Notifications\n",
    "\n",
    "Create SNS topics for monitoring asynchronous inference job status:\n",
    "\n",
    "- **Success Topic**: Receives notifications when transcription jobs complete successfully\n",
    "- **Error Topic**: Receives notifications when transcription jobs fail\n",
    "- **Automatic Creation**: Creates topics if they don't exist, reuses if they do\n",
    "- **Subscription Ready**: Topics are ready for email, SMS, or Lambda subscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf4e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_client = boto3.client('sts')\n",
    "\n",
    "print(f\"Account ID: {account_id}\")\n",
    "print(f\"Region: {region}\")\n",
    "\n",
    "# Initialize SNS client\n",
    "sns_client = boto3.client('sns')\n",
    "\n",
    "def create_sns_topic_if_not_exists(topic_name, description):\n",
    "    \"\"\"Create SNS topic if it doesn't exist, return the ARN\"\"\"\n",
    "    try:\n",
    "        # Try to create the topic (idempotent operation)\n",
    "        response = sns_client.create_topic(Name=topic_name)\n",
    "        topic_arn = response['TopicArn']\n",
    "        \n",
    "        # Set topic attributes for better identification\n",
    "        sns_client.set_topic_attributes(\n",
    "            TopicArn=topic_arn,\n",
    "            AttributeName='DisplayName',\n",
    "            AttributeValue=description\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Topic '{topic_name}' ready: {topic_arn}\")\n",
    "        return topic_arn\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"❌ Error creating topic '{topic_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "# Create success topic\n",
    "success_topic_name = \"async-success\"\n",
    "success_description = \"SageMaker Async Inference Success Notifications\"\n",
    "success_topic_arn = create_sns_topic_if_not_exists(success_topic_name, success_description)\n",
    "\n",
    "# Create error topic  \n",
    "error_topic_name = \"async-failed\"\n",
    "error_description = \"SageMaker Async Inference Error Notifications\"\n",
    "error_topic_arn = create_sns_topic_if_not_exists(error_topic_name, error_description)\n",
    "\n",
    "print(f\"\\n📧 SNS Topics Created Successfully:\")\n",
    "print(f\"Success Topic ARN: {success_topic_arn}\")\n",
    "print(f\"Error Topic ARN: {error_topic_arn}\")\n",
    "\n",
    "print(f\"\\n🔧 Topics are ready for AsyncInferenceConfig!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3470dd4a-f898-4344-9e65-bc95f0fa76b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sagemaker.async_inference import AsyncInferenceConfig\n",
    "\n",
    "# Create an AsyncInferenceConfig object\n",
    "async_config = AsyncInferenceConfig(\n",
    "    output_path=f\"s3://{bucket}/{prefix}/output\", \n",
    "    max_concurrent_invocations_per_instance = 4,\n",
    "    notification_config = {\n",
    "      \"SuccessTopic\": f\"arn:aws:sns:{region}:{account_id}:async-success\",\n",
    "      \"ErrorTopic\": f\"arn:aws:sns:{region}:{account_id}:async-failed\",\n",
    "    }, #  Notification configuration \n",
    "    failure_path=f\"s3://{bucket}/{prefix}/failed\"\n",
    ")\n",
    "\n",
    "# Deploy the model for async inference\n",
    "endpoint_name = f'parakeet-speaker-identify-async-endpoint-{id}'\n",
    "async_predictor = parakeet_model.deploy(\n",
    "    async_inference_config=async_config,\n",
    "    initial_instance_count=1, # number of instances\n",
    "    instance_type ='ml.g5.xlarge', # instance type\n",
    "    endpoint_name = endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473cc709",
   "metadata": {},
   "source": [
    "Upload test audio file to S3 for asynchronous processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ec77c-87a2-45f0-818d-ce49844558d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(s3_client, file_path, bucket_name, s3_key):\n",
    "    \"\"\"Upload file to S3\"\"\"\n",
    "    try:\n",
    "        s3_client.upload_file(file_path, bucket_name, s3_key)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading {s3_key}: {e}\")\n",
    "        return False\n",
    "s3_client = boto3.client('s3')\n",
    "audio_path = \"../data/test_audio.wav\" \n",
    "s3_key = prefix+f\"/data/{audio_path}\"\n",
    "upload_to_s3(s3_client, audio_path, bucket, s3_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9bf42a-5cba-49fb-96d2-4ac40c96d5fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Provide the S3 path for the audio file you want to processs\n",
    "\n",
    "input_path = f\"s3://{bucket}/{s3_key}\"\n",
    "input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddbb3d2-0b8d-4431-9cdb-6f48acaad709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform async inference\n",
    "initial_args = {'ContentType':\"audio/x-audio\"}\n",
    "response = async_predictor.predict_async(initial_args = initial_args, input_path=input_path)\n",
    "response.output_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeb24e8",
   "metadata": {},
   "source": [
    "Monitor and retrieve the results from asynchronous processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd11808-f270-4e04-b83e-6b76d7b87ecc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib, time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def get_output(output_location):\n",
    "    output_url = urllib.parse.urlparse(output_location)\n",
    "    bucket = output_url.netloc\n",
    "    key = output_url.path[1:]\n",
    "    while True:\n",
    "        try:\n",
    "            return sess.read_s3_file(\n",
    "                        bucket=output_url.netloc, \n",
    "                        key_prefix=output_url.path[1:])\n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == 'NoSuchKey':\n",
    "                print(\"waiting for output...\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            raise\n",
    "            \n",
    "output = get_output(response.output_path)\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d804fecd-42d2-494e-8169-9c48f5fec233",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Optional: Advanced Configuration Auto-scaling for Asynchronous Inference\n",
    "\n",
    "\n",
    "Auto-scaling supports scale down to zero which can help with cost saving when there are no workloads requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16a29a-67ac-4884-94ba-e2424debe2cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "autoscale = boto3.client('application-autoscaling') \n",
    "resource_id='endpoint/' + endpoint_name + '/variant/' + 'AllTraffic'\n",
    "\n",
    "# Register scalable target\n",
    "register_response = autoscale.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker', \n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    MinCapacity=0,  \n",
    "    MaxCapacity=3 # * check how many instances available in your account\n",
    ")\n",
    "\n",
    "# Define scaling policy\n",
    "scalingPolicy_response = autoscale.put_scaling_policy(\n",
    "    PolicyName='Invocations-ScalingPolicy',\n",
    "    ServiceNamespace='sagemaker', # The namespace of the AWS service that provides the resource. \n",
    "    ResourceId=resource_id,  \n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount', # SageMaker supports only Instance Count\n",
    "    PolicyType='TargetTrackingScaling', # 'StepScaling'|'TargetTrackingScaling'\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 3.0, # The target value for the metric. This needs to be setup based on load testing\n",
    "        'CustomizedMetricSpecification': {\n",
    "            'MetricName': 'ApproximateBacklogSizePerInstance',\n",
    "            'Namespace': 'AWS/SageMaker',\n",
    "            'Dimensions': [\n",
    "                {'Name': 'EndpointName', 'Value': endpoint_name }\n",
    "            ],\n",
    "            'Statistic': 'Average',\n",
    "        },\n",
    "        'ScaleInCooldown': 60, # The cooldown period helps you prevent your Auto Scaling group from launching or terminating \n",
    "                                # additional instances before the effects of previous activities are visible. \n",
    "                                # You can configure the length of time based on your instance startup time or other application needs.\n",
    "                                # ScaleInCooldown - The amount of time, in seconds, after a scale in activity completes before another scale in activity can start. \n",
    "        'ScaleOutCooldown': 60 # ScaleOutCooldown - The amount of time, in seconds, after a scale out activity completes before another scale out activity can start.\n",
    "        \n",
    "        # 'DisableScaleIn': True|False - indicates whether scale in by the target tracking policy is disabled. \n",
    "                            # If the value is true , scale in is disabled and the target tracking policy won't remove capacity from the scalable resource.\n",
    "    }\n",
    ")\n",
    "\n",
    "scalingPolicy_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4b7350-ed96-4ae8-b81b-97c2fe8a9cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger 1000 asynchronous invocations with autoscaling from 1 to 3\n",
    "# then scale down to 0 on completion\n",
    "\n",
    "print(endpoint_name)\n",
    "for i in range(1,100):\n",
    "    response = sm_runtime.invoke_endpoint_async(\n",
    "    EndpointName=endpoint_name, \n",
    "    InputLocation=input_path)\n",
    "    \n",
    "print(\"\\nAsync invocations for PyTorch serving with autoscaling\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67964e18-fab6-457a-9325-7ebea398d4a6",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660fd945-ee90-4773-add4-2168bb35d022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete Asynchronous inference endpoint\n",
    "async_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa31bad-1ccf-4cfe-ab0c-34ffb5eac997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edde762-d046-43f8-9e16-3b2c56d541c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
