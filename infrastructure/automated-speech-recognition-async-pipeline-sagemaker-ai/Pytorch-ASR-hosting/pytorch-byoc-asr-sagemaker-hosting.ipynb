{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db53f808-9fbd-408d-a6a0-d18200733876",
   "metadata": {},
   "source": [
    "# NVIDIA Parakeet ASR Model Deployment on Amazon SageMaker AI using Pytorch container\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to deploy the [**NVIDIA Parakeet TDT 0.6B v2**](https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2) model for Automatic Speech Recognition (ASR) tasks using Amazon SageMaker with both real-time and asynchronous inference capabilities.\n",
    "\n",
    "### About NVIDIA Parakeet TDT 0.6B v2\n",
    "\n",
    "The **Parakeet TDT (Transducer-Decoder-Transducer) 0.6B v2** is a state-of-the-art neural speech recognition model developed by NVIDIA:\n",
    "\n",
    "- **Architecture**: Transformer-based transducer model optimized for streaming ASR\n",
    "- **Model Size**: 600 million parameters, balancing accuracy and efficiency\n",
    "- **Performance**: Excellent accuracy on diverse speech datasets with low latency\n",
    "- **Language Support**: Primarily English, with robust performance across accents\n",
    "- **Optimization**: Built with NVIDIA NeMo framework for production deployment\n",
    "\n",
    "### SageMaker Asynchronous Inference Benefits\n",
    "\n",
    "**Asynchronous inference** is particularly well-suited for ASR workloads:\n",
    "\n",
    "1. **Long Processing Times**: Audio transcription can take several seconds to one hour\n",
    "2. **Variable Input Sizes**: Audio files range from seconds to hours in duration\n",
    "3. **Managed queuing**: Efficiently handle multiple audio files simultaneously\n",
    "4. **Cost Optimization**: Pay only for actual processing time, with automatic scaling to zero\n",
    "5. **Large File Support**: Handle audio files up to 1GB in size\n",
    "6. **Event driven pipeline**: Async endpoint supports SNS which helps with building a event driven pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a75f20-21ff-4d05-a0e8-50a6ceaa49c2",
   "metadata": {},
   "source": [
    "## Prerequisites and Setup\n",
    "\n",
    "**❗If you run this notebook in SageMaker Studio, please consider building the custom docker container from alternative services. This notebook was tested using SageMaker notebook instance with ml.g5.2xlarge and EBS volume 100G.**\n",
    "\n",
    "### Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12501f18-d807-4337-b4e9-7e1c2d7590df",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install datasets==2.14.7\n",
    "%pip install sagemaker==2.246.0 huggingface_hub\n",
    "%pip install librosa -q\n",
    "%pip install soundfile -q\n",
    "%pip install libcst==1.1.0\n",
    "%pip install --upgrade ml_dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9229164",
   "metadata": {},
   "source": [
    "Also install the nemo toolkit for asr, it will take a few mins for the installation to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f708b2d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install nemo_toolkit['asr']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab1118a-a0fa-4140-a4c8-7725f512f772",
   "metadata": {},
   "source": [
    "**❗Please restart the kernel before executing the cells below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fec1136-d7b0-402f-b959-d0302680c508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import sagemaker\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import soundfile as sf\n",
    "from datasets import load_dataset\n",
    "import nemo.collections.asr as nemo_asr\n",
    "\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56301238",
   "metadata": {},
   "source": [
    "## Configuration Setup\n",
    "\n",
    "Initialize the basic configuration for SageMaker deployment:\n",
    "\n",
    "- **SageMaker Session**: Manages interactions with SageMaker services\n",
    "- **S3 Bucket**: Default bucket for storing model artifacts and data\n",
    "- **IAM Role**: Execution role with necessary permissions for SageMaker\n",
    "- **S3 Prefixes**: Organized folder structure for model artifacts\n",
    "- **Runtime Client**: For invoking endpoints after deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ea851-3d8c-42d6-bcbe-334d7cde01ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic configurations\n",
    "sess = sagemaker.session.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'parakeet-asr'\n",
    "role = sagemaker.get_execution_role()\n",
    "s3_model_prefix = (\n",
    "    \"hf-asr-models/nvidia-asr\"  # folder within bucket where code artifact will go\n",
    ")\n",
    "# below boto3 clients are for invoking asynchronous endpoint \n",
    "sm_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "region = sess.boto_region_name\n",
    "account_id = sess.account_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb0f2b-1dd6-4624-ad4b-b65a19937d2d",
   "metadata": {},
   "source": [
    "## Model Preparation and Testing\n",
    "\n",
    "### Local Model Testing\n",
    "\n",
    "Before deploying to SageMaker, we'll first test the Parakeet model locally to ensure it works correctly. This step helps validate:\n",
    "\n",
    "- Model loading and initialization\n",
    "- Audio file processing capabilities\n",
    "\n",
    "### Download Model from HuggingFace Hub\n",
    "\n",
    "Download the NVIDIA Parakeet model from HuggingFace Hub for SageMaker deployment:\n",
    "\n",
    "- **Model Repository**: `nvidia/parakeet-tdt-0.6b-v2`\n",
    "- **File Filtering**: Only download necessary files (*.json, *.safetensors, *.nemo)\n",
    "- **Local Caching**: Store model files locally for packaging\n",
    "- **LFS Support**: Handle large model files using Git LFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2376d9-c0d1-4129-b082-7ac72c7ba1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sagemaker\n",
    "import jinja2\n",
    "\n",
    "parakeet = \"nvidia/parakeet-tdt-0.6b-v2\"\n",
    "\n",
    "# - This will download the model into the current directory where ever the jupyter notebook is running\n",
    "local_model_path = Path(\".\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = parakeet\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.safetensors\", \"*.nemo\"]\n",
    "\n",
    "# - Leverage the snapshot library to donload the model since the model is stored in repository using LFS\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_model_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6071c7-dfac-4114-ab17-c71bfaf5df66",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = nemo_asr.models.ASRModel.restore_from(restore_path=model_download_path + \"/parakeet-tdt-0.6b-v2.nemo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a9318a-f79f-4fe7-aafd-7f32aa8d8c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.transcribe(['../data/test.wav'])\n",
    "print(output[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a6e606",
   "metadata": {},
   "source": [
    "### Package Model for SageMaker\n",
    "\n",
    "Create a compressed archive of the model files for SageMaker deployment, the below cell might take a few mins to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9970f0f1-ac59-45a4-a7d9-d83ea1e17a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $model_download_path/parakeet-tdt-0.6b-v2.nemo parakeet-tdt-0.6b-v2.nemo\n",
    "!tar -cvzf model.tar.gz parakeet-tdt-0.6b-v2.nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ac982-d2c7-45cc-b596-a69f743c3782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model to S3\n",
    "model_uri = sess.upload_data('model.tar.gz', bucket=bucket, key_prefix=s3_model_prefix)\n",
    "!rm model.tar.gz\n",
    "!rm parakeet-tdt-0.6b-v2.nemo\n",
    "model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9da4e8",
   "metadata": {},
   "source": [
    "### Create Custom Docker Image by extending from AWS prebuilt PyTorch container\n",
    "\n",
    "Build a custom Docker image optimized for NVIDIA Parakeet ASR model:\n",
    "\n",
    "**Base Image**: PyTorch inference container with GPU support\n",
    "- `pytorch-inference:2.6.0-gpu-py312-cu124-ubuntu22.04-sagemaker`\n",
    "\n",
    "**System Dependencies**:\n",
    "- **ffmpeg**: Audio format conversion and processing\n",
    "- **libsndfile1**: Audio file I/O library\n",
    "\n",
    "**Python Dependencies**:\n",
    "- **nemo_toolkit[asr]**: NVIDIA NeMo framework for ASR\n",
    "- **ffmpeg-python**: Python wrapper for ffmpeg\n",
    "- **soundfile**: Audio file reading/writing\n",
    "\n",
    "Note that the notebook was tested in 'us-west-2', if you are using other region, please check the prebuilt image uri in that region from the [available images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327538f4-f49b-4bac-9c03-bf0df848326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "# SageMaker PyTorch image\n",
    "FROM 763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:2.6.0-gpu-py312-cu124-ubuntu22.04-sagemaker\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    ffmpeg \\\n",
    "    libsndfile1 \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN pip install nemo_toolkit[asr] ffmpeg-python soundfile cuda-python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c995fcf",
   "metadata": {},
   "source": [
    "### Configuring Docker Storage on Amazon SageMaker Notebook Instances\n",
    "Amazon SageMaker notebook instances come with a 5 GB Amazon EBS storage volume by default, but Docker uses the instance's root volume for storing images and containers. When building multiple Docker images, the root volume has limited disk space that can quickly run out of space, causing \"no space left on device\" errors.\n",
    "\n",
    "To solve this storage limitation, you can redirect Docker to use the larger EBS volume by modifying the Docker daemon configuration. Note that the EBS volume specified for the SageMaker Notebook instance is under path `/home/ec2-user/SageMaker`, but when you go to terminal the default path is `/home/ec2-user`.\n",
    "\n",
    "From the terminal, you can stop the Docker service and edit the Docker daemon configuration.\n",
    "\n",
    "```\n",
    "# Stop the Docker service\n",
    "sudo systemctl stop docker\n",
    "\n",
    "# Create the new Docker data directory on the EBS volume\n",
    "sudo mkdir -p /home/ec2-user/SageMaker/docker\n",
    "\n",
    "# Create or edit the Docker daemon configuration file\n",
    "sudo nano /etc/docker/daemon.json\n",
    "```\n",
    "\n",
    "Add the following configuration to /etc/docker/daemon.json:\n",
    "```json\n",
    "{\n",
    "    \"runtimes\": {\n",
    "        \"nvidia\": {\n",
    "            \"args\": [],\n",
    "            \"path\": \"nvidia-container-runtime\"\n",
    "        }\n",
    "    },\n",
    "    \"data-root\": \"/home/ec2-user/SageMaker/docker\"\n",
    "}\n",
    "```\n",
    "\n",
    "Restart the docker service\n",
    "```\n",
    "# Start the Docker service with new configuration\n",
    "sudo systemctl start docker\n",
    "\n",
    "# Verify Docker is running with correct data directory\n",
    "docker info | grep \"Docker Root Dir\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a30252",
   "metadata": {},
   "source": [
    "### Build and Push Docker Image to ECR\n",
    "\n",
    "Build the custom Docker image and push it to Amazon Elastic Container Registry (ECR):\n",
    "\n",
    "**Process Overview**:\n",
    "1. **ECR Repository**: Create or verify ECR repository exists\n",
    "2. **Docker Build**: Build the custom image with all dependencies\n",
    "3. **Authentication**: Login to ECR using AWS credentials\n",
    "4. **Tag and Push**: Tag the image and push to ECR\n",
    "\n",
    "**Repository Naming**: `asr-sagemaker` for easy identification\n",
    "\n",
    "Note that, you need to make sure the IAM role has proper permission to access the ECR service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b1956e-8048-466d-bb0e-766252a0ac3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\n",
    "REGION=$(aws configure get region)\n",
    "REPOSITORY_NAME=asr-sagemaker\n",
    "\n",
    "# login to access the base image from the prebuilt images\n",
    "aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin 763104351884.dkr.ecr.$REGION.amazonaws.com\n",
    "\n",
    "# Create ECR repository if needed\n",
    "if aws ecr describe-repositories --repository-names \"${REPOSITORY_NAME}\" &>/dev/null; then\n",
    "    echo \"Repository ${REPOSITORY_NAME} already exists\"\n",
    "else\n",
    "    echo \"Creating ECR repository ${REPOSITORY_NAME}...\"\n",
    "    aws ecr create-repository \\\n",
    "        --repository-name \"${REPOSITORY_NAME}\" \\\n",
    "        --region \"${REGION}\"\n",
    "fi\n",
    "\n",
    "#build docker image and push to ECR repository\n",
    "docker build -t asr-sagemaker .\n",
    "aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com\n",
    "docker tag asr-sagemaker:latest $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$REPOSITORY_NAME:latest\n",
    "docker push $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$REPOSITORY_NAME:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10a2c82",
   "metadata": {},
   "source": [
    "### Create PyTorch Model Object\n",
    "\n",
    "Create a SageMaker PyTorchModel with specific environment variables setup for async workloads:\n",
    "\n",
    "**Key Configuration Parameters**:\n",
    "- **SAGEMAKER_MODEL_SERVER_WORKERS**: set the number of torch worker that will load the the number of model copied into GPU memory\n",
    "- **TS_DEFAULT_RESPONSE_TIMEOUT**: time out setting for Torch server worker, for long audio processing you can set it to a higher number\n",
    "- **TS_MAX_REQUEST_SIZE**: byte size values for request, set to 1G for async endpoint\n",
    "- **TS_MAX_RESPONSE_SIZE**: byte size values for response\n",
    "  \n",
    "**Session Selection**: Switch between local testing and cloud deployment\n",
    "SageMaker local session is a feature in the SageMaker Python SDK that allows you to create estimators and run training, processing, and inference jobs locally using Docker containers instead of managed AWS infrastructure, providing a fast way to test and debug your machine learning scripts before scaling to production. You can see more examples in this [github repo](https://github.com/aws-samples/amazon-sagemaker-local-mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc49067-13c1-4512-b453-aead1c46164c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate a unique model name and provide image uri\n",
    "\n",
    "id = int(time.time())\n",
    "model_name = f'parakeet-model-{id}'\n",
    "\n",
    "# !Please change the image URI for the region that you are using: e.g. us-east-1\n",
    "image = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/asr-sagemaker:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b06cc44-3c20-4815-8323-d6f1cf9e9dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.local import LocalSession\n",
    "\n",
    "local_session = LocalSession()\n",
    "local_session.config = {'local': {'local_code': True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9682718-6ddf-4a75-a99e-7df8ba9a9c03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a PyTorchModel for deployment\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "parakeet_model = PyTorchModel(\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir=\"code\",\n",
    "    model_data=model_uri,\n",
    "    image_uri=image,\n",
    "    role=role,\n",
    "    name=model_name,\n",
    "    env={\"SAGEMAKER_MODEL_SERVER_WORKERS\": \"2\",\n",
    "         \"TS_MAX_REQUEST_SIZE\": \"1073741824\",\n",
    "         \"TS_MAX_RESPONSE_SIZE\": \"1073741824\",\n",
    "         \"TS_DEFAULT_RESPONSE_TIMEOUT\": \"300\"\n",
    "        },\n",
    "    # sagemaker_session=local_session # used for local test\n",
    "    sagemaker_session=sess  # used for actual endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee32e99-edb5-43da-9bd4-015d4477933d",
   "metadata": {},
   "source": [
    "### Real-time inference \n",
    "\n",
    "Set up data serialization for audio file processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25574fd6-d888-47b3-beb4-ab9a441d648a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serializers import DataSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Define serializers and deserializer\n",
    "audio_serializer = DataSerializer(content_type=\"audio/x-audio\")\n",
    "deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2077ac87",
   "metadata": {},
   "source": [
    "### Deploy Real-time Endpoint\n",
    "\n",
    "Deploy the model as a real-time inference endpoint. Note that if you choose to use the local SageMaker Session when creating the model object, change the `instance_type` to `local_gpu` to be able to quickly test the endpoint from local SageMaker notebook instance for fast testing. If you are going to deploy the model to an async endpoint, please make sure you create the PyTorchModel object with the actual sagemaker session. In this case, it will be `sess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a97d22-3990-4024-b859-1df12da7741d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deploy the model for real-time inference locally or remotely\n",
    "endpoint_name = f'parakeet-real-time-endpoint-{id}'\n",
    "\n",
    "real_time_predictor = parakeet_model.deploy(\n",
    "    initial_instance_count=1,  # number of instances\n",
    "    # instance_type=\"local_gpu\",\n",
    "    instance_type=\"ml.g5.xlarge\",  # instance type\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=audio_serializer,\n",
    "    deserializer=deserializer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bb24c1",
   "metadata": {},
   "source": [
    "### Test Real-time Inference\n",
    "\n",
    "Test the deployed real-time endpoint with a sample audio file:\n",
    "\n",
    "- **Input**: Audio file path (automatically serialized)\n",
    "- **Processing**: Synchronous transcription\n",
    "- **Output**: JSON response with transcription results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8635750c-1d6e-48b6-ba28-3122924392da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# Perform real-time inference\n",
    "audio_path = \"../data/test.wav\"\n",
    "response = real_time_predictor.predict(data=audio_path)\n",
    "print(response[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfac6e3",
   "metadata": {},
   "source": [
    "Uncomment below cell to delete the endpoint once you finish testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfdd88c-8ded-408c-b1ba-aa9b273d1d2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## optional: Delete real-time inference endpoint\n",
    "real_time_predictor.delete_endpoint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579c16fd-99e0-4178-8845-10a0fdaf02ac",
   "metadata": {},
   "source": [
    "## Asynchronous Inference Deployment\n",
    "\n",
    "Set up asynchronous inference configuration with comprehensive monitoring:\n",
    "\n",
    "**Configuration Components**:\n",
    "- **Output Path**: S3 location for storing transcription results\n",
    "- **Concurrency**: Maximum concurrent invocations per instance (4 for optimal GPU usage)\n",
    "- **SNS Notifications**: Real-time alerts for job completion status\n",
    "- **Failure Path**: Separate S3 location for failed job artifacts\n",
    "\n",
    "**SNS Topics**: Configure separate topics for success and failure notifications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d8d055",
   "metadata": {},
   "source": [
    "### Create SNS Topics for Async Notifications\n",
    "\n",
    "Create SNS topics for monitoring asynchronous inference job status:\n",
    "\n",
    "- **Success Topic**: Receives notifications when transcription jobs complete successfully\n",
    "- **Error Topic**: Receives notifications when transcription jobs fail\n",
    "- **Automatic Creation**: Creates topics if they don't exist, reuses if they do\n",
    "- **Subscription Ready**: Topics are ready for email, SMS, or Lambda subscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf4e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_client = boto3.client('sts')\n",
    "\n",
    "print(f\"Account ID: {account_id}\")\n",
    "print(f\"Region: {region}\")\n",
    "\n",
    "# Initialize SNS client\n",
    "sns_client = boto3.client('sns')\n",
    "\n",
    "def create_sns_topic_if_not_exists(topic_name, description):\n",
    "    \"\"\"Create SNS topic if it doesn't exist, return the ARN\"\"\"\n",
    "    try:\n",
    "        # Try to create the topic (idempotent operation)\n",
    "        response = sns_client.create_topic(Name=topic_name)\n",
    "        topic_arn = response['TopicArn']\n",
    "        \n",
    "        # Set topic attributes for better identification\n",
    "        sns_client.set_topic_attributes(\n",
    "            TopicArn=topic_arn,\n",
    "            AttributeName='DisplayName',\n",
    "            AttributeValue=description\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Topic '{topic_name}' ready: {topic_arn}\")\n",
    "        return topic_arn\n",
    "        \n",
    "    except ClientError as e:\n",
    "        print(f\"❌ Error creating topic '{topic_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "# Create success topic\n",
    "success_topic_name = \"async-success\"\n",
    "success_description = \"SageMaker Async Inference Success Notifications\"\n",
    "success_topic_arn = create_sns_topic_if_not_exists(success_topic_name, success_description)\n",
    "\n",
    "# Create error topic  \n",
    "error_topic_name = \"async-failed\"\n",
    "error_description = \"SageMaker Async Inference Error Notifications\"\n",
    "error_topic_arn = create_sns_topic_if_not_exists(error_topic_name, error_description)\n",
    "\n",
    "print(f\"\\n📧 SNS Topics Created Successfully:\")\n",
    "print(f\"Success Topic ARN: {success_topic_arn}\")\n",
    "print(f\"Error Topic ARN: {error_topic_arn}\")\n",
    "\n",
    "print(f\"\\n🔧 Topics are ready for AsyncInferenceConfig!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3470dd4a-f898-4344-9e65-bc95f0fa76b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sagemaker.async_inference import AsyncInferenceConfig\n",
    "\n",
    "# Create an AsyncInferenceConfig object\n",
    "async_config = AsyncInferenceConfig(\n",
    "    output_path=f\"s3://{bucket}/{prefix}/output\", \n",
    "    max_concurrent_invocations_per_instance = 4,\n",
    "    notification_config = {\n",
    "      \"SuccessTopic\": f\"arn:aws:sns:{region}:{account_id}:async-success\",\n",
    "      \"ErrorTopic\": f\"arn:aws:sns:{region}:{account_id}:async-failed\",\n",
    "    }, #  Notification configuration \n",
    "    failure_path=f\"s3://{bucket}/{prefix}/failed\"\n",
    ")\n",
    "\n",
    "# Deploy the model for async inference\n",
    "endpoint_name = f'parakeet-async-endpoint-{id}'\n",
    "async_predictor = parakeet_model.deploy(\n",
    "    async_inference_config=async_config,\n",
    "    initial_instance_count=1, # number of instances\n",
    "    instance_type ='ml.g5.xlarge', # instance type\n",
    "    endpoint_name = endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473cc709",
   "metadata": {},
   "source": [
    "Upload test audio file to S3 for asynchronous processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ec77c-87a2-45f0-818d-ce49844558d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(s3_client, file_path, bucket_name, s3_key):\n",
    "    \"\"\"Upload file to S3\"\"\"\n",
    "    try:\n",
    "        s3_client.upload_file(file_path, bucket_name, s3_key)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading {s3_key}: {e}\")\n",
    "        return False\n",
    "s3_client = boto3.client('s3')\n",
    "audio_path = \"../data/test_audio.wav\" \n",
    "s3_key = prefix+f\"/data/{audio_path}\"\n",
    "upload_to_s3(s3_client, audio_path, bucket, s3_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9bf42a-5cba-49fb-96d2-4ac40c96d5fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Provide the S3 path for the audio file you want to processs\n",
    "\n",
    "input_path = f\"s3://{bucket}/{s3_key}\"\n",
    "input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddbb3d2-0b8d-4431-9cdb-6f48acaad709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform async inference\n",
    "initial_args = {'ContentType':\"audio/x-audio\"}\n",
    "response = async_predictor.predict_async(initial_args = initial_args, input_path=input_path)\n",
    "response.output_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeb24e8",
   "metadata": {},
   "source": [
    "Monitor and retrieve the results from asynchronous processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd11808-f270-4e04-b83e-6b76d7b87ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def get_output(output_location):\n",
    "    output_url = urllib.parse.urlparse(output_location)\n",
    "    bucket = output_url.netloc\n",
    "    key = output_url.path[1:]\n",
    "    while True:\n",
    "        try:\n",
    "            return sess.read_s3_file(\n",
    "                        bucket=output_url.netloc, \n",
    "                        key_prefix=output_url.path[1:])\n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == 'NoSuchKey':\n",
    "                print(\"waiting for output...\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            raise\n",
    "            \n",
    "output = get_output(response.output_path)\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d804fecd-42d2-494e-8169-9c48f5fec233",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Optional: Advanced Configuration Auto-scaling for Asynchronous Inference\n",
    "\n",
    "\n",
    "Auto-scaling supports scale down to zero which can help with cost saving when there are no workloads requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16a29a-67ac-4884-94ba-e2424debe2cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "autoscale = boto3.client('application-autoscaling') \n",
    "resource_id='endpoint/' + endpoint_name + '/variant/' + 'AllTraffic'\n",
    "\n",
    "# Register scalable target\n",
    "register_response = autoscale.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker', \n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    MinCapacity=0,  \n",
    "    MaxCapacity=3 # * check how many instances available in your account\n",
    ")\n",
    "\n",
    "# Define scaling policy\n",
    "scalingPolicy_response = autoscale.put_scaling_policy(\n",
    "    PolicyName='Invocations-ScalingPolicy',\n",
    "    ServiceNamespace='sagemaker', # The namespace of the AWS service that provides the resource. \n",
    "    ResourceId=resource_id,  \n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount', # SageMaker supports only Instance Count\n",
    "    PolicyType='TargetTrackingScaling', # 'StepScaling'|'TargetTrackingScaling'\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 3.0, # The target value for the metric. This needs to be setup based on load testing\n",
    "        'CustomizedMetricSpecification': {\n",
    "            'MetricName': 'ApproximateBacklogSizePerInstance',\n",
    "            'Namespace': 'AWS/SageMaker',\n",
    "            'Dimensions': [\n",
    "                {'Name': 'EndpointName', 'Value': endpoint_name }\n",
    "            ],\n",
    "            'Statistic': 'Average',\n",
    "        },\n",
    "        'ScaleInCooldown': 60, # The cooldown period helps you prevent your Auto Scaling group from launching or terminating \n",
    "                                # additional instances before the effects of previous activities are visible. \n",
    "                                # You can configure the length of time based on your instance startup time or other application needs.\n",
    "                                # ScaleInCooldown - The amount of time, in seconds, after a scale in activity completes before another scale in activity can start. \n",
    "        'ScaleOutCooldown': 60 # ScaleOutCooldown - The amount of time, in seconds, after a scale out activity completes before another scale out activity can start.\n",
    "        \n",
    "        # 'DisableScaleIn': True|False - indicates whether scale in by the target tracking policy is disabled. \n",
    "                            # If the value is true , scale in is disabled and the target tracking policy won't remove capacity from the scalable resource.\n",
    "    }\n",
    ")\n",
    "\n",
    "scalingPolicy_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4b7350-ed96-4ae8-b81b-97c2fe8a9cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger 1000 asynchronous invocations with autoscaling from 1 to 3\n",
    "# then scale down to 0 on completion\n",
    "\n",
    "print(endpoint_name)\n",
    "for i in range(1,100):\n",
    "    response = sm_runtime.invoke_endpoint_async(\n",
    "    EndpointName=endpoint_name, \n",
    "    InputLocation=input_path)\n",
    "    \n",
    "print(\"\\nAsync invocations for PyTorch serving with autoscaling\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67964e18-fab6-457a-9325-7ebea398d4a6",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660fd945-ee90-4773-add4-2168bb35d022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete Asynchronous inference endpoint\n",
    "async_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa31bad-1ccf-4cfe-ab0c-34ffb5eac997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edde762-d046-43f8-9e16-3b2c56d541c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
